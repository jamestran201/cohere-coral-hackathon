Preliminary Post Incident Review (PIR) – Services impacted after power issue – West Europe

This is our “Preliminary” PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a “Final” PIR with additional details/learnings.

Note: During this incident, in addition to our standard communications via Azure Service Health and its alerts, we also communicated via the public Azure Status page. This temporary overcommunication was to ensure that all affected customers received information, described as 'scenario 3' in our documentation: https://aka.ms/StatusPageCriteria. Our final Post-Incident Review (PIR) will be communicated to affected customers through Azure Service Health, then this entry on the Status History page will be removed. As described in our documentation, public PIR postings on this page are reserved for 'scenario 1' incidents - typically broadly impacting incidents across entire zones or regions, or even multiple zones or regions.

What happened?

A power issue impacted a subset of our infrastructure in a single Availability Zone within the West Europe region, between 07:31 and 09:15 UTC on 20 October 2023. Customers using Azure services including App Service, Cosmos DB, SQL DB, Storage, and Virtual Machines may have experienced availability issues in the affected Availability Zone. While the vast majority of customer impact was mitigated by 09:15 UTC, a long-tail recovery for a small subset of Storage services restored full availability by 17:10 UTC. Beyond service-specific impacts, note that the Azure Resource Manager (ARM) control plane experienced a small dip in availability (total availability dropped to approximately 98.9%) due to issues caused by a downstream dependency on Cosmos DB, with failures returning to pre-incident levels by 08:05 UTC.

What went wrong and why?

We detected instability from the utility power grid in the form of voltage sags/swells to one of our datacenters within physical AZ-01, one of the region’s three Availability Zones. Note that the 'logical' zones used by each customer subscription may correspond to different physical zones – customers can use the Locations API to understand this mapping, to confirm which resources run in the impacted physical AZ, see: https://learn.microsoft.com/rest/api/resources/subscriptions/list-locations. Due to the prolonged nature of the instability, we decided to transfer load from the grid towards our back-up generators. However, during this process, a critical failure occurred in a section of the electrical distribution system, preventing 10% of our generators from taking load. This failure left the main distribution system offline and the redundant system inaccessible. As a result of this failure, approximately 1% of our server racks in this Availability Zone lost power.

How did we respond?

The issue was reported by our datacenter team to our incident management team immediately, so relevant on-call engineers started to investigate the impacted equipment in near real time. While investigating, we confirmed that grid power had stabilized, so we transferred power back to the grid at approximately 08:00 UTC. We then started to bring affected infrastructure back online, as per our standard operating procedures. Once networking and storage infrastructure had recovered, compute scale units were brought back online, in turn restoring service to the vast majority of Azure services by 09:15 UTC. In total, five Storage scale units were impacted by this incident. Following power restoration, four recovered completely by 09:10 UTC, while the fifth required hardware diagnostics and part replacements on approximately 5% of its storage nodes. As a result it took longer to restore availability for the last <1% of storage accounts, with downstream impact to customers and services reliant on this final Storage scale unit. By 14:30 UTC, all but a few storage accounts had their availability restored, and by 17:10 UTC, full restoration was complete.

How are we making incidents like this less likely or less impactful?

This is our “Preliminary” PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a “Final” PIR with additional details/learnings – including repair items related to the datacenter/power trigger event, and any potential repair items for downstream services to recover from scenarios like this one more quickly.
How can customers make incidents like this less impactful?

Consider using Availability Zones (AZs) to run your services across physically separate locations within an Azure region. To help services be more resilient to datacenter-level failures like this one, each AZ provides independent power, networking, and cooling. Many Azure services support zonal, zone-redundant, and/or always-available configurations: https://docs.microsoft.com/azure/availability-zones/az-overview
For mission-critical workloads, customers should consider a multi-region geodiversity strategy to avoid impact from incidents like this one that impacted a single region: https://learn.microsoft.com/training/modules/design-a-geographically-distributed-application and https://learn.microsoft.com/azure/architecture/patterns/geodes
More generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review: https://docs.microsoft.com/azure/architecture/framework/resiliency
Finally, consider ensuring that the right people in your organization will be notified about any future service issues – by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more: https://aka.ms/ash-alerts