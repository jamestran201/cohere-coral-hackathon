{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "We\u2019d like to share more about the service event that occurred on Monday, October 22nd in the US- East Region. We have now completed the analysis of the events that affected AWS customers, and we want to describe what happened, our understanding of how customers were affected, and what we are doing to prevent a similar issue from occurring in the future."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "The Primary Event and the Impact to Amazon Elastic Block Store (EBS) and Amazon Elastic Compute Cloud (EC2)"}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "At 10:00AM PDT Monday, a small number of Amazon Elastic Block Store (EBS) volumes in one of our five Availability Zones in the US-East Region began seeing degraded performance, and in some cases, became \u201cstuck\u201d (i.e. unable to process further I/O requests). The root cause of the problem was a latent bug in an operational data collection agent that runs on the EBS storage servers. Each EBS storage server has an agent that contacts a set of data collection servers and reports information that is used for fleet maintenance. The data collected with this system is important, but the collection is not time- sensitive and the system is designed to be tolerant of late or missing data. Last week, one of the data collection servers in the affected Availability Zone had a hardware failure and was replaced. As part of replacing that server, a DNS record was updated to remove the failed server and add the replacement server. While not noticed at the time, the DNS update did not successfully propagate to all of the internal DNS servers, and as a result, a fraction of the storage servers did not get the updated server address and continued to attempt to contact the failed data collection server. Because of the design of the data collection service (which is tolerant to missing data), this did not cause any immediate issues or set off any alarms. However, this inability to contact a data collection server triggered a latent memory leak bug in the reporting agent on the storage servers. Rather than gracefully deal with the failed connection, the reporting agent continued trying to contact the collection server in a way that slowly consumed system memory. While we monitor aggregate memory consumption on each EBS Server, our monitoring failed to alarm on this"}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "data collection server triggered a latent memory leak bug in the reporting agent on the storage servers. Rather than gracefully deal with the failed connection, the reporting agent continued trying to contact the collection server in a way that slowly consumed system memory. While we monitor aggregate memory consumption on each EBS Server, our monitoring failed to alarm on this memory leak. EBS Servers generally make very dynamic use of all of their available memory for managing customer data, making it difficult to set accurate alarms on memory usage and free memory. By Monday morning, the rate of memory loss became quite high and consumed enough memory on the affected storage servers that they were unable to keep up with normal request handling processes."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "The memory pressure on many of the EBS servers had reached a point where EBS servers began losing the ability to process customer requests and the number of stuck volumes increased quickly. This caused the system to begin to failover from the degraded servers to healthy servers. However, because many of the servers became memory-exhausted at the same time, the system was unable to find enough healthy servers to failover to, and more volumes became stuck. By approximately 11:00AM PDT, a large number of volumes in this Availability Zone were stuck. To remedy this, at 11:10AM PDT, the team made adjustments to reduce the failover rate. These adjustments removed load from the service, and by 11:35AM PDT, the system began automatically recovering many volumes. By 1:40PM PDT, about 60% of the affected volumes had recovered. The team continued to work to understand the issue and restore performance for the remaining volumes. The large surge in failover and recovery activity in the cluster made it difficult for the team to identify the root cause of the event. At 3:10PM PDT, the team identified the underlying issue and was able to begin restoring performance for the remaining volumes by freeing the excess memory consumed by the misbehaving collection agent. At this point, the system was able to recover most of the remaining stuck volumes; and by 4:15PM PDT, nearly all affected volumes were restored and performing normally."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "We have deployed monitoring that will alarm if we see this specific memory leak again in any of our production EBS servers, and next week, we will begin deploying a fix for the memory leak issue. We are also modifying our system memory monitoring on the EBS storage servers to monitor and alarm on each process\u2019s memory consumption, and we will be deploying resource limits to prevent low priority processes from consuming excess resources on these hosts. We are also updating our internal DNS configuration to further ensure that DNS changes are propagated reliably, and as importantly, make sure that our monitoring and alarming surface issues more quickly should these changes not succeed. These actions will address the problems that triggered the event. In addition, we are evaluating how to change the EBS failover logic that led to the rapid deterioration early in this event. We believe we can make adjustments to reduce the impact of any similar correlated failure or degradation of EBS servers within an Availability Zone."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "Impact on the EC2 and EBS APIs"}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "The primary event only affected EBS volumes in a single Availability Zone, so those customers running with adequate capacity in other Availability Zones in the US East Region were able to tolerate the event with limited impact to their applications. However, many customers reported difficulty using the service APIs to manage their resources during this event. We have invested heavily in making our service APIs resilient to failure during events affecting a single Availability Zone. And, other than a few short periods, our monitoring showed what looked to be a healthy level of launch and create activity throughout the event. However, we\u2019ve heard from customers that they struggled to use the APIs for several hours. We now understand that our API throttling during the event disproportionately impacted some customers and affected their ability to use the APIs."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "We use throttling to protect our services from being overwhelmed by internal and external callers that intentionally or unintentionally put excess load on our services. A simple example of the kind of issue throttling protects against is a runaway application that naively retries a request as fast as possible when it fails to get a positive result. Our systems are scaled to handle these sorts of client errors, but during a large operational event, it is not uncommon for many users to inadvertently increase load on the system. So, while we always have a base level of throttling in place, the team enabled a more aggressive throttling policy during this event to try to assure that the system remained stable during the period where customers and the system were trying to recover. Unfortunately, the throttling policy that was put in place was too aggressive."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "At 12:06PM PDT, the team implemented this aggressive API throttling policy to help assure stability of the system during the recovery. The team monitored the aggregate throttling rate as well as the overall activity (launches, volume creation, etc.) and did not at the time believe that customers were being substantially impacted. We now understand that this throttling policy, for a subset of our customers, was throttling a higher percentage of API calls than we realized during the event. The service APIs were still handling the vast majority of customer requests to launch and terminate instances and make other changes to their EC2 and EBS resources, but many customers experienced high levels of throttling on calls to describe their resources (e.g. DescribeInstances, DescribeImages, etc.). This made it difficult for these customers and their management applications to successfully use the service APIs during this period. It also affected users\u2019 ability to successfully manage their EC2 and EBS resources from the AWS Management Console. This throttling policy was in effect until 2:33PM PDT, after which we reduced the level of throttling considerably."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "We have changed our operational procedures to not use this more aggressive throttling policy during any future event. We believe that our other throttling policies will provide us with the necessary service protection while avoiding the impact that customers saw during this event. We are also modifying our operational dashboard to add per-customer throttling monitoring (rather than just aggregate throttling rates) so that we have better visibility into the number of customers seeing heavy throttling. This will allow us to quickly understand the impact throttling is having on individual customers, regardless of what the overall throttling rate is, and make appropriate adjustments more quickly."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "Throttling is a valuable tool for managing the health of our services, and we employ it regularly without significantly affecting customers\u2019 ability to use our services. While customers need to expect that they will encounter API throttling from time to time, we realize that the throttling policy we used for part of this event had a greater impact on many customers than we understood or intended. While this did not meaningfully affect users running high-availability applications architected to run across multiple Availability Zones with adequate running capacity to failover during Availability Zone disruptions, it did lead to several hours of significant API degradation for many of our customers. This inhibited these customers\u2019 ability to use the APIs to recover from this event, and in some cases, get normal work done. Therefore, AWS will be issuing a credit to any customer whose API calls were throttled by this aggressive throttling policy (i.e. any customer whose API access was throttled between 12:06PM PDT and 2:33PM PDT) for 100% of their EC2, EBS and ELB usage for three hours of their Monday usage (to cover the period the aggressive throttling policy was in place). Affected customers do not need to take any action; the credits will be automatically applied to their AWS account prior to their October 31 bill being calculated."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "Impact on Amazon Relational Database Service (RDS)"}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "This event also had an impact on the Amazon Relational Database Service (\u201cRDS\u201d). RDS uses EBS for database and log storage, and as a result, a portion of the RDS databases hosted in the affected Availability Zone became inaccessible. Throughout the course of the event, customers were able to create new RDS instances and access existing RDS instances in the unaffected Availability Zones in the region."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "Amazon RDS provides two modes of operation: Single Availability Zone (Single-AZ), where a single database instance operates in one Availability Zone; and Multi Availability Zone (Multi-AZ), where two database instances are synchronously operated in two different Availability Zones. For Multi-AZ RDS, one of the two database instances is the \u201cprimary\u201d and the other is a \u201cstandby.\u201d The primary handles all database requests and replicates to the standby. In the case where a primary fails, the standby is promoted to be the new primary and is available to handle database requests after integrity checks are completed."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "Single-AZ database instances are exposed to disruptions in an Availability Zone. In this case, a Single-AZ database instance would have been affected if one of the EBS volumes it was relying on got stuck. During this event, a significant number of the Single-AZ databases in the affected zone became stuck as the EBS volumes used by them were affected by the primary EBS event described above. In the case of these Single-AZ databases, recovery depended on waiting for the underlying EBS volumes to have their performance restored. By 1:30PM PDT, a significant number of the impaired Single-AZ RDS instances were restored as the volumes they depended on became unstuck. By 3:30PM PDT, the majority of the affected database instances were restored, and by 6:35PM PDT, almost all of the affected Single-AZ RDS instances were restored."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "During the course of the event, almost all of the Multi-AZ instances were promoted to their standby in a healthy Availability Zone, and were available to handle database requests after integrity checks were completed. However, a single digit percentage of Multi-AZ RDS instances in the affected Availability Zone did not failover automatically due to two different software bugs. The first group of RDS instances that did not failover as expected encountered an uncommon stuck I/O condition, which the automatic failover logic did not handle correctly. These instances required operator action and were fully restored by 11:30 AM PDT. We have developed a fix for this bug and are in the process of rolling it out. The second group of Multi-AZ instances did not failover automatically because the master database instances were disconnected from their standby for a brief time interval immediately before these master database instances\u2019 volumes became stuck. Normally these events are simultaneous. Between the period of time the masters were disconnected from their standbys and the point where volumes became stuck, the masters continued to process transactions without being able to replicate to their standbys. When these masters subsequently became stuck, the system blocked automatic failover to the out-of-date standbys. We have already been working on a fix for this issue which will allow the standby to be favored immediately when its master is in an impaired Availability Zone. Due to the subtle nature of the issues involved, we are still in the process of completing this fix and carefully testing it, but are on track to deploy it fully by December. Database instances affected by this condition were restored once the associated EBS volumes had performance restored. While we are disappointed with the impact to these Multi-AZ instances, we are confident that when we complete the"}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "involved, we are still in the process of completing this fix and carefully testing it, but are on track to deploy it fully by December. Database instances affected by this condition were restored once the associated EBS volumes had performance restored. While we are disappointed with the impact to these Multi-AZ instances, we are confident that when we complete the deployment of these two bug fixes, the root cause of the Multi-AZ failures we observed during this event will be addressed. It is the top priority of the team to complete these fixes and get them deployed to the fleet."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "Customers affected by the Multi-AZ RDS issues did not get the availability they or we expected. If an application is Multi-AZ and has enough resources running to continue operating if one Availability Zone is lost, then that application component should remain available (with minimal service disruption). Accordingly, AWS will issue service credits to customers whose RDS Multi-AZ instances took longer than 20 minutes to fail over to their secondary copies, equal to 10 days of charges for those affected Multi-AZ instances. Affected customers do not need to take any action; the credits will be automatically applied to their AWS account prior to their October 31 bill being calculated."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "Impact on Amazon Elastic Load Balancing (ELB)"}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "This event also affected the Amazon Elastic Load Balancing (ELB) service. Each ELB load balancer uses one or more load balancer instances to route traffic to customers\u2019 EC2 instances. These ELB load balancer instances use EBS for storing configuration and monitoring information, and when the EBS volumes on these load balancer instances hung, some of the ELB load balancers became degraded and the ELB service began executing recovery workflows to either restore or replace the affected load balancer instances. Customers can use ELB with applications that the run in either single or multiple Availability Zones."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "For customers using an ELB load balancer with an application running in a single Available Zone, ELB provisions load balancer instances in the Availability Zone in which the application is running (effectively creating a Single-AZ load balancer). During this event, a number of Single-AZ load balancers in the affected Availability Zone became impaired when some or all of the load balancer instances used by the load balancer became inaccessible due to the primary EBS issue. These affected load balancers recovered as soon as the ELB system was able to provision additional EBS volumes in the affected Availability Zone, or in some cases, when the EBS volumes on which particular load balancers relied, were restored. By 1:10PM PDT, the majority of affected Single-AZ load balancers had recovered, and by 3:30PM PDT, most of the remaining load balancers had also been recovered. Recovery of the last remaining load balancers was then slowed by an issue encountered by the ELB recovery workflows. ELB uses Elastic IP addresses (EIPs) to reliably route traffic to load balancer instances. EIPs are consumed as new load balancers are created and as existing load balancers are scaled. The increased demand for EIPs from the ELB recovery workflows (and the overall increase of customer activity during this period) caused ELB to consume all of the EIPs that were available to it. This stalled the recovery workflows and delayed recovery of the final affected load balancers. The team continued to manually recover the remaining impaired load balancers and was able to remediate the EIP shortage at 9:50PM PDT."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "We are working on a number of improvements to shorten the recovery time of ELB for all customers. We will ensure that we have additional EIP capacity available to the ELB system at all times to allow full recovery of any Availability Zone issue. We are already in the process of making a few changes to reduce the interdependency between ELB and EBS to avoid correlated failure in future events and allow ELB recovery even when there are EBS issues within an Availability Zone. Finally, we are also in the process of a few additional improvements to our recovery workflows that will be released in the coming weeks that will further improve the recovery time of ELB load balancers during any similar event."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "For customers using an ELB load balancer with an application running in multiple Availability Zones, ELB will provision load balancer instances in every Availability Zone in which the application is running. For these multiple Availability Zone applications, ELB can route traffic away from degraded Availability Zones to allow multiple Availability Zone applications to quickly recover. During this event, customers using ELB with applications running in multiple Availability Zones that included the affected Availability Zone may have experienced elevated error rates during the early parts of the primary event as load balancer instances or the EC2 instances running the customer\u2019s application were affected by the EBS issue. By 11:49AM PDT, the ELB service shifted customer traffic away from the impaired Availability Zone for most load balancers with multiple Availability Zone applications. This allowed applications behind these load balancers to serve traffic from their instances in other, unaffected Availability Zones. Unfortunately, a bug in the traffic shifting functionality incorrectly mapped a small number of the affected load balancers and therefore didn\u2019t shift traffic correctly. These load balancers continued to send a portion of the customer requests to the affected Availability Zone until the issue was identified and corrected at 12:45PM PDT. We have corrected the logic in the ELB traffic shifting functionality so this error will not occur in the future. We are also working to improve the sensitivity of the traffic shifting procedure so that traffic is more quickly failed away from a degraded Availability Zone in the future. Over time, we will also expose this traffic shifting functionality directly to ELB customers so that they have the ability to control the routing of their requests to the Availability Zones in which they run their applications. Finally, we will work on helping our customers understand and test the impact of"}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "away from a degraded Availability Zone in the future. Over time, we will also expose this traffic shifting functionality directly to ELB customers so that they have the ability to control the routing of their requests to the Availability Zones in which they run their applications. Finally, we will work on helping our customers understand and test the impact of this traffic shift so that they can be sure their applications can scale to handle the increased load caused by failing away from an Availability Zone."}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "Final Thoughts"}
{"title": "Summary of the October 22, 2012 AWS Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680342/", "snippet": "We apologize for the inconvenience and trouble this caused for affected customers. We know how critical our services are to our customers\u2019 businesses, and will work hard (and expeditiously) to apply the learning from this event to our services. While we saw that some of the changes that we previously made helped us mitigate some of the impact, we also learned about new failure modes. We will spend many hours over the coming days and weeks improving our understanding of the event and further investing in the resiliency of our services."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "Early Sunday morning, September 20, we had a DynamoDB service event in the US-East Region that impacted DynamoDB customers in US-East, as well as some other services in the region. The following are some additional details on the root cause, subsequent impact to other AWS services that depend on DynamoDB, and corrective actions we\u2019re taking."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "Some DynamoDB Context"}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "Among its many functions, DynamoDB stores and maintain tables for customers. A single DynamoDB table is separated into partitions, each containing a portion of the table\u2019s data. These partitions are spread onto many servers, both to provide consistent low-latency access and to replicate the data for durability."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "The specific assignment of a group of partitions to a given server is called a \u201cmembership.\u201d The membership of a set of table/partitions within a server is managed by DynamoDB\u2019s internal metadata service. The metadata service is internally replicated and runs across multiple datacenters. Storage servers hold the actual table data within a partition and need to periodically confirm that they have the correct membership. They do this by checking in with the metadata service and asking for their current membership assignment. In response, the metadata service retrieves the list of partitions and all related information from its own store, bundles this up into a message, and transmits back to the requesting storage server. A storage server will also get its membership assignment after a network disruption or on startup. Once a storage server has completed processing its membership assignment, it verifies that it has the table/partition data locally stored, creates any new table/partitions assigned, and retrieves data from other storage servers to replicate existing partitions assigned."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "The DynamoDB Event"}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "On Sunday, at 2:19am PDT, there was a brief network disruption that impacted a portion of DynamoDB\u2019s storage servers. Normally, this type of networking disruption is handled seamlessly and without change to the performance of DynamoDB, as affected storage servers query the metadata service for their membership, process any updates, and reconfirm their availability to accept requests. If the storage servers aren\u2019t able to retrieve this membership data back within a specific time period, they will retry the membership request and temporarily disqualify themselves from accepting requests."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "But, on Sunday morning, a portion of the metadata service responses exceeded the retrieval and transmission time allowed by storage servers. As a result, some of the storage servers were unable to obtain their membership data, and removed themselves from taking requests. The reason these metadata service requests were taking too long relates to a recent development in DynamoDB. Over the last few months, customers have rapidly adopted a new DynamoDB feature called Global Secondary Indexes (\u201cGSIs\u201d). GSIs allow customers to access their table data using alternate keys. Because GSIs are global, they have their own set of partitions on storage servers and therefore increase the overall size of a storage server\u2019s membership data. Customers can add multiple GSIs for a given table, so a table with large numbers of partitions could have its contribution of partition data to the membership lists quickly double or triple. With rapid adoption of GSIs by a number of customers with very large tables, the partitions-per-table ratio increased significantly. This, in turn, increased the size of some storage servers\u2019 membership lists significantly. With a larger size, the processing time inside the metadata service for some membership requests began to approach the retrieval time allowance by storage servers. We did not have detailed enough monitoring for this dimension (membership size), and didn\u2019t have enough capacity allocated to the metadata service to handle these much heavier requests."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "So, when the network disruption occurred on Sunday morning, and a number of storage servers simultaneously requested their membership data, the metadata service was processing some membership lists that were now large enough that their processing time was near the time limit for retrieval. Multiple, simultaneous requests for these large memberships caused processing to slow further and eventually exceed the allotted time limit. This resulted in the disrupted storage servers failing to complete their membership renewal, becoming unavailable for requests, and retrying these requests. With the metadata service now under heavy load, it also no longer responded as quickly to storage servers uninvolved in the original network disruption, who were checking their membership data in the normal cadence of when they retrieve this information. Many of those storage servers also became unavailable for handling customer requests. Unavailable servers continued to retry requests for membership data, maintaining high load on the metadata service. Though many storage servers\u2019 renewal requests were succeeding, healthy storage servers that had successfully processed a membership request previously were having subsequent renewals fail and were transitioning back to an unavailable state. By 2:37am PDT, the error rate in customer requests to DynamoDB had risen far beyond any level experienced in the last 3 years, finally stabilizing at approximately 55%."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "Initially, we were unable to add capacity to the metadata service because it was under such high load, preventing us from successfully making the requisite administrative requests. After several failed attempts at adding capacity, at 5:06am PDT, we decided to pause requests to the metadata service. This action decreased retry activity, which relieved much of the load on the metadata service. With the metadata service now able to respond to administrative requests, we were able to add significant capacity. Once these adjustments were made, we were able to reactivate requests to the metadata service, put storage servers back into the customer request path, and allow normal load back on the metadata service. At 7:10am PDT, DynamoDB was restored to error rates low enough for most customers and AWS services dependent on DynamoDB to resume normal operations."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "There\u2019s one other bit worth mentioning. After we resolved the key issue on Sunday, we were left with a low error rate, hovering between 0.15%-0.25%. We knew there would be some cleanup to do after the event, and while this rate was higher than normal, it wasn\u2019t a rate that usually precipitates a dashboard post or creates issues for customers. As Monday progressed, we started to get more customers opening support cases about being impacted by tables being stuck in the updating or deleting stage or higher than normal error rates. We did not realize soon enough that this low overall error rate was giving some customers disproportionately high error rates. It was impacting a relatively small number of customers, but we should have posted the green-i to the dashboard sooner than we did on Monday. The issue turned out to be a metadata partition that was still not taking the amount of traffic that it should have been taking. The team worked carefully and diligently to restore that metadata partition to its full traffic volume, and closed this out on Monday."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "There are several actions we'll take immediately to avoid a recurrence of Sunday's DynamoDB event. First, we have already significantly increased the capacity of the metadata service. Second, we are instrumenting stricter monitoring on performance dimensions, such as the membership size, to allow us to thoroughly understand their state and proactively plan for the right capacity. Third, we are reducing the rate at which storage nodes request membership data and lengthening the time allowed to process queries. Finally and longer term, we are segmenting the DynamoDB service so that it will have many instances of the metadata service each serving only portions of the storage server fleet. This will further contain the impact of software, performance/capacity, or infrastructure failures."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "Impact on Other Services"}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "There are several other AWS services that use DynamoDB that experienced problems during the event. Rather than list them all, which had similar explanations for their status, we\u2019ll list a few that customers most asked us about or where the actions are more independent from DynamoDB\u2019s Correction of Errors (\u201cCOE\u201d)."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "Simple Queue Service (SQS)"}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "In the early stages of the DynamoDB event, the Amazon Simple Queue Service was delivering slightly elevated errors and latencies. Amazon SQS uses an internal table stored in DynamoDB to store information describing its queues. While the queue information is cached within SQS, and is not in the direct path for \u201csend-message\u201d and \u201creceive-message\u201d APIs, the caches are refreshed frequently to accommodate creation, deletion, and reassignment across infrastructure. When DynamoDB finished disabling traffic at 5:45am PDT (to enable the metadata service to recover), the Simple Queue Service was unable to read this data to refresh caches, resulting in significantly elevated error rates. Once DynamoDB began re-enabling customer traffic at 7:10am PDT, the Simple Queue Service recovered. No data in queues, or information describing queues was lost as a result of the event."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "In addition to the actions being taken by the DynamoDB service, we will be adjusting our SQS metadata caching to ensure that send and receive operations continue even without prolonged access to the metadata table.\n\nEC2 Auto Scaling"}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "Between 2:15am PDT and 7:10am PDT, the EC2 Auto Scaling Service delivered significantly increased API faults. From 7:10am PDT to 10:52am PDT, the Auto Scaling service was substantially delayed in bringing new instances into service, or terminating existing unhealthy instances. Existing instances continued to operate properly throughout the event."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "Auto Scaling stores information about its groups and launch configurations in an internal table in DynamoDB. When DynamoDB began to experience elevated error rates starting at 2:19am PDT, Auto Scaling could not update this internal table when APIs were called. Once DynamoDB began recovery at 7:10am PDT, the Auto Scaling APIs recovered. Recovery was incomplete at this time, as a significant backlog of scaling activities had built up throughout the event. The Auto Scaling service executes its launch and termination activities in a background scheduling service. Throughout the event, a very large amount of pending activities built up in this job scheduler and it took until 10:52am PDT to complete all of these tasks."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "In addition to the actions taken by the DynamoDB team, to ensure we can recover quickly when a large backlog of scaling activities accumulate, we will adjust the way we partition work on the fleet of Auto Scaling servers to allow for more parallelism in processing these jobs, integrate mechanisms to prune older scaling activities that have been superseded, and increase the capacity available to process scaling activities."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "CloudWatch"}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "Starting at 2:35am PDT, the Amazon CloudWatch Metrics Service began experiencing delayed and missing EC2 Metrics along with slightly elevated errors. CloudWatch uses an internal table stored in DynamoDB to add information regarding Auto Scaling group membership to incoming EC2 metrics. From 2:35am PDT to 5:45am PDT, the elevated DynamoDB failure rates caused intermittent availability of EC2 metrics in CloudWatch. CloudWatch also observed an abnormally low rate of metrics publication from other services that were experiencing issues over this time period, further contributing to missing or delayed metrics."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "Then, from approximately 5:51am PDT to 7:10am PDT CloudWatch delivered significantly elevated error rates for PutMetricData calls affecting all AWS Service metrics and custom metrics. The impact was due to the significantly elevated error rates in DynamoDB for the group membership additions mentioned above. The CloudWatch Metrics Service was fully recovered at 7:29am PDT."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "We understand how important metrics are, especially during an event. To further increase the resilience of CloudWatch, we will adjust our caching strategy for the DynamoDB group membership data and only require refresh for the smallest possible set of metrics. We also have been developing faster metrics delivery through write-through caching. This cache will provide the ability to present metrics directly before persisting them and will, as a side benefit, provide additional protection during an event."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "Console"}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "The AWS Console was impacted for some customers from 5:45am PDT to 7:10am PDT. Customers who were already logged into the Console would have continued to remain connected. Customers attempting to log into the Console during this period saw much higher latency in the login process. This was due to a very long timeout being set on an API call that relied on DynamoDB. The API call did not have to complete successfully to allow login to proceed but, with the long timeout, it blocked progress for tens of seconds while it waited to finish. It should have simply failed quickly and allowed progress on login to continue."}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "The timeout had already been changed in a version of the login code that has entered our test process. Unfortunately it wasn\u2019t yet rolled out when the event happened. We will make this change in the coming days. The reduced timeout will mitigate any impact of latency in the API call on the Console.\n\nFinal Words"}
{"title": "Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region", "url": "https://aws.amazon.com/message/5467D2/", "snippet": "We apologize for the impact to affected customers. While we are proud of the last three years of availability on DynamoDB (it\u2019s effectively been 100%), we know how critical this service is to customers, both because many use it for mission-critical operations and because AWS services also rely on it. For us, availability is the most important feature of DynamoDB, and we will do everything we can to learn from the event and to avoid a recurrence in the future."}
{"title": "GitHub App authentication token issuance degradation due to load", "url": "https://github.blog/2023-05-16-addressing-githubs-recent-availability-issues/#may-10-github-app-auth-token-incident", "snippet": "On May 10, the database cluster serving GitHub App auth tokens saw a 7x increase in write latency for GitHub App permissions (status yellow). The failure rate of these auth token requests was 8-15% for the majority of this incident, but did peak at 76% percent for a short time."}
{"title": "GitHub App authentication token issuance degradation due to load", "url": "https://github.blog/2023-05-16-addressing-githubs-recent-availability-issues/#may-10-github-app-auth-token-incident", "snippet": "We determined that an API for managing GitHub App permissions had an inefficient implementation. When invoked under specific circumstances, it results in very large writes and a timeout failure. This API was invoked by a new caller that retried on timeouts, triggering the incident. While working to identify root cause, improve the data access pattern, and address the source of the new call pattern, we also took steps to reduce load from both internal and external paths, reducing impact to critical paths like GitHub Actions workflows. After recovery, we re-enabled all suspended sources before statusing green."}
{"title": "GitHub App authentication token issuance degradation due to load", "url": "https://github.blog/2023-05-16-addressing-githubs-recent-availability-issues/#may-10-github-app-auth-token-incident", "snippet": "While we update the backing data model to avoid this pattern entirely, we are updating the API to check for the shift in installation state and will fail the request if it would trigger these large writes as a temporary measure."}
{"title": "GitHub App authentication token issuance degradation due to load", "url": "https://github.blog/2023-05-16-addressing-githubs-recent-availability-issues/#may-10-github-app-auth-token-incident", "snippet": "Beyond the problem with the query performance, much of our observability is optimized for identifying high-volume patterns, not low-volume high-cost ones, which made it difficult to identify the specific circumstances that were causing degraded cluster health. Moving forward, we are prioritizing work to apply the experiences of our investigations during this incident to ensure we have quick and clear answers for similar cases in the future."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "We would like to share more details with our customers about the events that occurred with Amazon Elastic Compute Cloud (\u201cEC2\u201d), Amazon Elastic Block Store (\u201cEBS\u201d), and Amazon Relational Database Service (\u201cRDS\u201d) earlier this week, and what we are doing to prevent these sorts of issues from happening again. The service disruption primarily affected EC2 instances, RDS instances, and a subset of EBS volumes in a single Availability Zone in the EU West Region."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "Primary Outage"}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "The service disruption began at 10:41 AM PDT on August 7th when our utility provider suffered a failure of a 110kV 10 megawatt transformer. This failure resulted in a total loss of electricity supply to all of their customers connected to this transformer, including a significant portion of the affected AWS Availability Zone. The initial fault diagnosis from our utility provider indicated that a lightning strike caused the transformer to fail. The utility provider now believes it was not a lightning strike, and is continuing to investigate root cause."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "Normally, when utility power fails, electrical load is seamlessly picked up by backup generators. Programmable Logic Controllers (PLCs) assure that the electrical phase is synchronized between generators before their power is brought online. In this case, one of the PLCs did not complete the connection of a portion of the generators to bring them online. We currently believe (supported by all observations of the state and behavior of this PLC) that a large ground fault detected by the PLC caused it to fail to complete its task. We are working with our supplier and performing further analysis of the device involved to confirm. With no utility power, and backup generators for a large portion of this Availability Zone disabled, there was insufficient power for all of the servers in the Availability Zone to continue operating. Uninterruptable Power Supplies (UPSs) that provide a short period of battery power quickly drained and we lost power to almost all of the EC2 instances and 58% of the EBS volumes in that Availability Zone. We also lost power to the EC2 networking gear that connects this Availability Zone to the Internet and connects this Availability Zone to the other Availability Zones in the Region. This caused connectivity issues to the affected Availability Zone and resulted in API errors when customers targeted API requests (RunInstance, CreateVolume, etc.) to the impacted Availability Zone."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "At 11:05 AM PDT, we were seeing launch delays and API errors in all EU West Availability Zones. There were two primary factors that contributed to this. First, our EC2 management service (which handles API requests to RunInstance, CreateVolume, etc.), has servers in each Availability Zone. The management servers which receive requests continued to route requests to management servers in the affected Availability Zone. Because the management servers in the affected Availability Zone were inaccessible, requests routed to those servers failed. Second, the EC2 management servers receiving requests were continuing to accept RunInstances requests targeted at the impacted Availability Zone. Rather than failing these requests immediately, they were queued and our management servers attempted to process them. Fairly quickly, a large number of these requests began to queue up and we overloaded the management servers receiving requests, which were waiting for these queued requests to complete. The combination of these two factors caused long delays in launching instances and higher error rates for the EU West EC2 APIs. At 12:00 PM PDT, when we disabled EC2 launches in the affected Availability Zone and removed the failed management servers from service, EC2 API launch times for other Availability Zones recovered."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "At 11:54 AM PDT, we had been able to bring some of the backup generators online by manually phase-synchronizing the power sources. This restored power to many of the EC2 instances and EBS volumes, but a majority of the networking gear was still without power, so these restored instances were still inaccessible. By 1:49 PM PDT, power had been restored to enough of our network devices that we were able to re-establish connectivity to the Availability Zone. Many of the instances and volumes in the Availability Zone became accessible at this time."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "Recovering EBS in the Affected Availability Zone"}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "To understand why restoration of EBS took longer, it\u2019s helpful to understand a little about the EBS architecture. EBS volume data is replicated across a set of EBS nodes for durability and availability. These nodes serve read and write requests to EC2 instances. If one node loses connectivity to another node that it is replicating data to, it must find and replicate its data to a new node (this is called re-mirroring)-- and it will block writes until it has found that new node. From the perspective of an EC2 instance trying to do I/O on an EBS volume that is blocking writes, the volume will appear \u201cstuck.\u201d"}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "On Sunday, when a large portion of the EBS servers lost power and shut down, EBS volumes in the affected Availability Zone entered one of three states: (1) online \u2013 none of the nodes holding a volume\u2019s data lost power, (2) re-mirroring \u2013 a subset of the nodes storing the volume were offline due to power loss and the remaining nodes were re-replicating their data, and (3) offline \u2013 all nodes lost power."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "In the first case, EBS volumes continued to function normally."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "In the second case, the majority of nodes were able to leverage the significant amount of spare capacity in the affected Availability Zone, successfully re-mirror, and enable the volume to recover. But, because we had such an unusually large number of EBS volumes lose power, the spare capacity we had on hand to support re-mirroring wasn\u2019t enough. We ran out of spare capacity before all of the volumes were able to successfully re-mirror. As a result, a number of customers\u2019 volumes became \u201cstuck\u201d as they attempted to write to their volume, but their volume had not yet found a new node to receive a replica. In order to get the \u201cstuck\u201d volumes back online, we had to add more capacity. We brought in additional labor to get more onsite capacity online and trucked in servers from another Availability Zone in the Region. There were delays as this was nighttime in Dublin and the logistics of trucking required mobilizing transportation some distance from the datacenter. Once the additional capacity was available, we were able to recover the remaining volumes waiting for space to complete a successful re-mirror."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "In the third case, when an EC2 instance and all nodes containing EBS volume replicas concurrently lose power, we cannot verify that all of the writes to all of the nodes are completely consistent. If we cannot confirm that all writes have been persisted to disk, then we cautiously assume that the volume is in an inconsistent state (even though in many cases the volume is actually consistent). Bringing a volume back in an inconsistent state without the customer being aware could cause undetectable, latent data corruption issues which could trigger a serious impact later. For the volumes we assumed were inconsistent, we produced a recovery snapshot to enable customers to create a new volume and check its consistency before trying to use it. The process of producing recovery snapshots was time-consuming because we had to first copy all of the data from each node to Amazon Simple Storage Service (Amazon S3), process that data to turn it into the snapshot storage format, and re-copy the data to make it accessible from a customer\u2019s account. Many of the volumes contained a lot of data (EBS volumes can hold as much as 1 TB per volume). By 6:04 AM PDT on August 9th, we had delivered approximately 38% of the recovery snapshots for these potentially inconsistent volumes to customers. By 2:37 AM PDT on August 10th, 85% of the recovery snapshots had been delivered. By 8:25 PM PDT on August 10th, we were 98% complete, with the remaining few snapshots requiring manual attention."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "Impact on Amazon RDS"}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "RDS Instances were also affected by the disruption. RDS database instances utilize EBS volumes for database and log storage. As a result, the power outage in the affected Availability Zone had significant impact on RDS as well. Single Availability Zone (\u201cSingle-AZ\u201d) database instances in the affected Availability Zone were almost all initially unavailable. They recovered when their corresponding EBS volumes were restored or their databases were restored to new volumes. All Amazon RDS Single-AZ database instances have automated backups turned on by default. The majority of customers whose databases did not recover when the first tranche of EBS volumes came back online, or could not be recovered due to inconsistency of their volumes, used this backup functionality to initiate Point-in-Time-Restore operations, per our Service Health Dashboard instructions. Customers with automated backups turned off, could not initiate Point-in-Time-Restores."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "In addition to impacting Single-AZ database instances, the severity of the event and nature of failure also caused a portion of Multiple Availability Zone (\u201cMulti-AZ\u201d) database instances to be impacted. Rapid failover occurred for the vast majority of Multi-AZ databases, and all affected Multi-AZ databases in the EU-West Region failed over without data loss. However, a portion of Multi-AZ database instances experienced prolonged failover times."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "To understand why some Multi-AZ database instances did not promptly failover, it is useful to understand how Multi-AZ databases work. RDS Multi-AZ database instances consist of a \u201cprimary\u201d database instance and a synchronously replicated \u201csecondary\u201d database instance in another Availability Zone. When the system detects that a primary database instance might be failing, upon verification via a health check that the primary is no longer accepting traffic, the secondary is promoted to primary. This verification is important to avoid a \u201csplit brain\u201d situation, one where both the primary and the secondary database instances are accepting writes and some writes exist on one database while some exist on another. Similarly, when the system detects that a secondary database instance is failing, upon performing the health check and verifying that the secondary hasn\u2019t assumed the role of primary, the primary will allow itself to continue as a Single-AZ database instance until a new secondary is established and connected to the primary, bringing the pair back into Multi-AZ status."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "During the event, there were failures of Multi-AZ primary database instances in the affected Availability Zone. However, for a portion of these Multi-AZ primary-secondary pairs, a DNS connectivity issue related to the power loss prevented the health check from finding the IP address it needed to contact and kept the secondary from immediately assuming the role of the primary. DNS connectivity was restored within 4 minutes, and the majority of Multi-AZ deployments then completed failover within an additional 10 minutes. However, the DNS connectivity issues triggered a software bug that caused failover times to the secondary database instance to extend significantly for a small subset of Multi-AZ deployments."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "This DNS connectivity issue also triggered extended failover times for a small portion of Multi-AZ deployments with secondary replicas in the affected Availability Zone. For these deployments, DNS connectivity prevented the primary replicas from confirming their secondary replica\u2019s status. In the rare case where the status of the secondary cannot be determined, the primary does not make itself a Single AZ-mode database instance and instead immediately involves the RDS team. This cautious approach is taken to help prevent the \u201csplit brain\u201d scenario described above. Instead, an RDS engineer makes the decision to either promote the secondary to primary (if the old primary is not functioning), or to move the primary to Single-AZ mode (if the secondary is not functioning). This approach minimizes the risk of data loss in edge cases, but extends the period of time the Multi-AZ instance is unavailable."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "EBS Software Bug Impacting Snapshots"}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "Separately, and independent from issues emanating from the power disruption, we discovered an error in the EBS software that cleans up unused storage for snapshots after customers have deleted an EBS snapshot. An EBS snapshot contains a set of pointers to blocks of data, including the blocks shared between multiple snapshots. Each time a new snapshot is taken of an EBS volume, only the data that has been modified since the last snapshot is pushed to S3. When a snapshot is deleted, only the blocks not referenced by later snapshots should be deleted. A cleanup process runs periodically to identify all blocks that are no longer included in any snapshots. This snapshot cleanup identification process builds a list of the blocks included in the deleted customer snapshots, a list of blocks referenced by active EBS volumes, and a list of blocks referenced by other snapshots. Blocks that are referenced by active volumes or snapshots are removed from the list of blocks to cleanup."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "The resulting cleanup list is saved, but not acted upon. At least one week passes from the time the snapshot cleanup identification process runs before any blocks it has flagged for deletion are allowed to be removed. Each day, it updates the lists of blocks to delete, blocks referenced by active volumes, and blocks referenced by other snapshots. It also compares its updated lists to the prior day\u2019s and if any block eligible for deletion the day before now shows up in the most recent list of blocks referenced by active EBS volumes or snapshots, the process flags those blocks for analysis. Typically, there are very few, if any, items that get flagged for analysis. But, this part of the process was introduced to protect against system or software errors that could result in blocks falsely flagged for deletion. Actual deletion is executed by an engineer who first, before running the actual deletion process, evaluates the blocks flagged for analysis and verifies that there are no blocks in the list scheduled to be deleted that have been flagged for analysis. The engineer must present their verification step to another engineer who approves the deletion."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "In one of the days leading up to the Friday, August 5th deletion run, there was a hardware failure that the snapshot cleanup identification software did not correctly detect and handle. The result was that the list of snapshot references used as input to the cleanup process was incomplete. Because the list of snapshot references was incomplete, the snapshot cleanup identification process incorrectly believed a number of blocks were no longer referenced and had flagged those blocks for deletion even though they were still referenced by customer snapshots. A subsequent run of the snapshot cleanup identification process detected the error and flagged blocks for further analysis that had been incorrectly scheduled for deletion. On August 5th, the engineer running the snapshot deletion process checked the blocks flagged for analysis before running the actual deletion process in the EU West Region. The human checks in this process failed to detect the error and the deletion process was executed. On Friday evening, an error accessing one of the affected snapshots triggered us to investigate."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "By Sunday morning, August 7th, we had completed the work to fully understand root cause, prevent the problem from recurring, and build a tool that could create recovery snapshots for affected snapshots. We then started to do the work necessary to map these affected snapshots to customers and build the recovery snapshots, with the aim to communicate this information to customers by Sunday night. However, before we got very far in this endeavor, the power event began. We had to temporarily stop work on the snapshot issue to respond to the power event. Once we\u2019d been able to restore the majority of the EBS volumes affected by the power event, we returned to working on the snapshot issue in parallel with restoring the remainder of the EBS volumes that were recovering from the power event. By 4:19 PM PDT on August 8th, we\u2019d completed creating recovery snapshots for all affected snapshots, delivered them to customers\u2019 accounts, and communicated about the issue on the Service Health Dashboard."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "Actions to Prevent Recurrence\n\nThere are several actions we intend to take to protect against a similar occurrence. The following are some of the key ones."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "To further prevent the loss of power, we will add redundancy and more isolation for our PLCs so they are insulated from other failures. Specifically, in addition to correcting the isolation of the primary PLC, a cold, environmentally isolated backup PLC is being worked with our vendors. We will deploy this as rapidly as possible."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "For EC2, we are going to address the resource saturation that affected API calls at the beginning of the disruption. We will implement better load balancing to quickly take failed API management service hosts out of production. Over the last few months, we have been developing further isolation of EC2 control plane components (i.e. the APIs) to eliminate possible latency or failure in one Availability Zone from impacting our ability to process calls to other Availability Zones. While some of those mitigations significantly reduced the impact of this disruption and helped us recover the APIs quickly, we realize how important those APIs are to customers, especially during an event. It will take us several more months to complete some of the changes we\u2019re making, and we will test and roll out these changes carefully. At the time of the disruption, customers who had EC2 instances and EBS volumes independently operating in multiple EU West Region Availability Zones did not experience service interruption. We will continue to create additional capabilities that make it easier to develop and deploy applications in multiple Availability Zones."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "For EBS, our primary action will be to drastically reduce the long recovery time required to recover stuck or inconsistent EBS volumes when there is a substantial infrastructure disruption. While some volumes were recoverable immediately once we had power back, there was an extended period of time for many volumes to recover due to the need to create EBS snapshots within S3. As we described above, this long period of delay was caused by the time required to move a very large amount of data into S3 and then transfer that data to EBS recovery snapshots. To significantly reduce the time required to restore these volumes, we will create the capability to recover volumes directly on the EBS servers upon restoration of power, without having to move the data off of those servers. This will require providing a way for customers to know that a volume has been shut down and restored, but will avoid the need for restoration via snapshot. This will also substantially diminish any risk associated with lack of capacity, regardless of how many volumes fail."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "We\u2019ve made changes to our deletion process to prevent recurrence of the EBS software bug impacting snapshots. We are instrumenting an alarm that will alert us if there are any unusual situations discovered by the snapshot cleanup identification process, including blocks falsely flagged as being unreferenced. We\u2019re also adding another holding state for blocks flagged for deletion where they are logically unavailable but retrievable for an additional, longer period of time. This will provide additional time to discover and correct any problem without loss of data."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "We learned a number of lessons from this event that we will use to continually improve the reliability of RDS Multi-AZ deployments. First, we will implement changes to our health checks to avoid customer impact in the event of a unique DNS connectivity issue like we experienced here. Second, we will promptly fix the software bug that extended failover times for a portion of Multi-AZ customers with primaries in the affected Availability Zone. Third, we will implement an improved handling of the edge case where either primary or secondary is down and the health check cannot complete. In such a case, the successfully running member of the Multi-AZ pair will initiate connection retries to confirm it is no longer in a \u201csplit brain\u201d mode, such that involving an engineer might not be necessary."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "Improving Communication"}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "Communication in situations like this is difficult. Customers are understandably anxious about the timing for recovery and what they should do in the interim. We always prioritize getting affected customers back to health as soon as possible, and that was our top priority in this event, too. But, we know how important it is to communicate on the Service Health Dashboard and AWS Support mechanisms. Based on prior customer feedback, we communicated more frequently during this event on our Service Health Dashboard than we had in other prior events, we had evangelists tweet links to key early dashboard updates, we staffed up our AWS Support team to handle much higher forum and Premium Support contacts, and we tried to give an approximate time-frame early on for when the people with extra-long delays could expect to start seeing recovery. Still, we know what matters most to customers in circumstances like this is knowing the status of their resources, when the impacted ones will be healthy, and what they should do now. While we provided best estimates for the long-lead recovery snapshots, we truly didn\u2019t know how long that process was going to take or we would have shared it. For those waiting for recovery snapshots, we tried to communicate what was possible. If customers were architected to operate across multiple Availability Zones, they could flip over to and/or deploy resources in other Availability Zones. If customers were architected such that spinning up new instances or volumes in the same Availability Zone worked, they could do that. But, for those single Availability Zone customers who needed a specific EBS volume to recover, and whose EBS volume was in the group waiting for recovery snapshots, there were really no short term actions possible."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "There are several places we can improve on the communication front. First, we can accelerate the pace with which we staff up our Support team to be even more responsive in the early hours of an event. Second, we will do a better job of making it easier for customers (and AWS) to tell if their resources have been impacted. This will give customers (and AWS) important shared telemetry on what\u2019s happening to specific resources in the heat of the moment. We\u2019ve been hard at work on developing tools to allow you to see via the APIs if your instances/volumes are impaired, and hope to have this to customers in the next few months. Third, as we were sending customers recovery snapshots, we could have been clearer and more instructive on how to run the recovery tools, and provided better detail on the recovery actions customers could have taken. We sometimes assume a certain familiarity with these tools that we should not."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "Service Credit for Affected Customers"}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "For customers with an attached EBS volume or a running RDS database instance in the affected Availability Zone in the EU West Region at the time of the disruption, regardless of whether their resources and application were impacted or not, we are going to provide a 10 day credit equal to 100% of their usage of EBS Volumes, EC2 Instances and RDS database instances that were running in the affected Availability Zone in the EU West region. Additionally, any customers impacted by the EBS software bug that accidentally deleted blocks in their snapshots will receive a 30 day credit for 100% of their EBS usage in the entire EU West Region (inclusive of snapshot storage and requests as well as volume storage and I/O). These customers will also have access to our Premium Support Engineers (via the AWS Support Center) if these customers need any additional technical assistance in recovering from this issue."}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "These customers will not have to do anything in order to receive the credits, as they will be automatically applied to customers\u2019 next AWS bill. The credits can also be viewed as they become available over the next few weeks by logging into the AWS Account Activity page.\n\nSummary"}
{"title": "Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region", "url": "https://aws.amazon.com/message/2329B7/", "snippet": "Last, but certainly not least, we want to apologize. We know how critical our services are to our customers\u2019 businesses. We will do everything we can to learn from this event and use it to drive improvement across our services. As with any significant operational issue, we will spend many hours over the coming days and weeks improving our understanding of the details of the various parts of this event and determining how to make changes to improve our services and processes."}
{"title": "Summary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/11201/", "snippet": "We wanted to provide you with some additional information about the service disruption that occurred in the Northern Virginia (US-EAST-1) Region on November 25th, 2020."}
{"title": "Summary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/11201/", "snippet": "Amazon Kinesis enables real-time processing of streaming data. In addition to its direct use by customers, Kinesis is used by several other AWS services. These services also saw impact during the event. The trigger, though not root cause, for the event was a relatively small addition of capacity that began to be added to the service at 2:44 AM PST, finishing at 3:47 AM PST. Kinesis has a large number of \u201cback-end\u201d cell-clusters that process streams. These are the workhorses in Kinesis, providing distribution, access, and scalability for stream processing. Streams are spread across the back-end through a sharding mechanism owned by a \u201cfront-end\u201d fleet of servers. A back-end cluster owns many shards and provides a consistent scaling unit and fault-isolation. The front-end\u2019s job is small but important. It handles authentication, throttling, and request-routing to the correct stream-shards on the back-end clusters."}
{"title": "Summary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/11201/", "snippet": "The capacity addition was being made to the front-end fleet. Each server in the front-end fleet maintains a cache of information, including membership details and shard ownership for the back-end clusters, called a shard-map. This information is obtained through calls to a microservice vending the membership information, retrieval of configuration information from DynamoDB, and continuous processing of messages from other Kinesis front-end servers. For the latter communication, each front-end server creates operating system threads for each of the other servers in the front-end fleet. Upon any addition of capacity, the servers that are already operating members of the fleet will learn of new servers joining and establish the appropriate threads. It takes up to an hour for any existing front-end fleet member to learn of new participants."}
{"title": "Summary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/11201/", "snippet": "At 5:15 AM PST, the first alarms began firing for errors on putting and getting Kinesis records. Teams engaged and began reviewing logs. While the new capacity was a suspect, there were a number of errors that were unrelated to the new capacity and would likely persist even if the capacity were to be removed. Still, as a precaution, we began removing the new capacity while researching the other errors. The diagnosis work was slowed by the variety of errors observed. We were seeing errors in all aspects of the various calls being made by existing and new members of the front-end fleet, exacerbating our ability to separate side-effects from the root cause. At 7:51 AM PST, we had narrowed the root cause to a couple of candidates and determined that any of the most likely sources of the problem would require a full restart of the front-end fleet, which the Kinesis team knew would be a long and careful process. The resources within a front-end server that are used to populate the shard-map compete with the resources that are used to process incoming requests. So, bringing front-end servers back online too quickly would create contention between these two needs and result in very few resources being available to handle incoming requests, leading to increased errors and request latencies. As a result, these slow front-end servers could be deemed unhealthy and removed from the fleet, which in turn, would set back the recovery process. All of the candidate solutions involved changing every front-end server\u2019s configuration and restarting it. While the leading candidate (an issue that seemed to be creating memory pressure) looked promising, if we were wrong, we would double the recovery time as we would need to apply a second fix and restart again. To speed restart, in"}
{"title": "Summary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/11201/", "snippet": "set back the recovery process. All of the candidate solutions involved changing every front-end server\u2019s configuration and restarting it. While the leading candidate (an issue that seemed to be creating memory pressure) looked promising, if we were wrong, we would double the recovery time as we would need to apply a second fix and restart again. To speed restart, in parallel with our investigation, we began adding a configuration to the front-end servers to obtain data directly from the authoritative metadata store rather than from front-end server neighbors during the bootstrap process."}
{"title": "Summary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/11201/", "snippet": "At 9:39 AM PST, we were able to confirm a root cause, and it turned out this wasn\u2019t driven by memory pressure. Rather, the new capacity had caused all of the servers in the fleet to exceed the maximum number of threads allowed by an operating system configuration. As this limit was being exceeded, cache construction was failing to complete and front-end servers were ending up with useless shard-maps that left them unable to route requests to back-end clusters. We didn\u2019t want to increase the operating system limit without further testing, and as we had just completed the removal of the additional capacity that triggered the event, we determined that the thread count would no longer exceed the operating system limit and proceeded with the restart. We began bringing back the front-end servers with the first group of servers taking Kinesis traffic at 10:07 AM PST. The front-end fleet is composed of many thousands of servers, and for the reasons described earlier, we could only add servers at the rate of a few hundred per hour. We continued to slowly add traffic to the front-end fleet with the Kinesis error rate steadily dropping from noon onward. Kinesis fully returned to normal at 10:23 PM PST."}
{"title": "Summary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/11201/", "snippet": "For Kinesis, we have a number of learnings that we will be implementing immediately. In the very short term, we will be moving to larger CPU and memory servers, reducing the total number of servers and, hence, threads required by each server to communicate across the fleet. This will provide significant headroom in thread count used as the total threads each server must maintain is directly proportional to the number of servers in the fleet. Having fewer servers means that each server maintains fewer threads. We are adding fine-grained alarming for thread consumption in the service. We will also finish testing an increase in thread count limits in our operating system configuration, which we believe will give us significantly more threads per server and give us significant additional safety margin there as well. In addition, we are making a number of changes to radically improve the cold-start time for the front-end fleet. We are moving the front-end server cache to a dedicated fleet. We will also move a few large AWS services, like CloudWatch, to a separate, partitioned front-end fleet. In the medium term, we will greatly accelerate the cellularization of the front-end fleet to match what we\u2019ve done with the back-end. Cellularization is an approach we use to isolate the effects of failure within a service, and to keep the components of the service (in this case, the shard-map cache) operating within a previously tested and operated range. This had been under way for the front-end fleet in Kinesis, but unfortunately the work is significant and had not yet been completed. In addition to allowing us to operate the front-end in a consistent and well-tested range of total threads consumed, cellularization will provide better protection against any future unknown scaling limit."}
{"title": "Summary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/11201/", "snippet": "There were a number of services that use Kinesis that were impacted as well. Amazon Cognito uses Kinesis Data Streams to collect and analyze API access patterns. While this information is extremely useful for operating the Cognito service, this information streaming is designed to be best effort. Data is buffered locally, allowing the service to cope with latency or short periods of unavailability of the Kinesis Data Stream service. Unfortunately, the prolonged issue with Kinesis Data Streams triggered a latent bug in this buffering code that caused the Cognito webservers to begin to block on the backlogged Kinesis Data Stream buffers. As a result, Cognito customers experienced elevated API failures and increased latencies for Cognito User Pools and Identity Pools, which prevented external users from authenticating or obtaining temporary AWS credentials. In the early stages of the event, the Cognito team worked to mitigate the impact of the Kinesis errors by adding additional capacity and thereby increasing their capacity to buffer calls to Kinesis. While this initially reduced impact, by 7:01 AM PST errors rates increased significantly. The team was working in parallel on a change to Cognito to reduce the dependency on Kinesis. At 10:15 AM PST, deployment of this change began and error rates began falling. By 12:15 PM PST, error rates were significantly reduced, and by 2:18 PM PST Cognito was operating normally. To prevent a recurrence of this issue, we have modified the Cognito webservers so that they can sustain Kinesis API errors without exhausting their buffers that resulted in these user errors."}
{"title": "Summary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/11201/", "snippet": "CloudWatch uses Kinesis Data Streams for the processing of metric and log data. Starting at 5:15 AM PST, CloudWatch experienced increased error rates and latencies for the PutMetricData and PutLogEvents APIs, and alarms transitioned to the INSUFFICIENT_DATA state. While some CloudWatch metrics continued to be processed throughout the event, the increased error rates and latencies prevented the vast majority of metrics from being successfully processed. At 5:47 PM PST, CloudWatch began to see early signs of recovery as Kinesis Data Stream\u2019s availability improved, and by 10:31 PM PST, CloudWatch metrics and alarms fully recovered. Delayed metrics and log data backfilling completed over the subsequent hours. While CloudWatch was experiencing these increased errors, both internal and external clients were unable to persist all metric data to the CloudWatch service. These errors will manifest as gaps in data in CloudWatch metrics. While CloudWatch currently relies on Kinesis for its complete metrics and logging capabilities, the CloudWatch team is making a change to persist 3-hours of metric data in the CloudWatch local metrics data store. This change will allow CloudWatch users, and services requiring CloudWatch metrics (including AutoScaling), to access these recent metrics directly from the CloudWatch local metrics data store. This change has been completed in the US-EAST-1 Region and will be deployed globally in the coming weeks."}
{"title": "Summary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/11201/", "snippet": "Two services were also impacted as a result of the issues with CloudWatch metrics. First, reactive AutoScaling policies that rely on CloudWatch metrics experienced delays until CloudWatch metrics began to recover at 5:47 PM PST. And second, Lambda saw impact. Lambda function invocations currently require publishing metric data to CloudWatch as part of invocation. Lambda metric agents are designed to buffer metric data locally for a period of time if CloudWatch is unavailable. Starting at 6:15 AM PST, this buffering of metric data grew to the point that it caused memory contention on the underlying service hosts used for Lambda function invocations, resulting in increased error rates. At 10:36 AM PST, engineers took action to mitigate the memory contention, which resolved the increased error rates for function invocations."}
{"title": "Summary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/11201/", "snippet": "CloudWatch Events and EventBridge experienced increased API errors and delays in event processing starting at 5:15 AM PST. As Kinesis availability improved, EventBridge began to deliver new events and slowly process the backlog of older events. Elastic Container Service (ECS) and Elastic Kubernetes Service (EKS) both make use of EventBridge to drive internal workflows used to manage customer clusters and tasks. This impacted provisioning of new clusters, delayed scaling of existing clusters, and impacted task de-provisioning. By 4:15 PM PST, the majority of these issues had been resolved."}
{"title": "Summary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/11201/", "snippet": "Outside of the service issues, we experienced some delays in communicating service status to customers during the early part of this event. We have two ways of communicating during operational events \u2013 the Service Health Dashboard, which is our public dashboard to alert all customers of broad operational issues, and the Personal Health Dashboard, which we use to communicate directly with impacted customers. With an event such as this one, we typically post to the Service Health Dashboard. During the early part of this event, we were unable to update the Service Health Dashboard because the tool we use to post these updates itself uses Cognito, which was impacted by this event. We have a back-up means of updating the Service Health Dashboard that has minimal service dependencies. While this worked as expected, we encountered several delays during the earlier part of the event in posting to the Service Health Dashboard with this tool, as it is a more manual and less familiar tool for our support operators. To ensure customers were getting timely updates, the support team used the Personal Health Dashboard to notify impacted customers if they were impacted by the service issues. We also posted a global banner summary on the Service Health Dashboard to ensure customers had broad visibility into the event. During the remainder of event, we continued using a combination of the Service Health Dashboard, both with global banner summaries and service specific details, while also continuing to update impacted customers via Personal Health Dashboard. Going forward, we have changed our support training to ensure that our support engineers are regularly trained on the backup tool for posting to the Service Health Dashboard."}
{"title": "Summary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/11201/", "snippet": "Finally, we want to apologize for the impact this event caused for our customers. While we are proud of our long track record of availability with Amazon Kinesis, we know how critical this service, and the other AWS services that were impacted, are to our customers, their applications and end users, and their businesses. We will do everything we can to learn from this event and use it to improve our availability even further."}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "What happened?"}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Starting at approximately 10:30 UTC on 30 August 2023, customers may have experienced issues accessing or using Azure, Microsoft 365 and Power Platform services. This incident was triggered by a utility power sag at 08:41 UTC on 30 August 2023, which impacted one of the three Availability Zones of the Australia East region. This power sag tripped a subset of the cooling system chiller units offline and, while working to restore cooling, temperatures in the datacenter increased to levels above operational thresholds. We powered down a small subset of selected compute and storage scale units, both to lower temperatures and to prevent damage to hardware. Although the vast majority of services recovered by 22:40 UTC on 30 August 2023, full mitigation was not until 20:00 UTC on 3 September 2023 \u2013 as some services experienced prolonged impact, predominantly as a result of dependencies on recovering subsets of Storage, SQL Database, and/or Cosmos DB services."}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Multiple Azure services were impacted by this incident \u2013 including Azure Active Directory (AAD), Azure Active Directory B2C, Azure Active Directory Conditional Access, Azure Active Directory Connect Health, Azure Active Directory MyApps, Azure Activity Logs & Alerts, Azure API Management, Azure App Service, Azure Application Insights, Azure Arc enabled Kubernetes, Azure API for FHIR, Azure Backup, Azure Batch, Azure Chaos Studio, Azure Container Apps, Azure Container Registry, Azure Cosmos DB, Azure Databricks, Azure Data Explorer, Azure Data Factory, Azure Database for MySQL flexible servers, Azure Database for PostgreSQL flexible servers, Azure Digital Twins, Azure Device Update for IoT Hub, Azure Event Hubs, Azure ExpressRoute, Azure Health Data Services, Azure HDInsight, Azure IoT Central, Azure IoT Hub, Azure Kubernetes Service (AKS), Azure Logic Apps, Azure Log Analytics, Azure Log Search Alerts, Azure NetApp Files, Azure Notification Hubs, Azure Redis Cache, Azure Relay, Azure Resource Manager (ARM), Azure Role Based Access Control (RBAC), Azure Search, Azure Service Bus, Azure Service Fabric, Azure SQL Database, Azure Storage, Azure Stream Analytics, Azure Virtual Machines, Microsoft Purview, and Microsoft Sentinel."}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "What went wrong and why?"}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Starting at approximately 08:41 UTC on 30 August 2023, a utility voltage sag was caused by a lightning strike on electrical infrastructure approximately 18 miles from the impacted Availability Zone of the Australia East region. The voltage sag caused cooling system chillers for multiple datacenters to shut down. While some chillers automatically restarted, 13 failed to restart and required manual intervention. To do so, the onsite team accessed the datacenter rooftop facilities, where the chillers are located, and proceeded to sequentially restart chillers moving from one datacenter to the next. By the time the team reached the final five chillers requiring a manual restart, the water inside the pump system for these chillers (chilled water loop) had reached temperatures that were too high to allow them to be restarted. In this scenario, the restart is inhibited by a self-protection mechanism that acts to prevent damage to the chiller that would occur by processing water at the elevated temperatures. The five chillers that could not be restarted supported cooling for the two adjacent data halls which were impacted in this incident."}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "The two impacted data halls require at least four chillers to be operational. Before the voltage sag, our cooling capacity for these halls consisted of seven chillers, with five chillers in operation and two chillers in standby. At 10:30 UTC some networking, compute, and storage infrastructure began to shutdown automatically as data hall temperatures increased, impacting service availability. At 11:34 UTC, as temperatures continued to increase, our onsite datacenter team began a remote shutdown of remaining networking, compute, and storage infrastructure in the impacted data halls to protect data durability, infrastructure health, and address the thermal runaway. This shutdown allowed the chilled water loop to return to a safe temperature which allowed us to restart the chillers. This shutdown of infrastructure resulted in a further reduction of service availability for this Availability Zone."}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "The chillers were successfully brought back online at 12:12 UTC, and data hall temperatures returned to operational thresholds by 13:30 UTC. Power was then restored to the affected infrastructure and a phased process to bring the infrastructure back online commenced. All power to infrastructure was restored by 15:10 UTC. Once all networking and storage infrastructure had power restored, dependent compute scale units were then also returned to operation. As the underlying compute and storage scale units came online, dependent Azure services started to recover, but some services experienced issues coming back online."}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "From a storage perspective, seven storage scale units were impacted \u2013 five standard storage scale units, and two premium storage scale units. Availability impact to affected storage accounts began at 10:30 UTC as hardware shut down in response to elevated data hall temperatures. This was most impactful to storage accounts configured with the default local redundant storage (LRS), which is not resilient to a zonal failure. Accounts configured as zonally redundant (ZRS) remained 100% available, and accounts configured as geographically redundant (GRS) were eligible for customer-managed account failover. After power restoration, storage nodes started coming back online from 15:25 UTC. Four scale units required engineer intervention to check and reset some fault detection logic \u2013 the combination of investigation and management tooling performance problems delayed restoration of availability for these scale units. We also identified that some automation was incorrectly marking some already recovered nodes as unhealthy, which slowed storage recovery efforts. By 20:00 UTC, 99% of storage accounts had recovered. Restoring availability for the remaining <1% of storage accounts took more time due to hardware troubleshooting and replacement required on a small number of storage nodes in a single scale unit. Even identifying problematic hardware in the storage nodes took an extended period of time, as the nodes were offline and therefore not able to provide diagnostics. By 01:00 UTC on 31 August, availability was restored for all except a handful of storage accounts, with complete availability restored by 07:00 UTC on 1 September."}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "From a SQL Database perspective, database capacity in a region is divided into tenant rings. The Australia East region includes hundreds of rings, and each ring consists of a group of VMs (10-200) hosting a set of databases. Rings are managed by Azure Service Fabric to provide availability in cases of VM, network or storage failures. When infrastructure was powered down, customers using zone-redundant Azure SQL Database did not experience any downtime, except for a small subset of customers using proxy mode connection, due to one connectivity gateway not being configured with zonal-resilience. The fix for this issue was already being rolled out, but had not yet deployed to the Australia East region. As infrastructure was powered back on, all tenant rings except one came back online and databases became available to customers as expected. However, one ring remained impacted even after Azure Compute became available. In this ring, 20 nodes did not come back online as expected, so databases on these nodes continued to experience unavailability. As a result of Service Fabric attempting to move databases to healthy nodes, other databases on this ring experienced intermittent availability issues as a side-effect of the overall replica density and unhealthy nodes. The recovery involved first moving all the databases from unhealthy nodes to healthy nodes. All remote storage (general purpose) databases were successfully recovered by this move, but databases using local storage (business critical) only recovered as their underlying nodes recovered. All databases on unhealthy nodes were recovered by 11:00 UTC on 31 August. Since the health and capacity of this final ring did not completely recover, we decided to move all databases out of the ring, which extended the overall recovery time but did not negatively impact customer availability. During this extended recovery, most customers were not experiencing any"}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "recovered. All databases on unhealthy nodes were recovered by 11:00 UTC on 31 August. Since the health and capacity of this final ring did not completely recover, we decided to move all databases out of the ring, which extended the overall recovery time but did not negatively impact customer availability. During this extended recovery, most customers were not experiencing any issues but it was important to move all databases out of this unhealthy ring to prevent any potential impact. The operation of moving all databases out of this ring was completed at 20:00 UTC on 3 September. During this incident, customers who had \u2018active geo-replication\u2019 setup were able to failover manually to restore availability. For customers who have \u2018auto-failover groups\u2019 enabled, we did not execute automatic failover \u2013 our automatic failover policy was not initiated for the region, due to an incorrect initial assessment of the impact severity to SQL Database."}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "From a Cosmos DB perspective, zone resilient accounts and those with multi-region writes remained operational during the incident, transparently serving requests from a different zone or region, respectively. However, accounts not configured for AZ or multi-region writes experienced full or partial loss of availability, due to the infrastructure that was powered down. Multi-region accounts with single region write eligible for failover (those with Service Managed Failover enabled) were failed over to their alternate regions to restore availability. These were initiated at 12:07 UTC, 33 minutes after decision to power down scale units. The reason for this delay was to identify and failover the Cosmos DB control plane system resources \u2013 in retrospect this delay was unnecessary, as the Cosmos DB control plane was already fully zone-resilient. 95% of database accounts were failed over within 35 minutes by 12:42 UTC, and all eligible accounts were failed over by 16:13 UTC on 30 August. Accounts that were not eligible for failover had service restored to partitions only once the dependent storage and compute were restored."}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "From an Azure Kubernetes Service (AKS) perspective, the service experienced a loss of compute for the AKS control plane for Australia East as well as data access loss to SQL Database. The AKS control plane underlay is deployed across multiple availability zones. AKS uses Azure SQL Database for its operation queue which is used for Create/Read/Update/Delete (CRUD) activities. Although scheduled to be converted, the SQL Database in Australia East was not configured with AZ resiliency selection, leaving it unavailable during the incident period. In addition, AKS services in the Australia Southeast region depended on this same database, causing an AKS incident for CRUD activities in that region also. Existing customer workloads running on AKS clusters in either region should not have been impacted by the downtime, as long as they did not need to access the AKS resource provider for scaling or other CRUD activities. As the SQL Database recovered, service was restored without any other mitigation required."}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "From an Azure Resource Manager (ARM) perspective, the impact on customers was the result of degradation in Cosmos DB. This degradation impacted ARM between 10:45 UTC and 12:25 UTC and resulted in ARM platform availability for the Australia East region dropping from ~99.999% to (at its lowest) 88%, with a 62% success rate for write operations. For data consistency reasons, write operations are required to be sent to the associated Cosmos DB regional replica for a given resource group. While the migration to our next generation zonally-redundant storage architecture is still ongoing, it has not been completed and as a result this region is not yet leveraging fully zonally redundant storage for ARM. This meant that for the duration of the incident, customers worldwide trying to manage resources whose resource groups were homed in Australia East saw increased error rates (and this manifested in a small impact to global platform availability until 15:00 UTC)."}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "How did we respond?"}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "30 August 2023 @ 08:41 UTC \u2013 Voltage sag occurred on utility power line\n30 August 2023 @ 08:43 UTC \u2013 13 chillers failed to restart automatically\n30 August 2023 @ 08:51 UTC \u2013 Remote resets on chillers commenced\n30 August 2023 @ 09:09 UTC \u2013 Team arrived at first group of chillers for manual restarts\n30 August 2023 @ 09:18 UTC \u2013 Team arrived at second group of chillers for manual restarts\n30 August 2023 @ 09:42 UTC \u2013 Team arrived at third group of chillers for manual restarts\n30 August 2023 @ 09:45 UTC \u2013 Team arrived at the final group of chillers which could not be restarted \n30 August 2023 @ 10:30 UTC \u2013 Initial impact from automated infrastructure shutdown\n30 August 2023 @ 10:47 UTC \u2013 Cosmos DB Initial impact detected via monitoring\n30 August 2023 @ 10:48 UTC \u2013 First automated communications sent to Azure Service Health\n30 August 2023 @ 11:30 UTC \u2013 Initial communications posted to public Azure Status page \n30 August 2023 @ 11:34 UTC \u2013 Decision made to shutdown impacted infrastructure\n30 August 2023 @ 11:36 UTC \u2013 All subscriptions in Australia East sent portal communications\n30 August 2023 @ 12:07 UTC \u2013 Failover initiated for eligible Cosmos DB accounts \n30 August 2023 @ 12:12 UTC \u2013 Five chillers manually restarted\n30 August 2023 @ 13:30 UTC \u2013 Data hall temperature normalized\n30 August 2023 @ 14:10 UTC \u2013 Safety walkthrough completed for both data halls\n30 August 2023 @ 14:25 UTC \u2013 Decision made to start powering up hardware in the two affected data halls \n30 August 2023 @ 15:10 UTC \u2013 Power restored to all hardware \n30 August 2023 @ 15:25 UTC \u2013 Storage, networking, and compute infrastructure started coming back online after power restoration"}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "30 August 2023 @ 14:25 UTC \u2013 Decision made to start powering up hardware in the two affected data halls \n30 August 2023 @ 15:10 UTC \u2013 Power restored to all hardware \n30 August 2023 @ 15:25 UTC \u2013 Storage, networking, and compute infrastructure started coming back online after power restoration\n30 August 2023 @ 15:30 UTC \u2013 Identified three specific storage scale units still experiencing fault codes\n30 August 2023 @ 16:00 UTC \u2013 35% of VMs recovered / Began manual recovery efforts for three remaining storage scale units \n30 August 2023 @ 16:13 UTC \u2013 Account failover completed for all Cosmos DB accounts\n30 August 2023 @ 17:00 UTC \u2013 All but one SQL Database tenant ring had recovered\n30 August 2023 @ 19:20 UTC \u2013 90% of VMs recovered\n30 August 2023 @ 19:29 UTC \u2013 Successfully recovered all premium storage scale units\n30 August 2023 @ 20:00 UTC \u2013 99% of storage accounts were back online\n30 August 2023 @ 22:35 UTC \u2013 Standard storage scale units were recovered, except for one scale unit\n30 August 2023 @ 22:40 UTC \u2013 99% of VMs recovered\n31 August 2023 @ 04:04 UTC \u2013 Restoration of Cosmos DB accounts to Australia East initiated\n31 August 2023 @ 04:43 UTC \u2013 Final Cosmos DB cluster recovered, restoring all traffic for accounts that were not failed over to alternate regions\n31 August 2023 @ 05:00 UTC \u2013 100% of VMs recovered\n1 September 2023 @ 06:40 UTC \u2013 Successfully recovered all standard storage scale units \n3 September 2023 @ 20:00 UTC \u2013 Final SQL Database tenant ring evacuated and all customer databases online\nHow are we making incidents like this less likely or less impactful?"}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Incremental load increases over time in the Availability Zone resulted in a chiller operating configuration that was susceptible to a timing defect in the Chiller Management System. We have de-risked this failure to restart due to voltage fluctuations, by implementing a change to the control timing logic on the Chiller Management System (Completed)."}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Emergency Operation Procedure for manual restarts of simultaneous chiller failures has changed from an \u2018adjacent\u2019 sequence to a data hall \u2018load based\u2019 sequence to ensure all impacted data halls have partial cooling, to slow thermal runaway while full cooling is being restored (Completed)."}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Following this incident, as a temporary mitigation, we increased technician staffing levels at the datacenter to be prepared to execute manual restart procedures of our chillers prior to the change to the Chiller Management System to prevent restart failures. Based on our incident analysis the staffing levels at the time would have been sufficient to prevent impact if a \u2018load based' chiller restart sequence had been followed, which we have since implemented (Completed)."}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Datacenter staffing levels published in the Preliminary PIR only accounted for \u201ccritical environment\u201d staff onsite. This did not characterize our total datacenter staffing levels accurately. To alleviate this misconception, we made a change to the preliminary public PIR posted on the Status History page."}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Our Storage team has identified several optimizations in our large scale recovery process which will help to reduce time to mitigate. This includes augmenting data provided in our incidents to enable quicker decision making, and updates to our troubleshooting guides (TSGs) that enable faster execution (Estimated completion: December 2023)."}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Our Azure Service Fabric team is working to improve reliability of SQL Database tenant ring recovery. (Estimated completion: December 2023).\nOur SQL Database team is reviewing our \u2018auto-failover group\u2019 trigger criteria, to ensure that failovers can happen within the expected timeframe. (Estimated completion: October 2023).\nOur SQL Database team is upgrading internal tooling to enable mass migration of databases. (Estimated completion: December 2023).\nOur Cosmos DB team is working to optimize Service Managed Failover for single region write accounts to reduce time to mitigate (Estimated completion: November 2023).\nOur AKS team is immediately converting all operation queue SQL Database databases to be zone redundant (Estimated completion: September 2023).\nOur AKS team is also replacing all cross-region SQL Database queue usage with Service Bus queues that are zone redundant (Estimated completion: September 2023).\nOur ARM team will complete its storage layer migration to the next generation, zonally redundant architecture (Estimated completion: December 2023).\nOur incident management team is exploring ways to harden our readiness, process, and playbook surrounding power down scenarios (Estimated completion: October 2023)\nHow can customers make incidents like this less impactful?"}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Consider using Availability Zones (AZs) to run your services across physically separate locations within an Azure region. To help services be more resilient to datacenter-level failures like this one, each AZ provides independent power, networking, and cooling. Many Azure services support zonal, zone-redundant, and/or always-available configurations: https://docs.microsoft.com/azure/availability-zones/az-overview"}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "For mission-critical workloads, customers should consider a multi-region geodiversity strategy to avoid impact from incidents like this one that impacted a single region: https://learn.microsoft.com/training/modules/design-a-geographically-distributed-application/ and https://learn.microsoft.com/azure/architecture/patterns/geodes"}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Consider which are the right storage redundancy options for your critical applications. Zone redundant storage (ZRS/GZRS) remains available throughout a zone localized failure, like in this incident. Geo-redundant storage (GRS) enables account level failover in case the primary region endpoint becomes unavailable: https://learn.microsoft.com/azure/storage/common/storage-redundancy"}
{"title": "Post Incident Review (PIR) - Services impacted after power/cooling issue - Australia East", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Consider the relevant guidance for recovering your SQL Database databases during disaster recovery scenarios: https://learn.microsoft.com/azure/azure-sql/database/disaster-recovery-guidance\nConsider the relevant guidance for achieving high availability with Azure Cosmos DB https://learn.microsoft.com/azure/cosmos-db/high-availability\nMore generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review: https://docs.microsoft.com/azure/architecture/framework/resiliency\nFinally, consider ensuring that the right people in your organization will be notified about any future service issues \u2013 by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more: https://aka.ms/ash-alerts"}
{"title": "Post Incident Review (PIR) - Networking fiber cut - West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "What happened?"}
{"title": "Post Incident Review (PIR) - Networking fiber cut - West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Between approximately 07:22 UTC and 16:00 UTC on 5 July 2023, Azure customers using the West Europe region may have experienced packet drops, timeouts, and/or increased latency. This impact resulted from a fiber cut caused by severe weather conditions in the Netherlands. The West Europe region has multiple datacenters and is designed with four independent fiber paths for the traffic that flows between datacenters. In this incident, one of the four major paths was cut, which resulted in congestive packet loss when traffic on the remaining links exceeded their capacity."}
{"title": "Post Incident Review (PIR) - Networking fiber cut - West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Downstream Azure services dependent on this intra-region network connectivity were also impacted \u2013 including Azure App Services, Azure Application Insights, Azure Data Explorer, Azure Database for MySQL, Azure Databricks (which also experienced impact in North Europe, as a result of a control plane dependency), Azure Digital Twins, Azure HDInsight, Azure Kubernetes Service, Azure Log Analytics, Azure Monitor, Azure NetApp Files, Azure Resource Graph, Azure Site Recovery, Azure Service Bus, Azure SQL DB, Azure Storage, and Azure Virtual Machines \u2013 as well as subsets of Microsoft 365, Microsoft Power Platform, and Microsoft Sentinel services."}
{"title": "Post Incident Review (PIR) - Networking fiber cut - West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "What went wrong and why?"}
{"title": "Post Incident Review (PIR) - Networking fiber cut - West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Due to a fiber cut caused by severe weather conditions in the Netherlands, 25% of network links between two campuses of West Europe datacenters became unavailable. These links were already running at higher utilization than our design target, and there was a capacity augment project in progress to address this. Due to a previous incident related to this capacity augment on 16 June 2023 (Tracking ID VLB8-1Z0), the augment work was proceeding with extreme caution and was still in progress when the fiber cut occurred."}
{"title": "Post Incident Review (PIR) - Networking fiber cut - West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "As a result of the fiber cut and the higher utilization, congestion increased to a point where intermittent packet drops occurred in many of the intra-region paths. This primarily impacted network traffic between Availability Zones within the West Europe region, not traffic to and from the region itself. As a result of this interruption, Azure services that rely on internal communications with other services within the region may have experienced degraded performance, manifesting in the issues described above."}
{"title": "Post Incident Review (PIR) - Networking fiber cut - West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "How did we respond?\n\nNetwork alerting services indicated a fiber cut at 07:22 UTC and a congestion alert triggered at 07:46 UTC. Our networking on-call engineers engaged and began to investigate. Two parallel workstreams were spun up to mitigate impact:"}
{"title": "Post Incident Review (PIR) - Networking fiber cut - West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "The first workstream focused on reducing traffic in the region and balancing it across the remaining links. This balancing activity requires a detailed before-and-after traffic simulation to ensure safety, and these simulations were initiated as a first step. At 10:00 UTC we initiated throttling and migration of internal service traffic away from the region. We also started work on rebalancing traffic away from congested links. As a result of these activities, by 14:52 UTC packet drops were reduced significantly, by 15:30 UTC many internal and external services saw signs of recovery, and by 16:00 UTC packet drops had returned to pre-incident levels. We continued to work on reducing high link utilization and by 16:21 UTC the rebalancing activity was completed."}
{"title": "Post Incident Review (PIR) - Networking fiber cut - West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "The second workstream focused on repairing the impacted links, in partnership with our dark fiber provider in the Netherlands. These cable repairs took longer than expected since access to the impacted area was hindered by the weather and hazardous working conditions. Partial restoration was confirmed by 19:30 UTC, and full restoration was confirmed by 20:50 UTC. While this restored the network capacity between datacenters, we continued to monitor our network infrastructure and capacity before declaring the incident mitigated at 22:45 UTC."}
{"title": "Post Incident Review (PIR) - Networking fiber cut - West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "How are we making incidents like this less likely or less impactful?"}
{"title": "Post Incident Review (PIR) - Networking fiber cut - West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "We have repaired the impacted networking links, in partnership with our dark fiber provider in the Netherlands. (Completed)\nWithin 24 hours of the incident being mitigated we brought additional capacity online, on the impacted network path. (Completed)\nWithin a week of the incident, we are 90% complete with our capacity augments that will double capacity in our West Europe region to bring utilization within our design targets. (Estimated completion: July 2023)\nAs committed in a previous Post Incident Review (PIR), we are working towards auto-declaring regional incidents to ensure customers get notified more quickly (Estimated completion: August 2023).\nHow can customers make incidents like this less impactful?"}
{"title": "Post Incident Review (PIR) - Networking fiber cut - West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "During the incident, we advised customers who were able to fail out of the West Europe region to consider doing so. Customers should consider a multi-region geodiversity strategy for mission-critical workloads, to avoid impact from incidents like this one that impacted a single region: https://learn.microsoft.com/training/modules/design-a-geographically-distributed-application/ and https://learn.microsoft.com/azure/architecture/patterns/geodes"}
{"title": "Post Incident Review (PIR) - Networking fiber cut - West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "More generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review: https://docs.microsoft.com/azure/architecture/framework/resiliency\nFinally, consider ensuring that the right people in your organization will be notified about any future service issues - by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more: https://aka.ms/ash-alerts"}
{"title": "Apigee Integrated Developer Portal is experiencing issues while accessing it from Apigee Edge and Apigee X", "url": "https://status.cloud.google.com/incidents/hbQvquDhzwy2cpK9FU3k", "snippet": "Summary\nOn Monday, 11 September 2023, Apigee integrated developer portal customers experienced elevated 5xx responses, issues authenticating to API, and issues updating portals when accessing it via Apigee Edge or Apigee X for a duration of 8 hours and 25 minutes.\n\nTo our Apigee customers whose business analytics were impacted during this disruption, we sincerely apologize. This is not the level of quality and reliability we strive to offer you, and we are taking immediate steps to improve the platform\u2019s performance and availability.\n\nRoot Cause\nThe root cause of the issue was an incorrect configuration change made to one of the Apigee load balancers for developer portals. This configuration change caused all the requests that were routed through the affected Apigee load balancer to fail with a default 404 error code."}
{"title": "Apigee Integrated Developer Portal is experiencing issues while accessing it from Apigee Edge and Apigee X", "url": "https://status.cloud.google.com/incidents/hbQvquDhzwy2cpK9FU3k", "snippet": "Remediation and Prevention"}
{"title": "Apigee Integrated Developer Portal is experiencing issues while accessing it from Apigee Edge and Apigee X", "url": "https://status.cloud.google.com/incidents/hbQvquDhzwy2cpK9FU3k", "snippet": "Google engineers were alerted by a support case on Monday, 11 September at 07:00 US/Pacific and immediately started an investigation. They were able to identify a misconfiguration in one of the load balancers, which was caused by a change aimed at improving the load balancer\u2019s performance. Upon identifying the misconfiguration, Google Engineers quickly regenerated the configurations of the affected Apigee load balancer and ensured that both load balancers were properly configured."}
{"title": "Apigee Integrated Developer Portal is experiencing issues while accessing it from Apigee Edge and Apigee X", "url": "https://status.cloud.google.com/incidents/hbQvquDhzwy2cpK9FU3k", "snippet": "We apologize for the length and severity of this incident. We are taking the following steps to prevent a recurrence and improve reliability in the future:"}
{"title": "Apigee Integrated Developer Portal is experiencing issues while accessing it from Apigee Edge and Apigee X", "url": "https://status.cloud.google.com/incidents/hbQvquDhzwy2cpK9FU3k", "snippet": "Identify the cause of the inconsistent configurations for the developer portal load balancers\nImprove the change management process for Apigee load balancer changes to prevent similar issues\nImprove monitoring and visibility to ensure the incorrect configurations are identified proactively\nDetailed Description of Impact"}
{"title": "Apigee Integrated Developer Portal is experiencing issues while accessing it from Apigee Edge and Apigee X", "url": "https://status.cloud.google.com/incidents/hbQvquDhzwy2cpK9FU3k", "snippet": "On 11 September from 04:00 to 12:25 US/Pacific, users experienced elevated error rates with errors like \"Failed to load users\", \"Failed to load zone\" or \"Service Unavailable\u201d, when interacting with the developer portal. Customers also experienced issues authenticating to API portals or updating their portals."}
{"title": "Git Databases degraded due to configuration change", "url": "https://github.blog/2023-05-16-addressing-githubs-recent-availability-issues/#may-9-git-database-incident", "snippet": "On May 9, we had an incident that caused 8 of the 10 services on the status portal to be impacted by a major (status red) outage. The majority of downtime lasted just over an hour. During that hour-long period, many services could not read newly-written Git data, causing widespread failures. Following this outage, there was an extended timeline for post-incident recovery of some pull request and push data."}
{"title": "Git Databases degraded due to configuration change", "url": "https://github.blog/2023-05-16-addressing-githubs-recent-availability-issues/#may-9-git-database-incident", "snippet": "This incident was triggered by a configuration change to the internal service serving Git data. The change was intended to prevent connection saturation, and had been previously introduced successfully elsewhere in the Git backend.\n\nShortly after the rollout began, the cluster experienced a failover. We reverted the config change and attempted a rollback within a few minutes, but the rollback failed due to an internal infrastructure error.\n\nOnce we completed a gradual failover, write operations were restored to the database and broad impact ended. Additional time was needed to get Git data, website-visible contents, and pull requests consistent for pushes received during the outage to achieve a full resolution."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Incident and impact\nStarting on March 8, 2023, at 06:03 UTC, we experienced an outage that affected the US1, EU1, US3, US4, and US5 Datadog regions across all services.\n\nWhen the incident started, users could not access the platform or various Datadog services via the browser or APIs and monitors were unavailable and not alerting. Data ingestion for various services was also impacted at the beginning of the outage.\n\nChronology\nWe began our investigation immediately, leading to the first status page update indicating an issue at 06:31 UTC, and the first update indicating potential impact to data ingestion and monitors at 07:23 UTC."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "We started to see initial signs of recovery at 09:13 UTC (web access restored) and declared our first major service operational by 16:44 UTC. We continued our recovery efforts, and declared all services operational in all regions on March 9, 2023, 08:58 UTC. The incident was fully resolved on March 10, 2023, at 06:25 UTC once we had backfilled historical data."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Response"}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "We were first alerted to the issue by our internal monitoring, three minutes after the trigger of the first faulty upgrade (March 8, 2023, at 06:00 UTC). We declared an internal high-severity incident 18 minutes into the investigation, and we made it a public incident shortly thereafter, at 06:31 UTC."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Our incident response team had an emergency operation center of 10 senior engineering leaders, about 70 local incident commanders and a pool of 450 to 750 incident responders active throughout the incident, which required four shifts to bring the incident to full resolution.\n\nOur communications lead orchestrated about 200 status updates across four regions, and involved hundreds of support engineers in constant communication with our customers around the clock.\n\nMitigation and remediation proceeded as follows:"}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "We focused our initial efforts on restoring normal ingestion and processing for real-time telemetry data so that our customers could use the platform, even with limited access to historical data.\nOnce real-time data was usable, we switched our recovery efforts to historical data that had been ingested but left unprocessed at the beginning of the outage.\nSimultaneously, we started providing frequent updates on the progress on our status page to keep customers apprised of the situation.\nRoot cause"}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "We have identified (and since disabled) the initial trigger for the outage: on March 8, 2023, at 06:00 UTC, a security update to systemd was automatically applied to a number of VMs, which caused a latent adverse interaction in the network stack (on Ubuntu 22.04 via systemd v249) to manifest upon systemd-networkd restarting. Namely, systemd-networkd forcibly deleted the routes managed by the Container Network Interface (CNI) plugin (Cilium) we use for communication between containers. This caused the affected nodes to go offline."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "The aggravating factor is that neither a fresh node nor a rebooted node exhibit this behavior because during a normal boot sequence systemd-networkd always starts before routing rules are installed by the CNI plugin. In other words, outside of the systemd-networkd update scenario on a running node, no obvious test reproduces the exact sequence."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "What immediate effect did it have? This affected tens of thousands of nodes in our fleet between 06:00 and 07:00; by 06:31 UTC, enough of our fleet was offline that the outage was visible to our customers."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Why did the update automatically apply in five distinct regions that span dozens of availability zones and run on three different cloud providers? Whenever we push new code, security patches or configuration changes to our platform, we do so region-by-region, cluster-by-cluster, node-by-node. As a matter of process, we don\u2019t apply changes in place; rather, we replace nodes and containers in a blue/green fashion. Changes that are unsuccessful trigger an automatic rollback. We have confidence in this change management tooling because it is exercised at a large scale, dozens of times a day."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "However, the base OS image we use to run Kubernetes had a legacy security update channel enabled, which caused the update to apply automatically. By design we use fairly minimal base OS images so these updates are infrequent and until that day, weren\u2019t ever disruptive."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "To compound the issue, the time at which the automatic update happens is set by default in the OS to a window between 06:00 and 07:00 UTC. Thus it affected multiple regions at the exact same time, even though the regions have no direct network connection or coupling between them. This made the scale of the impact markedly larger, and confounded the initial responders who, like the rest of us, were unaware of this very indirect coupling of behavior between regions."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Will it happen again? No. We have disabled this legacy security update channel across all affected regions so it will not happen again. We have also changed the configuration of systemd-networkd so that it leaves the routing table unchanged upon restart. Finally, we have audited our infrastructure for potential similar legacy update channels."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Disabling this legacy security update channel has no adverse impact on our security posture because it is redundant with the way we apply security updates today, which are handled via the aforementioned change management tooling.\n\nRecovery\nThe high-level steps of the recovery had to be sequenced because of the magnitude of the compute capacity drop:"}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "First, for each region, recover enough compute capacity so that it could scale back to what it needed to process incoming data at the normal rate. In fact, working with each cloud provider, we scaled the compute capacity to a higher level than normal in order to accommodate the processing of real-time and historical data at the same time."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Once compute capacity was ready, recover each service in parallel, such that all services could be restored as soon as possible.\nCompute capacity\nThe effects of losing the network stack were felt differently across cloud providers and that complicated our recovery efforts."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Some cloud providers\u2019 auto-scaling logic correctly identify the affected node as unhealthy but do not initiate the immediate retirement of the node. This meant that simply rebooting the affected instance was the correct recovery step and was much faster for stateful workloads that still had their data intact and led to an overall faster recovery."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Other cloud providers\u2019 auto-scaling logic immediately replaces the unhealthy node with a fresh one. Data on the unhealthy node is lost and needs to be recovered, which adds to the complexity of the task at hand. At the scale of tens of thousands of nodes being replaced at once, it also created a thundering herd that tested the cloud provider\u2019s regional rate limits in various ways, none of which was obvious ex ante."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Once we had raw compute capacity at our disposal, we had to pay attention to the order of recovery: each region\u2019s internal control plane, whose role is to keep all Kubernetes clusters healthy, had to be recovered before the hundreds of Kubernetes clusters that power the platform could be repaired. When the regional control plane was fully restored, the various compute teams could fan out and make sure that each cluster was healthy enough to let the services running on it be repaired by other teams."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Service recovery"}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Each of our services had different recovery steps but they all share enough similarities to be presented here. In all cases, our number one priority was to restore the processing of live data. Most of our services use one system to intake and serve live data (meaning the last 15 minutes or the last 24 hours, depending on the product) and a separate system to serve historical data. All systems were affected to a different degree but that separation allowed us to bring our products to a functioning state earlier in the recovery process."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Another common theme that all teams encountered during the service recovery was the fact that distributed, share-nothing data stores handle massive failures much better than most distributed data stores that require a quorum-based control plane. For example a fleet of independent data nodes with static shard assignment degrades roughly linearly as the number of nodes drops. The same fleet of data nodes, bound together by a quorum will operate without degradation so long as the quorum is met and refuse to operate once it\u2019s not. Of course the quorum-based fleet is a lot easier to manage in the day-to-day, so going one way or another is not an obvious decision, but this outage highlights the need for us to re-examine past choices."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Generally speaking, our primary telemetry datastores (e.g., those that store point values and raw log data) are statically sharded, while various metadata (e.g., those that index and resolve tags) is stored in systems that require a quorum to operate. Thus, much of the recovery focused on restoring these metadata stores to the point that they would accept new writes for live data, while a smaller team was able to concurrently repair the independent static shards that indexed and stored live data."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Lessons learned\nThe following is not the sum total of everything we learned during the incident but rather important themes that pertain to the whole platform."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "This was our first global incident. We built Datadog regions with the explicit design goal of being autonomous and resilient to local failure. This is why they are built across multiple zones on completely independent cloud providers without any direct connections to one another. This is also why we have no global, shared control plane. And for good measure, each service has, in each region, failovers, active-active setups across availability zones, and extra capacity to deal with spikes during recovery. Still, we failed to imagine how they could remain indirectly related, and that miss will inform how we continue to strengthen our foundational infrastructure."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "We heard time and time again that there is a clear hierarchy among the data we process on our customers\u2019 behalf. Most important, usable live data and alerts are much more valuable than access to historical data. And even among all the live data, data that is actively monitored or visible on dashboards is more valuable than the rest of live data. We will take this clear hierarchy into account in how we handle processing and access in degraded mode."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "We run chaos tests (and are continuously subjected to a certain degree of chaos from our cloud providers) but we fell short in considering the scale of degradation we need to consider in our tests. Going forward we will use the level of infrastructure disruption we saw to prove that the platform can operate in degraded conditions without being completely down. Coupled with the previous point, this may take the form of having only urgent data accessible and processed in degraded mode."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Even with separate systems that serve live data when historical data is not accessible (e.g., APM\u2019s Live Search and Log Management\u2019s Live Tail), we did not provide our customers with clear guidance on how they could access our systems as soon as they were available, even while the broader incident was still being remediated. Going forward, we will prioritize sharing guidance with our customers about which services and data are available and how to access them."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "Our communication during the outage improved in precision and depth as time passed but we realize the inadequacy of the status page to communicate nuanced product status. We will devise a way to communicate in real-time a detailed status for each product, even when we run the platform in degraded mode."}
{"title": "2023-03-08 Incident: Infrastructure connectivity issue affecting multiple regions", "url": "https://www.datadoghq.com/blog/2023-03-08-multiregion-infrastructure-connectivity-issue/", "snippet": "In closing\nThis outage has been a humbling experience for everyone involved at Datadog. We all understand how important uninterrupted service is to our customers and we are committing to verifiably improve the resilience of our services based on all that we learned during and after this outage."}
{"title": "Summary of the December 17th event in the South America Region (SA-EAST-1)", "url": "https://aws.amazon.com/message/656481/", "snippet": "We want to give you some additional insight into the event that impacted a single Availability Zone in the South America Region (SA-EAST-1). On December 17th at 10:05PM PST, the impacted Availability Zone lost utility power due to a fault that happened at the substation of the local utility provider. The impacted Availability Zone automatically switched over to run on generator power when utility power was lost. Availability Zones are built with multiple layers of redundancy, and are designed to continue to operate even when multiple components fail at the same time. In this particular case when we experienced a loss in utility power, the load switched over to our backup generators as designed. During that failover a breaker in front of one of the generators opened, rendering that generator unavailable. Shortly thereafter, a second generator independently failed due to a mechanical issue. The loss of utility power combined with the unavailability of two additional generators meant that there was more load in the facility than the remaining healthy generators could handle. With more load on them than they could support, the remaining healthy generators also shut down. Our facilities team immediately began working to bring the failed generators back online. This facility uses an automated control system which allows it to aggregate power from multiple generators together. The team experienced several additional setbacks when trying to bring the power infrastructure back online, and eventually identified that the automated control system wasn\u2019t functioning properly. Once the team identified the issue, they bypassed the automated control system and began the slow process of manually bringing the generators online. Once there was sufficient generator capacity to fully support the facility, all of the impacted instances were recovered. While we have not completed forensics on the breaker opening and the generator mechanical"}
{"title": "Summary of the December 17th event in the South America Region (SA-EAST-1)", "url": "https://aws.amazon.com/message/656481/", "snippet": "control system wasn\u2019t functioning properly. Once the team identified the issue, they bypassed the automated control system and began the slow process of manually bringing the generators online. Once there was sufficient generator capacity to fully support the facility, all of the impacted instances were recovered. While we have not completed forensics on the breaker opening and the generator mechanical failure, we find the double failure to be extremely unusual, and are deeply reviewing the operational records of the failed components."}
{"title": "Summary of the December 17th event in the South America Region (SA-EAST-1)", "url": "https://aws.amazon.com/message/656481/", "snippet": "Instances in the second Availability Zone in the Region did not experience any power related issues, however instances in both Availability Zones did experience a total of 20 minutes of degraded network connectivity due to an error that was made in bringing our network back online once power was restored. As part of the recovery process, a network technician brought a network device up manually in the power-impacted Availability Zone and introduced a bad configuration. That misconfiguration led to the device advertising an invalid network route when it came back online, which resulted in degraded Internet connectivity for both SA-EAST-1 Availability Zones. Once we understood the issue, we took the device out of service and full connectivity to the Region was restored. After power and networking were fully restored to the facility, all of our services were brought back online and full customer access was restored."}
{"title": "Summary of the December 17th event in the South America Region (SA-EAST-1)", "url": "https://aws.amazon.com/message/656481/", "snippet": "We apologize for any difficulty this event may have caused you. We appreciate how critical our services are to our customers, and will take steps to ensure this Availability Zone in Brazil is better able to withstand a similar power failure in the future."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "Last week, GitHub experienced an incident that resulted in degraded service for 24 hours and 11 minutes. While portions of our platform were not affected by this incident, multiple internal systems were affected which resulted in our displaying of information that was out of date and inconsistent. Ultimately, no user data was lost; however manual reconciliation for a few seconds of database writes is still in progress. For the majority of the incident, GitHub was also unable to serve webhook events or build and publish GitHub Pages sites."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "All of us at GitHub would like to sincerely apologize for the impact this caused to each and every one of you. We\u2019re aware of the trust you place in GitHub and take pride in building resilient systems that enable our platform to remain highly available. With this incident, we failed you, and we are deeply sorry. While we cannot undo the problems that were created by GitHub\u2019s platform being unusable for an extended period of time, we can explain the events that led to this incident, the lessons we\u2019ve learned, and the steps we\u2019re taking as a company to better ensure this doesn\u2019t happen again."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "Background"}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "The majority of user-facing GitHub services are run within our own data center facilities. The data center topology is designed to provide a robust and expandable edge network that operates in front of several regional data centers that power our compute and storage workloads. Despite the layers of redundancy built into the physical and logical components in this design, it is still possible that sites will be unable to communicate with each other for some amount of time."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "At 22:52 UTC on October 21, routine maintenance work to replace failing 100G optical equipment resulted in the loss of connectivity between our US East Coast network hub and our primary US East Coast data center. Connectivity between these locations was restored in 43 seconds, but this brief outage triggered a chain of events that led to 24 hours and 11 minutes of service degradation."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "A high-level depiction of GitHub's network architecture, including two physical datacenters, 3 POPS, and cloud capacity in multiple regions connected via peering."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "In the past, we\u2019ve discussed how we use MySQL to store GitHub metadata as well as our approach to MySQL High Availability. GitHub operates multiple MySQL clusters varying in size from hundreds of gigabytes to nearly five terabytes, each with up to dozens of read replicas per cluster to store non-Git metadata, so our applications can provide pull requests and issues, manage authentication, coordinate background processing, and serve additional functionality beyond raw Git object storage. Different data across different parts of the application is stored on various clusters through functional sharding."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "To improve performance at scale, our applications will direct writes to the relevant primary for each cluster, but delegate read requests to a subset of replica servers in the vast majority of cases. We use Orchestrator to manage our MySQL cluster topologies and handle automated failover. Orchestrator considers a number of variables during this process and is built on top of Raft for consensus. It\u2019s possible for Orchestrator to implement topologies that applications are unable to support, therefore care must be taken to align Orchestrator\u2019s configuration with application-level expectations."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "In the normal topology, all apps perform reads locally with low latency."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "Incident timeline\n2018 October 21 22:52 UTC"}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "During the network partition described above, Orchestrator, which had been active in our primary data center, began a process of leadership deselection, according to Raft consensus. The US West Coast data center and US East Coast public cloud Orchestrator nodes were able to establish a quorum and start failing over clusters to direct writes to the US West Coast data center. Orchestrator proceeded to organize the US West Coast database cluster topologies. When connectivity was restored, our application tier immediately began directing write traffic to the new primaries in the West Coast site."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "The database servers in the US East Coast data center contained a brief period of writes that had not been replicated to the US West Coast facility. Because the database clusters in both data centers now contained writes that were not present in the other data center, we were unable to fail the primary back over to the US East Coast data center safely."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "2018 October 21 22:54 UTC"}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "Our internal monitoring systems began generating alerts indicating that our systems were experiencing numerous faults. At this time there were several engineers responding and working to triage the incoming notifications. By 23:02 UTC, engineers in our first responder team had determined that topologies for numerous database clusters were in an unexpected state. Querying the Orchestrator API displayed a database replication topology that only included servers from our US West Coast data center."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "2018 October 21 23:07 UTC"}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "By this point the responding team decided to manually lock our internal deployment tooling to prevent any additional changes from being introduced. At 23:09 UTC, the responding team placed the site into yellow status. This action automatically escalated the situation into an active incident and sent an alert to the incident coordinator. At 23:11 UTC the incident coordinator joined and two minutes later made the decision change to status red."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "2018 October 21 23:13 UTC"}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "It was understood at this time that the problem affected multiple database clusters. Additional engineers from GitHub\u2019s database engineering team were paged. They began investigating the current state in order to determine what actions needed to be taken to manually configure a US East Coast database as the primary for each cluster and rebuild the replication topology. This effort was challenging because by this point the West Coast database cluster had ingested writes from our application tier for nearly 40 minutes. Additionally, there were the several seconds of writes that existed in the East Coast cluster that had not been replicated to the West Coast and prevented replication of new writes back to the East Coast."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "Guarding the confidentiality and integrity of user data is GitHub\u2019s highest priority. In an effort to preserve this data, we decided that the 30+ minutes of data written to the US West Coast data center prevented us from considering options other than failing-forward in order to keep user data safe. However, applications running in the East Coast that depend on writing information to a West Coast MySQL cluster are currently unable to cope with the additional latency introduced by a cross-country round trip for the majority of their database calls. This decision would result in our service being unusable for many users. We believe that the extended degradation of service was worth ensuring the consistency of our users\u2019 data."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "In the invalid topology, replication from US West to US East is broken and apps are unable to read from current replicas as they depend on low latency to maintain transaction performance."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "2018 October 21 23:19 UTC"}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "It was clear through querying the state of the database clusters that we needed to stop running jobs that write metadata about things like pushes. We made an explicit choice to partially degrade site usability by pausing webhook delivery and GitHub Pages builds instead of jeopardizing data we had already received from users. In other words, our strategy was to prioritize data integrity over site usability and time to recovery."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "2018 October 22 00:05 UTC"}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "Engineers involved in the incident response team began developing a plan to resolve data inconsistencies and implement our failover procedures for MySQL. Our plan was to restore from backups, synchronize the replicas in both sites, fall back to a stable serving topology, and then resume processing queued jobs. We updated our status to inform users that we were going to be executing a controlled failover of an internal data storage system."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "Overview of recovery plan was to fail forward, synchronize, fall back, then churn through backlogs before returning to green."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "While MySQL data backups occur every four hours and are retained for many years, the backups are stored remotely in a public cloud blob storage service. The time required to restore multiple terabytes of backup data caused the process to take hours. A significant portion of the time was consumed transferring the data from the remote backup service. The process to decompress, checksum, prepare, and load large backup files onto newly provisioned MySQL servers took the majority of time. This procedure is tested daily at minimum, so the recovery time frame was well understood, however until this incident we have never needed to fully rebuild an entire cluster from backup and had instead been able to rely on other strategies such as delayed replicas."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "2018 October 22 00:41 UTC\nA backup process for all affected MySQL clusters had been initiated by this time and engineers were monitoring progress. Concurrently, multiple teams of engineers were investigating ways to speed up the transfer and recovery time without further degrading site usability or risking data corruption."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "2018 October 22 06:51 UTC"}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "Several clusters had completed restoration from backups in our US East Coast data center and begun replicating new data from the West Coast. This resulted in slow site load times for pages that had to execute a write operation over a cross-country link, but pages reading from those database clusters would return up-to-date results if the read request landed on the newly restored replica. Other larger database clusters were still restoring."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "Our teams had identified ways to restore directly from the West Coast to overcome throughput restrictions caused by downloading from off-site storage and were increasingly confident that restoration was imminent, and the time left to establishing a healthy replication topology was dependent on how long it would take replication to catch up. This estimate was linearly interpolated from the replication telemetry we had available and the status page was updated to set an expectation of two hours as our estimated time of recovery."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "2018 October 22 07:46 UTC"}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "GitHub published a blog post to provide more context. We use GitHub Pages internally and all builds had been paused several hours earlier, so publishing this took additional effort. We apologize for the delay. We intended to send this communication out much sooner and will be ensuring we can publish updates in the future under these constraints."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "2018 October 22 11:12 UTC"}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "All database primaries established in US East Coast again. This resulted in the site becoming far more responsive as writes were now directed to a database server that was co-located in the same physical data center as our application tier. While this improved performance substantially, there were still dozens of database read replicas that were multiple hours delayed behind the primary. These delayed replicas resulted in users seeing inconsistent data as they interacted with our services. We spread the read load across a large pool of read replicas and each request to our services had a good chance of hitting a read replica that was multiple hours delayed."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "In reality, the time required for replication to catch up had adhered to a power decay function instead of a linear trajectory. Due to increased write load on our database clusters as users woke up and began their workday in Europe and the US, the recovery process took longer than originally estimated."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "2018 October 22 13:15 UTC"}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "By now, we were approaching peak traffic load on GitHub.com. A discussion was had by the incident response team on how to proceed. It was clear that replication delays were increasing instead of decreasing towards a consistent state. We\u2019d begun provisioning additional MySQL read replicas in the US East Coast public cloud earlier in the incident. Once these became available it became easier to spread read request volume across more servers. Reducing the utilization in aggregate across the read replicas allowed replication to catch up."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "2018 October 22 16:24 UTC"}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "Once the replicas were in sync, we conducted a failover to the original topology, addressing the immediate latency/availability concerns. As part of a conscious decision to prioritize data integrity over a shorter incident window, we kept the service status red while we began processing the backlog of data we had accumulated."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "2018 October 22 16:45 UTC"}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "During this phase of the recovery, we had to balance the increased load represented by the backlog, potentially overloading our ecosystem partners with notifications, and getting our services back to 100% as quickly as possible. There were over five million hook events and 80 thousand Pages builds queued."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "As we re-enabled processing of this data, we processed ~200,000 webhook payloads that had outlived an internal TTL and were dropped. Upon discovering this, we paused that processing and pushed a change to increase that TTL for the time being.\n\nTo avoid further eroding the reliability of our status updates, we remained in degraded status until we had completed processing the entire backlog of data and ensured that our services had clearly settled back into normal performance levels.\n\n2018 October 22 23:03 UTC\nAll pending webhooks and Pages builds had been processed and the integrity and proper operation of all systems had been confirmed. The site status was updated to green."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "Next steps\nResolving data inconsistencies"}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "During our recovery, we captured the MySQL binary logs containing the writes we took in our primary site that were not replicated to our West Coast site from each affected cluster. The total number of writes that were not replicated to the West Coast was relatively small. For example, one of our busiest clusters had 954 writes in the affected window. We are currently performing an analysis on these logs and determining which writes can be automatically reconciled and which will require outreach to users. We have multiple teams engaged in this effort, and our analysis has already determined a category of writes that have since been repeated by the user and successfully persisted. As stated in this analysis, our primary goal is preserving the integrity and accuracy of the data you store on GitHub."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "Communication"}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "In our desire to communicate meaningful information to you during the incident, we made several public estimates on time to repair based on the rate of processing of the backlog of data. In retrospect, our estimates did not factor in all variables. We are sorry for the confusion this caused and will strive to provide more accurate information in the future."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "Technical initiatives\nThere are a number of technical initiatives that have been identified during this analysis. As we continue to work through an extensive post-incident analysis process internally, we expect to identify even more work that needs to happen."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "Adjust the configuration of Orchestrator to prevent the promotion of database primaries across regional boundaries. Orchestrator\u2019s actions behaved as configured, despite our application tier being unable to support this topology change. Leader-election within a region is generally safe, but the sudden introduction of cross-country latency was a major contributing factor during this incident. This was emergent behavior of the system given that we hadn\u2019t previously seen an internal network partition of this magnitude."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "We have accelerated our migration to a new status reporting mechanism that will provide a richer forum for us to talk about active incidents in crisper and clearer language. While many portions of GitHub were available throughout the incident, we were only able to set our status to green, yellow, and red. We recognize that this doesn\u2019t give you an accurate picture of what is working and what is not, and in the future will be displaying the different components of the platform so you know the status of each service."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "In the weeks prior to this incident, we had started a company-wide engineering initiative to support serving GitHub traffic from multiple data centers in an active/active/active design. This project has the goal of supporting N+1 redundancy at the facility level. The goal of that work is to tolerate the full failure of a single data center failure without user impact. This is a major effort and will take some time, but we believe that multiple well-connected sites in a geography provides a good set of trade-offs. This incident has added urgency to the initiative."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "We will take a more proactive stance in testing our assumptions. GitHub is a fast growing company and has built up its fair share of complexity over the last decade. As we continue to grow, it becomes increasingly difficult to capture and transfer the historical context of trade-offs and decisions made to newer generations of Hubbers."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "Organizational initiatives"}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "This incident has led to a shift in our mindset around site reliability. We have learned that tighter operational controls or improved response times are insufficient safeguards for site reliability within a system of services as complicated as ours. To bolster those efforts, we will also begin a systemic practice of validating failure scenarios before they have a chance to affect you. This work will involve future investment in fault injection and chaos engineering tooling at GitHub."}
{"title": "October 21 post-incident analysis", "url": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "snippet": "Conclusion\nWe know how much you rely on GitHub for your projects and businesses to succeed. No one is more passionate about the availability of our services and the correctness of your data. We will continue to analyze this event for opportunities to serve you better and earn the trust you place in us."}
{"title": "Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "We want to provide you with some additional information about the service disruption that occurred in the Northern Virginia (US-EAST-1) Region on December 7th, 2021.\n\nIssue Summary"}
{"title": "Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "To explain this event, we need to share a little about the internals of the AWS network. While the majority of AWS services and all customer applications run within the main AWS network, AWS makes use of an internal network to host foundational services including monitoring, internal DNS, authorization services, and parts of the EC2 control plane. Because of the importance of these services in this internal network, we connect this network with multiple geographically isolated networking devices and scale the capacity of this network significantly to ensure high availability of this network connection. These networking devices provide additional routing and network address translation that allow AWS services to communicate between the internal network and the main AWS network. At 7:30 AM PST, an automated activity to scale capacity of one of the AWS services hosted in the main AWS network triggered an unexpected behavior from a large number of clients inside the internal network. This resulted in a large surge of connection activity that overwhelmed the networking devices between the internal network and the main AWS network, resulting in delays for communication between these networks. These delays increased latency and errors for services communicating between these networks, resulting in even more connection attempts and retries. This led to persistent congestion and performance issues on the devices connecting the two networks."}
{"title": "Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "This congestion immediately impacted the availability of real-time monitoring data for our internal operations teams, which impaired their ability to find the source of congestion and resolve it. Operators instead relied on logs to understand what was happening and initially identified elevated internal DNS errors. Because internal DNS is foundational for all services and this traffic was believed to be contributing to the congestion, the teams focused on moving the internal DNS traffic away from the congested network paths. At 9:28 AM PST, the team completed this work and DNS resolution errors fully recovered. This change improved the availability of several impacted services by reducing load on the impacted networking devices, but did not fully resolve the AWS service impact or eliminate the congestion. Importantly, monitoring data was still not visible to our operations team so they had to continue resolving the issue with reduced system visibility. Operators continued working on a set of remediation actions to reduce congestion on the internal network including identifying the top sources of traffic to isolate to dedicated network devices, disabling some heavy network traffic services, and bringing additional networking capacity online. This progressed slowly for several reasons. First, the impact on internal monitoring limited our ability to understand the problem. Second, our internal deployment systems, which run in our internal network, were impacted, which further slowed our remediation efforts. Finally, because many AWS services on the main AWS network and AWS customer applications were still operating normally, we wanted to be extremely deliberate while making changes to avoid impacting functioning workloads. As the operations teams continued applying the remediation actions described above, congestion significantly improved by 1:34 PM PST, and all network devices fully recovered by 2:22 PM PST."}
{"title": "Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "We have taken several actions to prevent a recurrence of this event. We immediately disabled the scaling activities that triggered this event and will not resume them until we have deployed all remediations. Our systems are scaled adequately so that we do not need to resume these activities in the near-term. Our networking clients have well tested request back-off behaviors that are designed to allow our systems to recover from these sorts of congestion events, but, a latent issue prevented these clients from adequately backing off during this event. This code path has been in production for many years but the automated scaling activity triggered a previously unobserved behavior. We are developing a fix for this issue and expect to deploy this change over the next two weeks. We have also deployed additional network configuration that protects potentially impacted networking devices even in the face of a similar congestion event. These remediations give us confidence that we will not see a recurrence of this issue."}
{"title": "Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "AWS Service Impact"}
{"title": "Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "While AWS customer workloads were not directly impacted from the internal networking issues described above, the networking issues caused impact to a number of AWS Services which in turn impacted customers using these service capabilities. Because the main AWS network was not affected, some customer applications which did not rely on these capabilities only experienced minimal impact from this event."}
{"title": "Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "Several AWS services experienced impact to the control planes that are used for creating and managing AWS resources. These control planes use services hosted in the internal network. For example, while running EC2 instances were unaffected by this event, the EC2 APIs that customers use to launch new instances or to describe their current instances experienced increased error rates and latencies starting at 7:33 AM PST. By 1:15 PM PST, as congestion was improving, EC2 API error rates and latencies began to improve, except for launches of new EC2 instances, which recovered by 2:40 PM PST. Customers of AWS services like Amazon RDS, EMR, Workspaces would not have been able to create new resources because of the inability to launch new EC2 instances during the event. Similarly, existing Elastic Load Balancers remained healthy during the event, but the elevated API error rates and latencies for the ELB APIs resulted in increased provisioning times for new load balancers and delayed instance registration times for adding new instances to existing load balancers. Additionally, Route 53 APIs were impaired from 7:30 AM PST until 2:30 PM PST preventing customers from making changes to their DNS entries, but existing DNS entries and answers to DNS queries were not impacted during this event. Customers also experienced login failures to the AWS Console in the impacted region during the event. Console access was fully restored by 2:22 PM PST. Amazon Secure Token Service (STS) experienced elevated latencies when providing credentials for third party identity providers via OpenID Connect (OIDC). This resulted in login failures for other AWS services that utilize STS for authentication, such as Redshift. While latencies improved at 2:22 PM PST when the issue affecting network devices was addressed, full recovery for STS occurred at 4:28 PM PST."}
{"title": "Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "Customers were also impacted by CloudWatch monitoring delays throughout this event and, as a result, found it difficult to understand impact to their applications. A small amount of CloudWatch monitoring data was not captured during this event and may be missing from some metrics for parts of the event."}
{"title": "Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "Customers accessing Amazon S3 and DynamoDB were not impacted by this event. However, access to Amazon S3 buckets and DynamoDB tables via VPC Endpoints was impaired during this event."}
{"title": "Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "AWS Lambda APIs and invocation of Lambda functions operated normally throughout the event. However, API Gateway, which is often used to invoke Lambda functions as well as an API management service for customer applications, experienced increased error rates. API Gateway servers were impacted by their inability to communicate with the internal network during the early part of this event. As a result of these errors, many API Gateway servers eventually got into a state where they needed to be replaced in order to serve requests successfully. This normally happens through an automated recycling process, but this was not possible until the EC2 APIs began recovering. While API Gateways began seeing recovery at 1:35 PM PST, errors and latencies remained elevated as API Gateway capacity was recycled by the automated process working through the backlog of affected servers. The service largely recovered by 4:37 PM PST, but API Gateway customers may have continued to experience low levels of errors and throttling for several hours as API Gateways fully stabilized. The API Gateway team is working on a set of mitigations to ensure that API Gateway servers remain healthy even when the internal network is unavailable and making improvements to the recycling process to speed recovery efforts in the event of a similar issue in the future. EventBridge, which is also often used in conjunction with Lambda, experienced elevated errors during the initial phases of the event but saw some improvement at 9:28 AM PST when the internal DNS issue was resolved. However, during mitigation efforts to reduce the load on the affected network devices, operators disabled event delivery for EventBridge at 12:35 PM. Event delivery was re-enabled at 2:35 PM PST, however the service experienced elevated event delivery latency until 6:40 PM PST as it processed the backlog of events."}
{"title": "Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "The AWS container services, including Fargate, ECS and EKS, experienced increased API error rates and latencies during the event. While existing container instances (tasks or pods) continued to operate normally during the event, if a container instance was terminated or experienced a failure, it could not be restarted because of the impact to the EC2 control plane APIs described above. At 1:35 PM PST, most of the container-related API error rates returned to normal, but Fargate experienced increased request load due to the backlog of container instances that needed to be started, which led to continued elevated error rates and Insufficient Capacity Errors as container capacity pools were being replenished. At 5:00 PM PST, Fargate API error rates began to return to normal levels. Some customers saw elevated Insufficient Capacity Errors for \u201c4 vCPU\u201d task sizes for several hours following recovery."}
{"title": "Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "Amazon Connect experienced elevated failure rates for handling phone calls, chat sessions, and task contacts during the event. Issues with API Gateways used by Connect for the execution of Lambda functions resulted in elevated failure rates for inbound phone calls, chat sessions or task contacts. At 4:41 PM PST, when the affected API Gateway fully recovered, Amazon Connect resumed normal operations."}
{"title": "Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "Event Communication"}
{"title": "Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "We understand that events like this are more impactful and frustrating when information about what\u2019s happening isn\u2019t readily available. The impairment to our monitoring systems delayed our understanding of this event, and the networking congestion impaired our Service Health Dashboard tooling from appropriately failing over to our standby region. By 8:22 AM PST, we were successfully updating the Service Health Dashboard. As the impact to services during this event all stemmed from a single root cause, we opted to provide updates via a global banner on the Service Health Dashboard, which we have since learned makes it difficult for some customers to find information about this issue. Our Support Contact Center also relies on the internal AWS network, so the ability to create support cases was impacted from 7:33 AM until 2:25 PM PST. We have been working on several enhancements to our Support Services to ensure we can more reliably and quickly communicate with customers during operational issues. We expect to release a new version of our Service Health Dashboard early next year that will make it easier to understand service impact and a new support system architecture that actively runs across multiple AWS regions to ensure we do not have delays in communicating with customers."}
{"title": "Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "In closing"}
{"title": "Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "Finally, we want to apologize for the impact this event caused for our customers. While we are proud of our track record of availability, we know how critical our services are to our customers, their applications and end users, and their businesses. We know this event impacted many customers in significant ways. We will do everything we can to learn from this event and use it to improve our availability even further."}
{"title": "Post Incident Review (PIR) - Azure Monitor - Logs data access issues", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "What happened?"}
{"title": "Post Incident Review (PIR) - Azure Monitor - Logs data access issues", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Between 23:15 UTC on 6 July 2023 and 09:00 UTC on 7 July 2023, a subset of data for Azure Monitor Log Analytics and Microsoft Sentinel failed to ingest. Additionally, platform logs gathered via Diagnostic Settings failed to route some data to customer destinations such as Log Analytics, Storage, Event Hub and Marketplace. These failures were caused by a deployment of a service within Microsoft, with a bug that caused a much higher than expected call volume that overwhelmed the telemetry management control plane. Customers in all regions experienced impact."}
{"title": "Post Incident Review (PIR) - Azure Monitor - Logs data access issues", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Security Operations Center (SOC) functionality in Sentinel may have been impacted. Queries against impacted tables with date range listed above, inclusive of the logs data that we failed to ingest, might have returned partial or empty results. This includes analytics (detections), hunting queries, workbooks with custom queries, and notebooks. In cases where Event or Security Event tables were impacted, incident investigations of a correlated incident may have showed partial or empty results."}
{"title": "Post Incident Review (PIR) - Azure Monitor - Logs data access issues", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "What went wrong and why?"}
{"title": "Post Incident Review (PIR) - Azure Monitor - Logs data access issues", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "A code deployment for the Azure Container Apps service was started on 3 July 2023 via the normal Safe Deployment Practices (SDP), first rolling out to Azure canary and staging regions. This version contained a misconfiguration that blocked the service from starting normally. Due to the misconfiguration, the service bootstrap code threw an exception, and was automatically restarted. This caused the bootstrap service to be stuck in a loop where it was being restarted every 5 to 10 seconds. Each time the bootstrap service was restarted, it provided configuration information to the telemetry agents also installed on the service hosts. Each time the configuration information was sent to the telemetry hosts, they interpreted this as a configuration change, and therefore they also automatically exited their current process and restarted as well. Three separate instances of the agent telemetry host, per application host, were now also restarting every 5 to 10 seconds."}
{"title": "Post Incident Review (PIR) - Azure Monitor - Logs data access issues", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Upon each startup of the telemetry agent, the agent immediately contacted the telemetry control plane to download the latest version of the telemetry configuration. Normally this is an action that would take place one time every several days, as this configuration would be cached on the agent. However, as the deployment of the Container Apps service progressed, several hundred hosts now had their telemetry agents requesting startup configuration information from the telemetry control plane every 5-10 seconds. The Container Apps team detected the fault in their deployment on 6 July 2023, stopped the original deployment before it was released to any production regions, and started a new deployment of their service in the canary and staging regions to correct the misconfiguration."}
{"title": "Post Incident Review (PIR) - Azure Monitor - Logs data access issues", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "However, the aggregate rate of requests from the services that received the build with the misconfiguration exhausted capacity on the telemetry control plane. The telemetry control plane is a global service, used by services running in all public regions of Azure. As capacity on the control plane was saturated, other services involved in ingestion of telemetry, such as the ingestion front doors and the pipeline services that route data between services internally, began to fail as their operations against the telemetry control plane were either rejected or timed out. The design of the telemetry control plane as a single point of failure is a known risk, and investment to eliminate this risk has been underway in Azure Monitor to design this risk out of the system."}
{"title": "Post Incident Review (PIR) - Azure Monitor - Logs data access issues", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "How did we respond?"}
{"title": "Post Incident Review (PIR) - Azure Monitor - Logs data access issues", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "The impact on the telemetry control plane grew slowly and did not create problems that were detected until 12:30 UTC on 6 July 2023. When the issues were detected, the source of the additional load against the telemetry control plane was not known, but the team suspected additional load had been created against the control plane and took these actions:"}
{"title": "Post Incident Review (PIR) - Azure Monitor - Logs data access issues", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "6 July 2023 @ 14:53 UTC \u2013 Internal incident bridge created.\n6 July 2023 @ 15:56 UTC \u2013 ~500 instances of garbage collector service were removed, to reduce load on telemetry control plane\n6 July 2023 @ 16:09 UTC \u2013 First batch of Node Diagnostics servers were removed, to reduce load on telemetry control plane. This process of removing this type of server continued over the next 10 hours.\n6 July 2023 @ 20:20 UTC \u2013 Source of anomalously high traffic was identified, and the responsible team was paged to assist.\n6 July 2023 @ 23:00 UTC \u2013 IP address blocks deployed, to prevent anomalous traffic from hitting telemetry control plane.\n6 July 2023 @ 23:15 UTC \u2013 External customer impact started, as cached data started to expire.\n7 July 2023 @ 01:30 UTC \u2013 Three additional clusters were added to telemetry control plane to handle additional load, and we began restarting existing clusters to clear backlogged connections.\n7 July 2023 @ 02:19 UTC \u2013 Initial customer notification was posted to the Azure Status page, to acknowledge the incident was being investigated while we worked to identify which specific subscriptions were impacted.\n7 July 2023 @ 02:45 UTC \u2013 An additional three clusters were added to telemetry control plane.\n7 July 2023 @ 06:15 UTC \u2013 Targeted customer notifications sent via Azure Service Health to customers with impacted subscriptions (sent on Tracking ID XMGF-5Z0).\n7 July 2023 @ 09:00 UTC \u2013 Incident declared mitigated, as call error rate and call latency against control plane APIs stabilized at typical levels.\nHow are we making incidents like this less likely or less impactful?"}
{"title": "Post Incident Review (PIR) - Azure Monitor - Logs data access issues", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "We know customer trust is earned and must be maintained, not just by saying the right thing but by doing the right thing. Data retention is a fundamental responsibility of the Microsoft cloud, including every engineer working on every cloud service. We have learned from this incident and are committed to the following improvements:"}
{"title": "Post Incident Review (PIR) - Azure Monitor - Logs data access issues", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "We have ensured that our telemetry control plane services are now running with additional capacity (Completed)\nWe have created additional alerting on certain metrics that indicate critical, unusual failure patterns in API calls (Completed)\nWe will be adding new positive caching and negative caching to the control plane, to reduce load on backing store (Estimated completion: September 2023)\nWe are putting in place additional throttling and circuit breaker patterns to our core telemetry control plane APIs (Estimated completion: September 2023)\nIn the longer term, we are creating isolation between internal-facing and external-facing services using the telemetry control plane (Estimated completion: December 2023) \nHow can customers make incidents like this less impactful?"}
{"title": "Post Incident Review (PIR) - Azure Monitor - Logs data access issues", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Note that the Azure Monitoring Agent (AMA) provides more advanced collection and ingestion resilience capabilities (such as caching, buffering and retries) than the legacy Microsoft Monitoring Agent (MMA). Customers who have not yet completed their migration from MMA to AMA, would benefit from accelerating and completing the migration - for use cases required by them and supported in AMA. For more details: https://learn.microsoft.com/azure/azure-monitor/agents/azure-monitor-agent-migration"}
{"title": "Post Incident Review (PIR) - Azure Monitor - Logs data access issues", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Customers using Microsoft Sentinel can consider the following compensating steps:"}
{"title": "Post Incident Review (PIR) - Azure Monitor - Logs data access issues", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Identify high priority assets, log sources that cover those assets, and detection or hunting logic normally applied to those assets and logs.\nIf possible, run one-time queries at the source, for the indicated date range where ingestion was impacted - based on the prioritized assets, logs, and logic. It may also be useful to run the same queries for week prior to the incident, compare, and look for differences.\nAlternatively, to the extent the collection architecture allows for it, re-ingest data from the source into Sentinel, and run those one-time queries in Sentinel.\nFinally, consider ensuring that the right people in your organization will be notified about any future service issues - by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more: https://aka.ms/ash-alerts"}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "Summary"}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "On 5 October, multiple Google Cloud products experienced networking connectivity issues which impacted new and migrated VMs in the us-central1 region for a duration of 7 hours, 47 minutes. Existing VMs were not directly affected. We sincerely apologize for the impact caused to your business. We have identified the root cause and are taking immediate steps to prevent future failures."}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "Root Cause"}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "The root cause of the issues was a management plane behavior change that had been rolling out slowly across Google Cloud. The aim of the change was to provide better decoupling in processing API updates to GCP Instance Groups and Network Endpoint Groups used as load balancer backends, thus providing better reliability and performance."}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "This change had been rolled out to several regions without incident. However, when it was deployed in us-central1, large workload sizes in the region triggered an unexpected memory increase for the control plane for virtual network routers. The controllers eventually ran out of memory, and although they were automatically restarted, the large workload size meant that they repeated the out-of-memory and restart sequence."}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "Virtual routers and their controllers are deployed into separate zonal failure domains. However, as the management plane change affected a regional API, this extended the issue to all virtual routers in the region, causing synchronized memory pressure and unavailability of controllers."}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "This unavailability of controllers prevented the virtual network routers from being updated with fresh state, such as new VMs, new locations of migrated VMs, dynamic routes, and health state of load balancer backends. As the frequency of out-of-memory events increased, delays in updating router state increased until there was no practical progress being made."}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "Existing VMs that did not migrate and did not change their health state were not affected directly. However, traffic to or from these VMs may have passed through a separate affected device such as a VPN Gateway, internal load balancer, or other VM.\n\nThere are separate sets of virtual routers for intra-region and cross-region traffic, each with their own control plane component. The cross-region routers were affected first and for a longer duration than the intra-region routers."}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "Remediation and Prevention"}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "Google engineers were alerted to slowness in the virtual network control plane in us-central1 on 04 October at 21:45 US/Pacific and immediately started investigations. Initial investigations revealed that slowness was intermittent. At 02:11 US/Pacific on 05 October alerts were received for failures in the virtual network router controllers due to memory exhaustion. Engineers immediately began an attempt to mitigate by allocating more memory. At 03:08 US/Pacific, our networking telemetry began to indicate cross-region packet loss to or from us-central1."}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "By 05:27 US/Pacific, the memory allocation change started to reach production. At 07:00 US/Pacific, the telemetry indicated intra-region packet loss primarily to and from us-central1-c, but it then subsided at 08:15 US/Pacific due to the rollout of the increased memory allocation.\n\nAt 08:22 US/Pacific, the increased memory usage was correlated with the rollout of the management plane change. At 08:52 US/Pacific, a rollback of the management plane change was started, completing in us-central1 at 09:35 US/Pacific. At this point all out of memory events had stopped.\n\nWhile impact had been greatly reduced, a small number of routers were not accepting updates and had to be manually restarted. These restarts did not cause any additional packet loss. By 10:55 US/Pacific all packet loss had stopped and the control plane was processing updates normally.\n\nIf your service or application was affected, we apologize \u2014 this is not the level of quality and reliability we strive to offer you. Google is committed to preventing a repeat of this issue in the future and is completing the following actions:"}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "Proactive alerting of memory risks and unexpected increases.\nRefactoring our deployment configuration to allow engineers to reallocate memory much more quickly.\nRe-evaluating existing and establishing new practices and safety mechanisms for API reconciliation.\nIncreasing visibility of management plane changes across teams so that they can be correlated more quickly.\nAdjusting our deployment footprint to reduce the chance of simultaneous regional memory exhaustion due to regional API changes.\nMemory optimizations in the traffic routers and their controllers to prevent unnecessary overhead.\nDetailed Description of Impact"}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "On 5 October 2023 from 03:08 to 10:55 US/Pacific, multiple Google cloud products experienced networking connectivity issues in us-central1. Newly created and recently migrated VMs experienced extended delays before networking became functional. This impacted higher level workloads that rely on provisioning VMs."}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "Virtual Private Cloud:"}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "Newly created VMs for some projects in us-central1 experienced extended delays before networking became funcitonal.\nLive migrated VMs for some projects in us-central1 experienced extended loss of connectivity after migrating.\nHealth check state of VMs in some projects in us-central1 were not being propagated to load balancers in a timely manner.\nFrom 3:08 to 10:55, impact was largely limited to cross-regional virtual network traffic. From 7:00 to 8:15, there was a substantial impact on intra-region flows.\nGoogle Kubernetes Engine:"}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "Up to 0.4 percent of clusters in us-central1 may have experienced downtime and/or delays during the cluster operations such as recreate and upgrade.\n24 percent of cluster creation attempts experienced failure or delay. For the majority, no action was needed with the operations succeeding after a period of up to 120 minutes.\nNote that affected operations may have been triggered automatically by Google, e.g. autoupgrade or node repair, as well as by customers.\nDuring the downtime, in the Google Cloud Console Clusters page, customers may have seen that some Nodes had not registered and the cluster was unhealthy.\nCloud Data Fusion:"}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "New Cloud Data Fusion VM creation may have failed in the us-central1 region. This issue impacted existing instance operations in the us-central1 region.\nAround 27 percent of the total Data Fusion requests in us-central1 at the time encountered issues.\nCloud Filestore:"}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "New Filestore VMs created during upgrades, were unable to communicate with each other. Filestore upgrades may have failed and were rolled back to the previous version.\nBefore the incident started, only ~6 percent of instances in us-central1-a had been updated. This issue prevented the update of the rest.\nUpdate backlogs processed gradually and eventually completed ~1 day later on 06 October.\nCloud SQL:"}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "New Cloud SQL VM creations failed and Cloud SQL databases were unavailable if changes that resulted in a new VM being created were made to the existing instances. (e.g. Update, Self Service Maintenance, Clone/Failover, etc.)\nCloud Dataproc:\n\nNew cluster creations experienced elevated latencies and failures: up to 5 percent of new cluster creations failed in us-central1\nExisting clusters may have failed to execute jobs.\nCloud Dataflow:"}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "Existing jobs experienced degradation in acquiring resources in response to demand (horizontal scaling failures).\nNew jobs faced start up failure or extended initialization latencies.\nContainer downloads from Artifact Registry failed (resulting in failure to instantiate or horizontally scale workloads).\nThe issue impacted less than 5 percent of all Dataflow jobs.\nCloud Datastream:"}
{"title": "Incident affecting Batch, Virtual Private Cloud (VPC), Google Compute Engine, Google Kubernetes Engine, Google Cloud Dataflow, Google Cloud Networking, Google Cloud SQL, Cloud Filestore, Cloud Data Fusion, Google Cloud Dataproc", "url": "https://status.cloud.google.com/incidents/U39RSGjaANJXtjHpRkdq", "snippet": "On 5 October from 08:00 to 10:10 US/Pacific, streams in the us-central1 region experienced delayed ingestion of data due to a high restart rate of the stream\u2019s pods which were in charge of scheduling the ingestion tasks.\nLess than 5 percent of streams in us-central-1 were impacted.\nThe impacted streams had a spike in the \u201cData freshness\u201d and \u201cTotal latency\u201d monitoring metrics in this timeframe."}
{"title": "Summary of the Amazon SimpleDB Service Disruption", "url": "https://aws.amazon.com/message/65649/", "snippet": "We wanted to share what we've learned from our investigation of the June 13 SimpleDB disruption in the US East Region. The service was unavailable to all API calls (except a fraction of the eventually consistent read calls) from 9:16 PM to 11:16 PM (PDT). From 11:16 PM to 1:30 AM, we continued to have elevated error rates for CreateDomain and DeleteDomain API calls."}
{"title": "Summary of the Amazon SimpleDB Service Disruption", "url": "https://aws.amazon.com/message/65649/", "snippet": "SimpleDB is a distributed datastore that replicates customer data across multiple data centers. The service employs servers in various roles. Some servers are responsible for the storage of user data (\u201cstorage nodes\u201d), with each customer Domain replicated across a group of storage nodes. Other nodes store metadata about each customer Domain (\u201cmetadata nodes\u201d), such as which storage nodes it is located on. SimpleDB uses an internal lock service to determine which set of nodes are responsible for a given Domain. This lock service itself is replicated across multiple data centers. Each node handshakes with the lock service periodically to verify it still has responsibility for the data or metadata it hosts."}
{"title": "Summary of the Amazon SimpleDB Service Disruption", "url": "https://aws.amazon.com/message/65649/", "snippet": "In this event, multiple storage nodes became unavailable simultaneously in a single data center (after power was lost to the servers on which these nodes lived). While SimpleDB can handle multiple simultaneous node failures, and has successfully endured larger infrastructure failures in the past without incident, the server failure pattern in this event resulted in a sudden and significant increase in load on the lock service as it rapidly de-registered the failed storage nodes from their respective replication groups. This simultaneous volume resulted in elevated handshake latencies between healthy SimpleDB nodes and the lock service, and the nodes were not able to complete their handshakes prior to exceeding a set \u201chandshake timeout\u201d value. After several handshake retries and subsequent timeouts, SimpleDB storage and metadata nodes removed themselves from the SimpleDB production cluster, and SimpleDB API requests returned error messages (http response code 500 for server-side error). The affected storage nodes were not able to rejoin the SimpleDB cluster and serve API requests until receiving authorization to rejoin from metadata nodes. This process ensures that we do not allow a node with stale data to join the production cluster accidentally and start serving customer requests. However, in this case the metadata nodes were also down due to the same handshake timeout issue, and therefore could not authenticate the storage nodes."}
{"title": "Summary of the Amazon SimpleDB Service Disruption", "url": "https://aws.amazon.com/message/65649/", "snippet": "Once the problem was identified, we had to manually increase the handshake timeout values and restart a subset of metadata nodes so that they could authorize the storage nodes. This allowed the affected storage nodes to rejoin the SimpleDB cluster and resume serving customer data requests. At this point (11:16 PM), all APIs but CreateDomain and DeleteDomain were functioning normally. To allow the rest of the metadata nodes to fully recover without risk, we throttled CreateDomain and DeleteDomain API calls (which are served from metadata nodes) until 1:30 AM."}
{"title": "Summary of the Amazon SimpleDB Service Disruption", "url": "https://aws.amazon.com/message/65649/", "snippet": "We have identified two significant improvements that can be made to SimpleDB coming out of the event to prevent recurrence of similar issues. First, we will set a longer lock service handshake timeout. The original intent behind the low handshake timeout value we set was to enable rapid detection of replica failure. However, hindsight shows the value was too low. Second, the behavior of nodes removing themselves from the SimpleDB cluster immediately after experiencing multiple handshake timeouts increased the scope of the event and caused SimpleDB API errors. Instead, the nodes should have waited and retried handshake requests later with an increased handshake timeout value. We are addressing these two issues immediately and rolling out fixes to all SimpleDB Regions. We apologize for the impact this issue had on SimpleDB customers."}
{"title": "Summary of the Amazon EC2 and Amazon EBS Service Event in the Tokyo (AP-NORTHEAST-1) Region", "url": "https://aws.amazon.com/message/56489/", "snippet": "We\u2019d like to give you some additional information about the service disruption that occurred in the Tokyo (AP-NORTHEAST-1) Region on August 23, 2019. Beginning at 12:36 PM JST, a small percentage of EC2 servers in a single Availability Zone in the Tokyo (AP-NORTHEAST-1) Region shut down due to overheating. This resulted in impaired EC2 instances and degraded EBS volume performance for some resources in the affected area of the Availability Zone. The overheating was due to a control system failure that caused multiple, redundant cooling systems to fail in parts of the affected Availability Zone. The affected cooling systems were restored at 3:21 PM JST and temperatures in the affected areas began to return to normal. As temperatures returned to normal, power was restored to the affected instances. By 6:30 PM JST, the vast majority of affected instances and volumes had recovered. A small number of instances and volumes were hosted on hardware which was adversely affected by the loss of power and excessive heat. It took longer to recover these instances and volumes and some needed to be retired as a result of failures to the underlying hardware."}
{"title": "Summary of the Amazon EC2 and Amazon EBS Service Event in the Tokyo (AP-NORTHEAST-1) Region", "url": "https://aws.amazon.com/message/56489/", "snippet": "In addition to the impact to affected instances and EBS volumes, there was some impact to the EC2 RunInstances API. At 1:21 PM JST, attempts to launch new EC2 instances targeting the impacted Availability Zone and attempts to use the \u201cidempotency token\u201d (a feature which allows customers to retry run instance commands without risking multiple resulting instance launches) with the RunInstances API in the region began to experience error rates. Other EC2 APIs and launches that did not include an \u201cidempotency token,\u201d continued to operate normally. This issue also prevented new launches from Auto Scaling which depends on the \u201cidempotency token\u201d. At 2:51 PM JST, engineers resolved the issue affecting the \u201cidempotency token\u201d and Auto Scaling. Launches of new EC2 instances in the affected Availability Zone continued to fail until 4:05 PM JST, when the EC2 control plane subsystem had been restored in the impacted Availability Zone. Attempts to create new snapshots for affected EBS volumes, also experienced increased error rates during the event."}
{"title": "Summary of the Amazon EC2 and Amazon EBS Service Event in the Tokyo (AP-NORTHEAST-1) Region", "url": "https://aws.amazon.com/message/56489/", "snippet": "This event was caused by a failure of our datacenter control system, which is used to control and optimize the various cooling systems used in our datacenters. The control system runs on multiple hosts for high availability. This control system contains third-party code which allows it to communicate with third-party devices such as fans, chillers, and temperature sensors. It communicates either directly or through embedded Programmable Logic Controllers (PLC) which in turn communicate with the actual devices. Just prior to the event, the datacenter control system was in the process of failing away from one of the control hosts. During this kind of failover, the control system has to exchange information with other control systems and the datacenter equipment it controls (e.g., the cooling equipment and temperature sensors throughout the datacenter) to ensure that the new control host has the most up-to-date information about the state of the datacenter. Due to a bug in the third-party control system logic, this exchange resulted in excessive interactions between the control system and the devices in the datacenter which ultimately resulted in the control system becoming unresponsive. Our datacenters are designed such that if the datacenter control system fails, the cooling systems go into maximum cooling mode until the control system functionality is restored. While this worked correctly in most of the datacenter, in a small portion of the datacenter, the cooling system did not correctly transition to this safe cooling configuration and instead shut down. As an added safeguard, our datacenter operators have the ability to bypass the datacenter control systems and put our cooling system in \u201cpurge\u201d mode to quickly exhaust hot air in the event of a malfunction. The team attempted to activate purge in the affected areas of the datacenter, but this also failed. At this point, temperatures"}
{"title": "Summary of the Amazon EC2 and Amazon EBS Service Event in the Tokyo (AP-NORTHEAST-1) Region", "url": "https://aws.amazon.com/message/56489/", "snippet": "and instead shut down. As an added safeguard, our datacenter operators have the ability to bypass the datacenter control systems and put our cooling system in \u201cpurge\u201d mode to quickly exhaust hot air in the event of a malfunction. The team attempted to activate purge in the affected areas of the datacenter, but this also failed. At this point, temperatures began to rise in the affected part of the datacenter and servers began to power off when they became too hot. Because the datacenter control system was unavailable, the operations team had minimum visibility into the health and state of the datacenter cooling systems. To recover, the team had to manually investigate and reset all of the affected pieces of equipment and put them into a maximum cooling configuration. During this process, it was discovered that the PLCs controlling some of the air handling units were also unresponsive. These controllers needed to be reset. It was the failure of these PLC controllers which prevented the default cooling and \u201cpurge\u201d mode from correctly working. After these controllers were reset, cooling was restored to the affected area of the datacenter and temperatures began to decrease."}
{"title": "Summary of the Amazon EC2 and Amazon EBS Service Event in the Tokyo (AP-NORTHEAST-1) Region", "url": "https://aws.amazon.com/message/56489/", "snippet": "We are still working with our third-party vendors to understand the bug, and subsequent interactions, that caused both the control system and the impacted PLCs to become unresponsive. In the interim, we have disabled the failover mode that triggered this bug on our control systems to ensure we do not have a recurrence of this issue. We have also trained our local operations teams to quickly identify and remediate this situation if it were to recur, and we are confident that we could reset the system before seeing any customer impact if a similar situation was to occur for any reason. Finally, we are working to modify the way that we control the impacted air handling units to ensure that \u201cpurge mode\u201d is able to bypass the PLC controllers completely. This is an approach we have begun using in our newest datacenter designs and will make us even more confident that \u201cpurge mode\u201d will work even if PLCs become unresponsive."}
{"title": "Summary of the Amazon EC2 and Amazon EBS Service Event in the Tokyo (AP-NORTHEAST-1) Region", "url": "https://aws.amazon.com/message/56489/", "snippet": "During this event, EC2 instances and EBS volumes in other Availability Zones in the region were not affected. Customers that were running their applications thoroughly across multiple Availability Zones were able to maintain availability throughout the event. For customers that need the highest availability for their applications, we continue to recommend running applications with this multiple Availability Zone architecture; any application component that can create availability issues for customers should run in this fault tolerant way."}
{"title": "Summary of the Amazon EC2 and Amazon EBS Service Event in the Tokyo (AP-NORTHEAST-1) Region", "url": "https://aws.amazon.com/message/56489/", "snippet": "We apologize for any inconvenience this event may have caused. We know how critical our services are to our customers\u2019 businesses. We are never satisfied with operational performance that is anything less than perfect, and we will do everything we can to learn from this event and drive improvement across our services."}
{"title": "Summary of the Amazon S3 Service Disruption in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/41926/", "snippet": "We\u2019d like to give you some additional information about the service disruption that occurred in the Northern Virginia (US-EAST-1) Region on the morning of February 28th, 2017. The Amazon Simple Storage Service (S3) team was debugging an issue causing the S3 billing system to progress more slowly than expected. At 9:37AM PST, an authorized S3 team member using an established playbook executed a command which was intended to remove a small number of servers for one of the S3 subsystems that is used by the S3 billing process. Unfortunately, one of the inputs to the command was entered incorrectly and a larger set of servers was removed than intended. The servers that were inadvertently removed supported two other S3 subsystems.  One of these subsystems, the index subsystem, manages the metadata and location information of all S3 objects in the region. This subsystem is necessary to serve all GET, LIST, PUT, and DELETE requests. The second subsystem, the placement subsystem, manages allocation of new storage and requires the index subsystem to be functioning properly to correctly operate. The placement subsystem is used during PUT requests to allocate storage for new objects. Removing a significant portion of the capacity caused each of these systems to require a full restart. While these subsystems were being restarted, S3 was unable to service requests. Other AWS services in the US-EAST-1 Region that rely on S3 for storage, including the S3 console, Amazon Elastic Compute Cloud (EC2) new instance launches, Amazon Elastic Block Store (EBS) volumes (when data was needed from a S3 snapshot), and AWS Lambda were also impacted while the S3 APIs were unavailable."}
{"title": "Summary of the Amazon S3 Service Disruption in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/41926/", "snippet": "S3 subsystems are designed to support the removal or failure of significant capacity with little or no customer impact. We build our systems with the assumption that things will occasionally fail, and we rely on the ability to remove and replace capacity as one of our core operational processes. While this is an operation that we have relied on to maintain our systems since the launch of S3, we have not completely restarted the index subsystem or the placement subsystem in our larger regions for many years. S3 has experienced massive growth over the last several years and the process of restarting these services and running the necessary safety checks to validate the integrity of the metadata took longer than expected. The index subsystem was the first of the two affected subsystems that needed to be restarted. By 12:26PM PST, the index subsystem had activated enough capacity to begin servicing S3 GET, LIST, and DELETE requests. By 1:18PM PST, the index subsystem was fully recovered and GET, LIST, and DELETE APIs were functioning normally.  The S3 PUT API also required the placement subsystem. The placement subsystem began recovery when the index subsystem was functional and finished recovery at 1:54PM PST. At this point, S3 was operating normally. Other AWS services that were impacted by this event began recovering. Some of these services had accumulated a backlog of work during the S3 disruption and required additional time to fully recover."}
{"title": "Summary of the Amazon S3 Service Disruption in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/41926/", "snippet": "We are making several changes as a result of this operational event. While removal of capacity is a key operational practice, in this instance, the tool used allowed too much capacity to be removed too quickly. We have modified this tool to remove capacity more slowly and added safeguards to prevent capacity from being removed when it will take any subsystem below its minimum required capacity level. This will prevent an incorrect input from triggering a similar event in the future. We are also auditing our other operational tools to ensure we have similar safety checks. We will also make changes to improve the recovery time of key S3 subsystems. We employ multiple techniques to allow our services to recover from any failure quickly. One of the most important involves breaking services into small partitions which we call cells. By factoring services into cells, engineering teams can assess and thoroughly test recovery processes of even the largest service or subsystem. As S3 has scaled, the team has done considerable work to refactor parts of the service into smaller cells to reduce blast radius and improve recovery. During this event, the recovery time of the index subsystem still took longer than we expected. The S3 team had planned further partitioning of the index subsystem later this year. We are reprioritizing that work to begin immediately."}
{"title": "Summary of the Amazon S3 Service Disruption in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/41926/", "snippet": "From the beginning of this event until 11:37AM PST, we were unable to update the individual services\u2019 status on the AWS Service Health Dashboard (SHD) because of a dependency the SHD administration console has on Amazon S3. Instead, we used the AWS Twitter feed (@AWSCloud) and SHD banner text to communicate status until we were able to update the individual services\u2019 status on the SHD.  We understand that the SHD provides important visibility to our customers during operational events and we have changed the SHD administration console to run across multiple AWS regions."}
{"title": "Summary of the Amazon S3 Service Disruption in the Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/41926/", "snippet": "Finally, we want to apologize for the impact this event caused for our customers. While we are proud of our long track record of availability with Amazon S3, we know how critical this service is to our customers, their applications and end users, and their businesses. We will do everything we can to learn from this event and use it to improve our availability even further."}
{"title": "Incident affecting Chronicle Security", "url": "https://status.cloud.google.com/incidents/kVn7cDezHPmNh3AubTGH", "snippet": "We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Cloud Support using https://cloud.google.com/support (All Times US/Pacific)"}
{"title": "Incident affecting Chronicle Security", "url": "https://status.cloud.google.com/incidents/kVn7cDezHPmNh3AubTGH", "snippet": "Incident Start: 25 July 2023 at 06:30\n\nIncident End: 29 July 2023 at 08:03\n\nDuration: 4 days, 1 hour, 33 minutes\n\nAffected Services and Features:\n\nChronicle Security\n\nRegions/Zones: Multi-regions: us\n\nDescription:"}
{"title": "Incident affecting Chronicle Security", "url": "https://status.cloud.google.com/incidents/kVn7cDezHPmNh3AubTGH", "snippet": "Starting on Tuesday, 25 July at 06:30, Chronicle Security began experiencing a slow down in data processing in the US region. This resulted in stale data for Unified Data Model (UDM) [1] Search and delayed threat detections. Chronicle's data processing returned to normal for new events on Wednesday, 27 July at 13:45, with the last events in the incident window processed by customer rules by Saturday, 29 July at 08:03. From a preliminary analysis, the root cause was a surge in traffic from a single customer that occurred on Saturday 22 July. The traffic surge was exacerbated by a pipeline that did not have sufficient rate limiting and ultimately overloaded our persistence layer."}
{"title": "Incident affecting Chronicle Security", "url": "https://status.cloud.google.com/incidents/kVn7cDezHPmNh3AubTGH", "snippet": "Google engineers mitigated the issue by temporarily limiting the traffic from the high volume customer, removing inter-dependency from as many pipelines as possible, and by disabling several non-critical pipelines.\n\nCustomer Impact:\n\nAll features in Chronicle Security were still working but the impacted users could have got stale data while searching for UDM events and may have experienced missed threat detections for recently ingested telemetry.\n\n[1] - https://cloud.google.com/chronicle/docs/event-processing/udm-overview"}
{"title": "Summary of the AWS Lambda Service Event in Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/061323/", "snippet": "We wanted to provide you with some additional information about the service disruption that occurred in the Northern Virginia (US-EAST-1) Region on June 13th, 2023.\nIssue Summary"}
{"title": "Summary of the AWS Lambda Service Event in Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/061323/", "snippet": "Starting at 11:49 AM PDT on June 13th, 2023, customers experienced increased error rates and latencies for Lambda function invocations within the Northern Virginia (US-EAST-1) Region. Some other AWS services \u2013 including Amazon STS, AWS Management Console, Amazon EKS, Amazon Connect, and Amazon EventBridge \u2013 also experienced increased error rates and latencies as a result of the degraded Lambda function invocations. Lambda function invocations began to return to normal levels at 1:45 PM PDT, and all affected services had fully recovered by 3:37 PM PDT."}
{"title": "Summary of the AWS Lambda Service Event in Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/061323/", "snippet": "To explain this event, we need to share a little about the internals of AWS Lambda. AWS Lambda makes use of a cellular architecture, where each cell consists of multiple subsystems to serve function invocations for customer code. First, the Lambda Frontend is responsible for receiving and routing customer function invocations. Second, the Lambda Invocation Manager is responsible for managing the underlying compute capacity \u2013 in the form of Lambda Execution Environments \u2013 depending on the scale of function invocation traffic on a per-function, per-account basis."}
{"title": "Summary of the AWS Lambda Service Event in Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/061323/", "snippet": "At 10:01 AM PDT, the Lambda Frontend fleet started scaling in response to an increase in service traffic, within the normal daily traffic patterns, in the Northern Virginia (US-EAST-1) Region. At 11:49 AM PDT, the Lambda Frontend fleet, while adding additional compute capacity to handle the increase in service traffic, crossed a capacity threshold that had previously never been reached within a single cell. This triggered a latent software defect which caused Lambda Execution Environments to be successfully allocated for incoming requests, but never fully utilized by the Lambda Frontend. Since Lambda was not able to provision new Lambda Execution Environments for incoming requests, function invocations within the affected cell experienced increased error rates and latencies. Customers triggering Lambda functions through asynchronous or streaming event sources also saw an increase in their event backlog since the events were not being processed. Lambda function invocations within other Lambda cells were not affected by this event."}
{"title": "Summary of the AWS Lambda Service Event in Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/061323/", "snippet": "Engineering teams were immediately engaged and began investigating. By 12:26 PM PDT, engineers had identified the latent software defect and the impact on the provisioning of underlying compute capacity. As an immediate mitigation, engineers were able to confirm that traffic levels had subsided, and initiated a scale down of the Lambda Frontend fleet to a level that no longer triggered the latent software defect. By 1:30 PM PDT, new Lambda function invocations began to see recovery. By 1:45 PM PDT, Lambda was fully recovered for synchronous function invocations. At this stage, the vast majority of affected AWS services began to fully recover. Between 1:45 PM and 3:37 PM PDT, Lambda completed processing the backlog of asynchronous events from various event sources, consistent with customer specified event source specific retry policies. By 3:37 PM PDT, the AWS Lambda service had resumed normal operations, and all services dependent on Lambda were operating normally."}
{"title": "Summary of the AWS Lambda Service Event in Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/061323/", "snippet": "We have taken several actions to prevent a recurrence of this event. We immediately disabled the scaling activities for the Lambda Frontend fleet activities that triggered the event, while we worked to address the latent bug that caused the issue; this bug has since been resolved and deployed to all Regions. This event also uncovered a gap in our Lambda cellular architecture for the scaling of the Lambda Frontend, which allowed a latent bug to cause impact as the affected cell scaled. Lambda has already completed several action items to address the immediate concern with cellular scaling and remains on track to complete a larger effort later this year to ensure that all cells are bounded to a well-tested size to avoid future unexpected scaling issues."}
{"title": "Summary of the AWS Lambda Service Event in Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/061323/", "snippet": "AWS Service Impact"}
{"title": "Summary of the AWS Lambda Service Event in Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/061323/", "snippet": "Several AWS services experienced impact as a result of increased error rates and latencies for Lambda function invocations. Amazon Secure Token Service (STS) experienced elevated error rates between 11:49 AM and 2:10 PM PDT with three distinct periods of impact. AWS Sign-in experienced elevated error rates for SAML federation in the US-EAST-1 Region between 1:13 PM and 2:11 PM PDT. Customers using AWS Sign-in to federate from external identity providers (IDP) using SAML, experienced elevated error rates when throttles were put in place within Amazon STS. Existing IAM sessions were not impacted, but new sign-in federation via SAML was degraded."}
{"title": "Summary of the AWS Lambda Service Event in Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/061323/", "snippet": "Amazon EventBridge supports routing events to Lambda and experienced elevated delivery latencies of up to 801 seconds between 11:49 AM and 1:45 PM PDT. Amazon EKS experience increased errors rates and latencies during the provisioning of new EKS clusters, however existing EKS clusters were not affected. At 1:45 PM PDT, these error rates returned to normal levels when the Lambda function invocation issue was resolved. AWS Management Console for the Northern Virginia (US-EAST-1) Region experienced elevated error rates from 11:48 AM to 2:02 PM PDT. During this time, customers accessing the AWS Management Console saw either an 'AWS Management Console is currently unavailable', or a '504 Time-out' error page. AWS Management Consoles outside of the Northern Virginia (US-EAST-1) Region were not affected by this event."}
{"title": "Summary of the AWS Lambda Service Event in Northern Virginia (US-EAST-1) Region", "url": "https://aws.amazon.com/message/061323/", "snippet": "Amazon Connect uses AWS Lambda to process contacts and agent events. As a result of the Lambda event, Amazon Connect experienced degraded contact handling between 11:49 AM and 1:40 PM PDT. During this time, calls would have failed to connect, and chats and tasks would have failed to initiate. Agents would have experienced issues logging in and using Connect. AWS Support Center functionality was degraded between 11:49 AM and 2:38 PM PDT. During the first eleven minutes of impact, requests to create/view/update support cases may have failed. By 12:00 PM we had restored access to support cases. However, our call and chat functionality remained impaired, making these channels of communication unavailable. The ability to create/view/update cases via the web/email method was not impacted. AWS Support Center functionality was fully restored by 2:38 PM PDT after dependent services recovered alongside AWS Lambda."}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "What happened?"}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Between 07:24 and 19:00 UTC on 16 September 2023, a subset of customers using Virtual Machines (VMs) in the East US region experienced connectivity issues. This incident was triggered when a number of scale units within one of the datacenters in one of the Availability Zones lost power and, as a result, the nodes in these scale units rebooted. While the majority rebooted successfully, a subset of these nodes failed to come back online automatically. This issue caused downstream impact to services that were dependent on these VMs - including SQL Databases, Service Bus and Event Hubs. Impact varied by service and configuration:"}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Virtual Machines were offline during this time. while recovery began at approximately 16:30 UTC, full mitigation was declared at 19:00 UTC. \nWhile the vast majority of zone-redundant Azure SQL Databases leveraging were not impacted, some customers using proxy mode connection may have experienced impact, due to one connectivity gateway not being configured with zone-resilience.\nSQL Databases with \u2018auto-failover groups\u2019 enabled were failed out of the region, incurring approximately eight hours of downtime prior to the failover completing.\nSQL Databases with \u2018active geo-replication\u2019 were able to self-initiate a failover to an alternative region manually to restore availability.\nThe majority of SQL Databases were recovered no later than 19:00 UTC. Customers would have seen gradual recovery over time during mitigation efforts.\nFinally, non-zonal deployments of Service Bus and Event Hubs would have experienced a degradation. Zonal deployments of Service Bus and Event Hubs were unaffected.\n What went wrong and why?"}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "It is not uncommon for datacenters to experience an intermittent loss of power, and one of the ways we protect against this is by leveraging Uninterruptible Power Supplies (UPS). The role of the UPS is to provide stable power to infrastructure during short periods of power fluctuations, so that infrastructure does not fault or go offline. Although we have redundant UPS systems in place for added resilience, this incident was initially triggered by a UPS rectifier failure on a Primary UPS."}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "The UPS was connected to three Static Transfer Switches (STS) \u2013 which are designed to transfer power loads between independent and redundant power sources, without interruption. The STS is designed to remain on the primary source whenever possible, and to transfer back to it when stable power is available again. When the UPS rectifier failed, the STS successfully transferred to the redundant UPS \u2013 but then the primary UPS recovered temporarily, albeit in a degraded state. In this degraded state, the primary UPS is unable to provide stable power for the full load. So, after a 5-second retransfer delay, when the STS transferred from the redundant UPS back to the primary UPS, the primary UPS failed completely."}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "While the STS should then have transferred power back to the redundant UPS, the STS has logic designed to stagger these power transfers when there are multiple transmissions (to and from primary and redundant UPS) happening in a short period of time. This logic prevented the STS from transferring back to the redundant power, after the primary UPS failed completely, which ultimately caused a power loss to a subset of the scale units within the datacenter \u2013 at 07:24 UTC, for 1.9 seconds. This scenario of load transfers, to and from degraded UPS, over a short period of time, was not accounted for in the design. After 1.9 seconds, the load moved to the redundant source automatically for a final time. Our onsite datacenter team validated that stable power was feeding all racks immediately after the event, and verified that all devices were powered on."}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Following the restoration of power, our SQL monitoring immediately observed customer impact, and automatic communications were sent to customers within 12 minutes. SQL telemetry also provided our first indication that some nodes were stuck during the boot up process. When compute nodes come online, they first check the network connectivity, then make multiple attempts to communicate with the preboot execution environment (PXE) server, to ensure that the correct network routing protocols can be applied. If the host cannot find a PXE server, it is designed to retry indefinitely until one becomes available so it can complete the boot process."}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "A previously discovered bug that applied to some of our BIOS software led to several hosts not retrying to connect to a PXE server, and remaining in a stuck state. Although this was a known issue, the initial symptoms led us to believe that there was a potential issue with the network and/or our PXE servers \u2013 troubleshooting these symptoms led to significant delays in correlating to the known BIOS issue. While multiple teams were engaged to help troubleshoot these issues, our attempts at force rebooting multiple nodes were not successful. As such, a significant amount of time was spent exploring additional mitigation options. Unbeknownst to our on call engineering team, these bulk reboot attempts were blocked by an internal approval process, which has been implemented as a safety measure to restrict the number of nodes that are allowed to be forced rebooted at one time. Once we understood all of the factors inhibiting mitigation, at around 16:30 UTC we proceeded to reboot the relevant nodes within the safety thresholds, which mitigated the BIOS issue successfully."}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "One of the mechanisms our platform deploys when VMs enter an unhealthy state is \u2018service healing\u2019 in which our platform automatically redeploys or migrates it to a healthy node. One of the prerequisites to initiate service healing requires a high percentage of nodes to be healthy \u2013 to ensure that, during a major incident, our self-healing systems do not exacerbate the situation. Once we had recovered past the safe threshold, the service healing mechanism initiated for the remainder of the nodes."}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Throughout this incident, we did not have adequate alerting in place, and could not determine which specific VMs were impacted, because our assessment tooling relies on a heartbeat emitted from the compute nodes, which were stuck during the boot up process. Unfortunately, the time taken to understand the nature of this incident meant that communications were delayed. For customers using Service Bus and Event Hubs, this was multiple hours. For customers using Virtual Machines, this was multiple days. As such, we are investigating several communications related repairs, including why automated communications were not able to inform customers with impacted VMs in near real time, as expected."}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "How did we respond?"}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "16 September 2023 @ 07:23 UTC - Loss of power to the three STSs.\n16 September 2023 @ 07:24 UTC - All three downstream STSs fully re-energized.\n16 September 2023 @ 07:33 UTC - Initial customer impact to SQL DB detected via monitoring.\n16 September 2023 @ 07:34 UTC - Communications sent to Azure Service Health for SQL DB customers.\n16 September 2023 @ 11:40 UTC - The relevant compute deployment team engaged to assist in rebooting nodes.\n16 September 2023 @ 12:13 UTC - The infrastructure firmware team was engaged to troubleshoot the BIOS issues.\n16 September 2023 @ 13:38 UTC - Multiple compute nodes attempted to be forcefully rebooted with no success.\n16 September 2023 @ 15:30 UTC - SQL Databases with \u2018auto-failover groups\u2019 were successfully failed over.\n16 September 2023 @ 15:37 UTC - Communications sent to Azure Service Health for Service Bus and Event Hub customers.\n16 September 2023 @ 16:30 UTC - Safety thresholds blocking reboot attempts understood, successful batch rebooting begins.\n16 September 2023 @ 16:37 UTC - Communications published to Azure Status page, in lieu of more accurate impact assessment.\n16 September 2023 @ 19:00 UTC - All compute and SQL nodes successfully mitigated.\n16 September 2023 @ 22:07 UTC - Once mitigation was validated, communications sent to Azure Service Health for SQL, Service Bus, and Event Hub customers.\n19 September 2023 @ 04:10 UTC \u2013 Once VM impact was determined, communications sent to Azure Service Health for VM customers. \nHow are we making incidents like this less likely or less impactful?"}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "First and foremost, we have replaced the failed rectifier inside the UPS. (Completed)\nWe are working with the manufacturer to perform a UPS rectifier failure analysis. (Estimated completion: October 2023)\nWe are reviewing the status of STS automated transfer logic across all of our datacenters. (Estimated completion: October 2023)\nWe are working to modify the STS logic to correct the transfer delay issue. (Estimated completion: December 2023)\nWe have been deploying the fix for the BIOS issue as of January 2023 \u2013 we are expediting rollout. (Estimated completion: June 2024)\nWe are improving our detection of stuck nodes for incidents of this class. (Estimated completion: October 2023)\nWe are improving our automated mitigation of stuck nodes for incidents of this class. (Estimated completion: March 2024)\nWe are improving the resiliency of our automated communication system for incidents of this class. (Estimated completion: October 2023)\nWe are reviewing the status of STS automated transfer in all our sites. (Estimated completion: October 2023)\nFor the issue surrounding Multi-AZ Azure SQL Databases using a proxy mode connection, the fix was already underway before this incident and has since been deployed. (Completed)\nHow can customers make incidents like this less impactful?"}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Consider using Availability Zones (AZs) to run your services across physically separate locations within an Azure region. To help services be more resilient to datacenter-level failures like this one, each AZ provides independent power, networking, and cooling. Many Azure services support zonal, zone-redundant, and/or always-available configurations: https://docs.microsoft.com/azure/availability-zones/az-overview"}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "For mission-critical workloads, customers should consider a multi-region geodiversity strategy to avoid impact from incidents like this one that impacted a single region: https://learn.microsoft.com/training/modules/design-a-geographically-distributed-application/ and https://learn.microsoft.com/azure/architecture/patterns/geodes"}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "We encourage customers to review and follow our guidance and best practices around Azure SQL Database disaster recovery \u2013 practice disaster drills to ensure that your application can handle the cross-region failover gracefully: https://learn.microsoft.com/azure/azure-sql/database/disaster-recovery-guidance"}
{"title": "Post Incident Review (PIR) \u2013 Services impacted by power, BIOS, and Virtual Machine issues \u2013 East US", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "More generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review: https://docs.microsoft.com/azure/architecture/framework/resiliency\nConsider ensuring that the right people in your organization will be notified about any future service issues by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more: https://aka.ms/ash-alerts"}
{"title": "Summary of the AWS Service Event in the Sydney Region", "url": "https://aws.amazon.com/message/4372T8/", "snippet": "We\u2019d like to share more detail about the AWS service disruption that occurred this past weekend in the AWS Sydney Region.  The service disruption primarily affected EC2 instances and their associated Elastic Block Store (\u201cEBS\u201d) volumes running in a single Availability Zone. \n\nLoss of Power"}
{"title": "Summary of the AWS Service Event in the Sydney Region", "url": "https://aws.amazon.com/message/4372T8/", "snippet": "At 10:25 PM PDT on June 4th, our utility provider suffered a loss of power at a regional substation as a result of severe weather in the area. This failure resulted in a total loss of utility power to multiple AWS facilities. In one of the facilities, our power redundancy didn't work as designed, and we lost power to a significant number of instances in that Availability Zone."}
{"title": "Summary of the AWS Service Event in the Sydney Region", "url": "https://aws.amazon.com/message/4372T8/", "snippet": "Normally, when utility power fails, electrical load is maintained by multiple layers of power redundancy. Every instance is served by two independent power delivery line-ups, each providing access to utility power, uninterruptable power supplies (UPSs), and back-up power from generators. If either of these independent power line-ups provides power, the instance will maintain availability. During this weekend\u2019s event, the instances that lost power lost access to both their primary and secondary power as several of our power delivery line-ups failed to transfer load to their generators. These particular power line-ups utilize a technology known as a diesel rotary uninterruptable power supply (DRUPS), which integrates a diesel generator and a mechanical UPS. Under normal operation, the DRUPS uses utility power to spin a flywheel which stores energy. If utility power is interrupted, the DRUPS uses this stored energy to continue to provide power to the datacenter while the integrated generator is turned on to continue to provide power until utility power is restored. The specific signature of this weekend\u2019s utility power failure resulted in an unusually long voltage sag (rather than a complete outage). Because of the unexpected nature of this voltage sag, a set of breakers responsible for isolating the DRUPS from utility power failed to open quickly enough. Normally, these breakers would assure that the DRUPS reserve power is used to support the datacenter load during the transition to generator power. Instead, the DRUPS system\u2019s energy reserve quickly drained into the degraded power grid. The rapid, unexpected loss of power from DRUPS resulted in DRUPS shutting down, meaning the generators which had started up could not be engaged and connected to the datacenter racks. DRUPS shutting down this rapidly and in this fashion is unusual and required some inspection. Once our on-site technicians were able to determine it"}
{"title": "Summary of the AWS Service Event in the Sydney Region", "url": "https://aws.amazon.com/message/4372T8/", "snippet": "drained into the degraded power grid. The rapid, unexpected loss of power from DRUPS resulted in DRUPS shutting down, meaning the generators which had started up could not be engaged and connected to the datacenter racks. DRUPS shutting down this rapidly and in this fashion is unusual and required some inspection. Once our on-site technicians were able to determine it was safe to manually re-engage the power line-ups, power was restored at 11:46PM PDT.\u200e \u200e"}
{"title": "Summary of the AWS Service Event in the Sydney Region", "url": "https://aws.amazon.com/message/4372T8/", "snippet": "Recovery"}
{"title": "Summary of the AWS Service Event in the Sydney Region", "url": "https://aws.amazon.com/message/4372T8/", "snippet": "As power was restored to the affected infrastructure, our automated systems began to bring customers\u2019 EC2 instances and EBS volumes back online. By 1:00 AM PDT, over 80% of the impacted customer instances and volumes were back online and operational. After power recovery, some instances in the Availability Zone experienced DNS resolution failures as the internal DNS hosts for that Availability Zone were brought back online and handled the recovery load. DNS error rates recovered by 2:49 AM PDT."}
{"title": "Summary of the AWS Service Event in the Sydney Region", "url": "https://aws.amazon.com/message/4372T8/", "snippet": "A latent bug in our instance management software led to a slower than expected recovery of the remaining instances. The team worked over the next several hours to manually recover these remaining instances. Instances were recovered continually during this time, and by 8AM PDT, nearly all instances had been recovered."}
{"title": "Summary of the AWS Service Event in the Sydney Region", "url": "https://aws.amazon.com/message/4372T8/", "snippet": "There were also a small number of EBS volumes (less than 0.01% of the volumes in the Availability Zone) that were unable to recover after power was restored.  EBS volumes are replicated to multiple storage servers in the same Availability Zone, which protects against most hardware failure scenarios and allows EBS to provide a 0.1%-0.2% annualized failure rate. This does mean volumes can be lost when multiple servers fail at the same time. During the power event, a small number of storage servers suffered failed hard drives which led to a loss of the data stored on those servers. In cases where both of the replicas were hosted on failed servers, we were unable to automatically restore the volume. After the initial wave of automated recovery, the EBS team focused on manually recovering as many damaged storage servers as possible. This is a slow process, which is why some volumes took much longer to return to service."}
{"title": "Summary of the AWS Service Event in the Sydney Region", "url": "https://aws.amazon.com/message/4372T8/", "snippet": "During the initial part of this event, customers experienced errors when trying to launch new instances, or when trying to scale their auto-scaling groups.  To remediate this, our team had to manually fail away from degraded services in the affected zone. Starting at 11:42 PM PDT, the manual failover was complete and customers were able to launch instances in the unaffected Availability Zones.  When the APIs initially recovered, our systems were delayed in propagating some state changes and making them available via describe API calls. This meant that some customers could not see their newly launched resources, and some existing instances appeared as stuck in pending or shutting down when customers tried to make changes to their infrastructure in the affected Availability Zone. These state delays also increased latency of adding new instances to existing Elastic Load Balancing (ELB) load balancers."}
{"title": "Summary of the AWS Service Event in the Sydney Region", "url": "https://aws.amazon.com/message/4372T8/", "snippet": "Remediation"}
{"title": "Summary of the AWS Service Event in the Sydney Region", "url": "https://aws.amazon.com/message/4372T8/", "snippet": "While we have experienced excellent operational performance from the power configuration used in this facility, it is apparent that we need to enhance this particular design to prevent similar power sags from affecting our power delivery infrastructure. In order to prevent a recurrence of this correlated power delivery line-up failure, we are adding additional breakers to assure that we more quickly break connections to degraded utility power to allow our generators to activate before the UPS systems are depleted."}
{"title": "Summary of the AWS Service Event in the Sydney Region", "url": "https://aws.amazon.com/message/4372T8/", "snippet": "Additionally, we will be taking actions to improve our recovery systems.  The first is to fix the latent issue that led to our recovery systems not being able to automatically recover a subset of customer instances. That fix is already in testing, and will be deployed over the coming days. We will also be starting a program to regularly test our recovery processes on unoccupied, long-running hosts in our fleet. By continually testing our recovery workflows on long-running hosts, we can assure that no latent issues or configuration setting exists that would impact our ability to quickly remediate customer impact when instances need to be recovered."}
{"title": "Summary of the AWS Service Event in the Sydney Region", "url": "https://aws.amazon.com/message/4372T8/", "snippet": "For this event, customers that were running their applications across multiple Availability Zones in the Region were able to maintain availability throughout the event. For customers that need the highest availability for their applications, we continue to recommend running applications with this architecture. We know that it was problematic that for a period of time there were errors and delays for the APIs that launch instances. We are working on changes that will assure our APIs are even more resilient to failure and believe these changes will be rolled out to the Sydney Region in July."}
{"title": "Summary of the AWS Service Event in the Sydney Region", "url": "https://aws.amazon.com/message/4372T8/", "snippet": "In Closing"}
{"title": "Summary of the AWS Service Event in the Sydney Region", "url": "https://aws.amazon.com/message/4372T8/", "snippet": "We apologize for any inconvenience this event caused. We know how critical our services are to our customers\u2019 businesses. We are never satisfied with operational performance that is anything less than perfect, and we will do everything we can to learn from this event and use it to drive improvement across our services."}
{"title": "Git database degraded due to loss of read replicas", "url": "https://github.blog/2023-05-16-addressing-githubs-recent-availability-issues/#may-11-git-database-incident", "snippet": "On May 11, a database cluster serving git data crashed, triggering an automated failover. The failover of the primary was successful, but in this instance read replicas were not attached. The primary cannot handle full read/write load, so an average of 15% of requests for Git data were failed or slow, with peak impact of 26% at the start of the incident. We mitigated this by reattaching the read replicas and the core scenarios recovered. Similar to the May 9 incident, additional work was required to recover pull request push updates, but we were eventually able to achieve full resolution."}
{"title": "Git database degraded due to loss of read replicas", "url": "https://github.blog/2023-05-16-addressing-githubs-recent-availability-issues/#may-11-git-database-incident", "snippet": "Beyond the immediate mitigation work, the top workstreams underway are focused on determining and resolving what caused the cluster to crash and why the failure didn\u2019t leave the cluster in a good state. We want to clarify that the team was already working to understand and address a previous cluster crash as part of a repair item from a different recent incident. This failover replica failure is new."}
{"title": "Summary of the Amazon EC2 DNS Resolution Issues in the Asia Pacific (Seoul) Region (AP-NORTHEAST-2)", "url": "https://aws.amazon.com/message/74876/", "snippet": "We\u2019d like to give you some additional information about the service disruption that occurred in the Seoul (AP-NORTHEAST-2) Region on November 22, 2018. Between 8:19 AM and 9:43 AM KST, EC2 instances experienced DNS resolution issues in the AP-NORTHEAST-2 region. This was caused by a reduction in the number of healthy hosts that were part of the EC2 DNS resolver fleet, which provides a recursive DNS service to EC2 instances. Service was restored when the number of healthy hosts was restored to previous levels. EC2 network connectivity and DNS resolution outside of EC2 instances were not affected by this event."}
{"title": "Summary of the Amazon EC2 DNS Resolution Issues in the Asia Pacific (Seoul) Region (AP-NORTHEAST-2)", "url": "https://aws.amazon.com/message/74876/", "snippet": "The root cause of DNS resolution issues was a configuration update which incorrectly removed the setting that specifies the minimum healthy hosts for the EC2 DNS resolver fleet in the AP-NORTHEAST-2 Region. This resulted in the minimum healthy hosts configuration setting being interpreted as a very low default value that resulted in fewer in-service healthy hosts. With the reduced healthy host capacity for the EC2 DNS resolver fleet, DNS queries from within EC2 instances began to fail. At 8:21 AM KST, the engineering team was alerted to the DNS resolution issue within the AP-NORTHEAST-2 Region and immediately began working on resolution. We identified root cause at 8:48 AM KST and we first ensured that there was no further impact by preventing additional healthy hosts from being removed from service; this took an additional 15 minutes. We then started restoring capacity to previous levels which took the bulk of the recovery time. At 9:43 AM KST, DNS queries from within EC2 instances saw full recovery."}
{"title": "Summary of the Amazon EC2 DNS Resolution Issues in the Asia Pacific (Seoul) Region (AP-NORTHEAST-2)", "url": "https://aws.amazon.com/message/74876/", "snippet": "We are taking multiple steps to prevent recurrence of this issue, some of which are already complete. We have immediately validated and ensured that every AWS region has the correct capacity settings for the EC2 DNS resolver service. We are implementing semantic configuration validation for all EC2 DNS resolver configuration updates, to ensure every region always has sufficient minimum healthy hosts. We are also adding throttling to ensure that only a limited amount of healthy host capacity can be removed from service each hour. This will prevent the downscaling of the EC2 DNS resolver fleet in the event of an invalid configuration parameter."}
{"title": "Summary of the Amazon EC2 DNS Resolution Issues in the Asia Pacific (Seoul) Region (AP-NORTHEAST-2)", "url": "https://aws.amazon.com/message/74876/", "snippet": "Finally, we want to apologize for the impact this event caused for our customers. While we\u2019ve had a strong track record of availability with EC2 DNS, we know how critical this service is to our customers, their applications and end users, and their businesses. We will do everything we can to learn from this event and use it to improve our availability even further.\u200e"}
{"title": "Preliminary Post Incident Review (PIR) \u2013 Services impacted after power issue \u2013 West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "This is our \u201cPreliminary\u201d PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \u201cFinal\u201d PIR with additional details/learnings."}
{"title": "Preliminary Post Incident Review (PIR) \u2013 Services impacted after power issue \u2013 West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Note: During this incident, in addition to our standard communications via Azure Service Health and its alerts, we also communicated via the public Azure Status page. This temporary overcommunication was to ensure that all affected customers received information, described as 'scenario 3' in our documentation: https://aka.ms/StatusPageCriteria. Our final Post-Incident Review (PIR) will be communicated to affected customers through Azure Service Health, then this entry on the Status History page will be removed. As described in our documentation, public PIR postings on this page are reserved for 'scenario 1' incidents - typically broadly impacting incidents across entire zones or regions, or even multiple zones or regions."}
{"title": "Preliminary Post Incident Review (PIR) \u2013 Services impacted after power issue \u2013 West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "What happened?"}
{"title": "Preliminary Post Incident Review (PIR) \u2013 Services impacted after power issue \u2013 West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "A power issue impacted a subset of our infrastructure in a single Availability Zone within the West Europe region, between 07:31 and 09:15 UTC on 20 October 2023. Customers using Azure services including App Service, Cosmos DB, SQL DB, Storage, and Virtual Machines may have experienced availability issues in the affected Availability Zone. While the vast majority of customer impact was mitigated by 09:15 UTC, a long-tail recovery for a small subset of Storage services restored full availability by 17:10 UTC. Beyond service-specific impacts, note that the Azure Resource Manager (ARM) control plane experienced a small dip in availability (total availability dropped to approximately 98.9%) due to issues caused by a downstream dependency on Cosmos DB, with failures returning to pre-incident levels by 08:05 UTC."}
{"title": "Preliminary Post Incident Review (PIR) \u2013 Services impacted after power issue \u2013 West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "What went wrong and why?"}
{"title": "Preliminary Post Incident Review (PIR) \u2013 Services impacted after power issue \u2013 West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "We detected instability from the utility power grid in the form of voltage sags/swells to one of our datacenters within physical AZ-01, one of the region\u2019s three Availability Zones. Note that the 'logical' zones used by each customer subscription may correspond to different physical zones \u2013 customers can use the Locations API to understand this mapping, to confirm which resources run in the impacted physical AZ, see: https://learn.microsoft.com/rest/api/resources/subscriptions/list-locations. Due to the prolonged nature of the instability, we decided to transfer load from the grid towards our back-up generators. However, during this process, a critical failure occurred in a section of the electrical distribution system, preventing 10% of our generators from taking load. This failure left the main distribution system offline and the redundant system inaccessible. As a result of this failure, approximately 1% of our server racks in this Availability Zone lost power."}
{"title": "Preliminary Post Incident Review (PIR) \u2013 Services impacted after power issue \u2013 West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "How did we respond?"}
{"title": "Preliminary Post Incident Review (PIR) \u2013 Services impacted after power issue \u2013 West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "The issue was reported by our datacenter team to our incident management team immediately, so relevant on-call engineers started to investigate the impacted equipment in near real time. While investigating, we confirmed that grid power had stabilized, so we transferred power back to the grid at approximately 08:00 UTC. We then started to bring affected infrastructure back online, as per our standard operating procedures. Once networking and storage infrastructure had recovered, compute scale units were brought back online, in turn restoring service to the vast majority of Azure services by 09:15 UTC. In total, five Storage scale units were impacted by this incident. Following power restoration, four recovered completely by 09:10 UTC, while the fifth required hardware diagnostics and part replacements on approximately 5% of its storage nodes. As a result it took longer to restore availability for the last <1% of storage accounts, with downstream impact to customers and services reliant on this final Storage scale unit. By 14:30 UTC, all but a few storage accounts had their availability restored, and by 17:10 UTC, full restoration was complete."}
{"title": "Preliminary Post Incident Review (PIR) \u2013 Services impacted after power issue \u2013 West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "How are we making incidents like this less likely or less impactful?"}
{"title": "Preliminary Post Incident Review (PIR) \u2013 Services impacted after power issue \u2013 West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "This is our \u201cPreliminary\u201d PIR that we endeavor to publish within 3 days of incident mitigation, to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a \u201cFinal\u201d PIR with additional details/learnings \u2013 including repair items related to the datacenter/power trigger event, and any potential repair items for downstream services to recover from scenarios like this one more quickly."}
{"title": "Preliminary Post Incident Review (PIR) \u2013 Services impacted after power issue \u2013 West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "How can customers make incidents like this less impactful?"}
{"title": "Preliminary Post Incident Review (PIR) \u2013 Services impacted after power issue \u2013 West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "Consider using Availability Zones (AZs) to run your services across physically separate locations within an Azure region. To help services be more resilient to datacenter-level failures like this one, each AZ provides independent power, networking, and cooling. Many Azure services support zonal, zone-redundant, and/or always-available configurations: https://docs.microsoft.com/azure/availability-zones/az-overview"}
{"title": "Preliminary Post Incident Review (PIR) \u2013 Services impacted after power issue \u2013 West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "For mission-critical workloads, customers should consider a multi-region geodiversity strategy to avoid impact from incidents like this one that impacted a single region: https://learn.microsoft.com/training/modules/design-a-geographically-distributed-application and https://learn.microsoft.com/azure/architecture/patterns/geodes"}
{"title": "Preliminary Post Incident Review (PIR) \u2013 Services impacted after power issue \u2013 West Europe", "url": "https://azure.status.microsoft/en-us/status/history/", "snippet": "More generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review: https://docs.microsoft.com/azure/architecture/framework/resiliency\nFinally, consider ensuring that the right people in your organization will be notified about any future service issues \u2013 by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more: https://aka.ms/ash-alerts"}
{"title": "Summary of AWS Direct Connect Event in the Tokyo (AP-NORTHEAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "We would like to provide additional information about the AWS Direct Connect service disruption that occurred in the Tokyo (AP-NORTHEAST-1) Region on September 2, 2021. Beginning 7:30 AM JST, Direct Connect customers began to experience intermittent connectivity issues and elevated packet loss for their traffic destined towards the Tokyo Region. This was caused by the failure of a subset of network devices on one of the network layers along the network path from Direct Connect edge locations to the Datacenter network in the Tokyo Region, where customers\u2019 Virtual Private Clouds (VPCs) reside. Customers started seeing recovery by 12:30 PM JST and by 1:42 PM JST, connectivity issues were fully resolved. All other forms of network connectivity, including traffic between Availability Zones, internet connectivity to the Region, and AWS Virtual Private Network (VPN) connectivity (which some customers use as a back-up to Direct Connect) were not impacted. Direct Connect traffic to other AWS Regions was also not impacted."}
{"title": "Summary of AWS Direct Connect Event in the Tokyo (AP-NORTHEAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "On September 2, 2021 at 7:30 AM JST, internal alarms alerted AWS engineers to elevated packet loss for Direct Connect customers connecting to the Tokyo Region. The Direct Connect service provides private connectivity between a customer\u2019s data center and their AWS VPCs by forwarding traffic from the edge locations where AWS interconnects with customers, to the AWS Region through multiple network layers - each with many redundant network devices. These alarms identified that the impact was caused by the failure of several devices in a single layer of the Direct Connect network. While these devices were not correctly forwarding traffic, they were not being removed from the network through the normal automated processes that monitor and remove failed network devices. Our automation instead noticed a higher rate of failed devices than normal and alerted engineers to investigate and take remediation action. When engineers were alerted, they determined that there was enough redundancy at this layer and began removing the impacted devices from service so that traffic could be handled by other healthy devices. In parallel, the team investigated the cause of the failure. While the removal of additional devices provided temporary remediation, several other network devices subsequently began to experience the same failure, resulting in network congestion, connectivity issues, or elevated packet loss for Direct Connect customers. Engineers attempted several mitigations, such as resetting failed devices and slowly bringing them back into service, but the failures continued and the engineers were unable to maintain adequate healthy capacity to fully mitigate the customer impact. Engineers also looked for any recent deployments that may have triggered the failure. By 12:00 PM JST, engineers suspected that the failure may be related to a new protocol that was introduced to optimize the network\u2019s reaction time to infrequent network convergence events and fiber cuts."}
{"title": "Summary of AWS Direct Connect Event in the Tokyo (AP-NORTHEAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "engineers were unable to maintain adequate healthy capacity to fully mitigate the customer impact. Engineers also looked for any recent deployments that may have triggered the failure. By 12:00 PM JST, engineers suspected that the failure may be related to a new protocol that was introduced to optimize the network\u2019s reaction time to infrequent network convergence events and fiber cuts. This new protocol was introduced many months prior and this change had been in production since then without any issues. However, engineers suspected that the failure was related to the interaction of this new protocol and a new traffic pattern on the network devices at this layer of the Direct Connect network. Engineers started disabling this new protocol in a single Availability Zone to monitor and establish sustained recovery, while in parallel preparing the change to be deployed across the Tokyo Region. Customers started reporting recovery to their applications by 12:30 PM JST and by 1:42 PM JST affected networking devices were restored to a stable operational state and the Direct Connect service returned to normal operations."}
{"title": "Summary of AWS Direct Connect Event in the Tokyo (AP-NORTHEAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "While disabling the new protocol resolved the event, engineering teams have continued working to identify the underlying root cause. We have now confirmed that this event was caused by a latent issue within the network device operating system. This version of the operating system enables a new protocol which is used to improve the failover time of our network. The new operating system and protocol have been running successfully in production for multiple months. We use a controlled, automated, tested, and instrumented procedure for changing the operating system and introducing the new protocol to the AWS network. This procedure starts with a series of stress tests in a dedicated lab to validate the resiliency of the network device to both valid and invalid (i.e., malformed) packets. Any anomalies identified in lab testing are diagnosed, root causes identified, and remediated before the new code is released to production. Even with this comprehensive testing, it is not possible to test every traffic and packet permutation in a lab environment. Therefore, AWS uses a deployment procedure that releases network device operating system changes to production in a slow and controlled fashion. This procedure upgrades individual devices in specific places where the upgraded devices can be exposed to production traffic but where traffic can easily fail away from the upgraded devices to non-upgraded devices. During this gradual production deployment, the upgraded devices are extensively monitored for performance issues and functionality errors. This upgrade process has been used many times successfully and was followed with this most recent device operating system upgrade. The new protocol and the operating system were first deployed to production in January 2021. Over the last 8 months, this new protocol and the operating system have been gradually released to production in all AWS Regions and has been serving Direct"}
{"title": "Summary of AWS Direct Connect Event in the Tokyo (AP-NORTHEAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "process has been used many times successfully and was followed with this most recent device operating system upgrade. The new protocol and the operating system were first deployed to production in January 2021. Over the last 8 months, this new protocol and the operating system have been gradually released to production in all AWS Regions and has been serving Direct Connect customer traffic without any indication of the latent issue. Over the last several days, engineers have been able to identify the defect in the network operating system and determined that it requires a very specific set of packet attributes and contents to trigger the issue. While these conditions are very specific and unlikely, this event was triggered by customer traffic that was able to consistently generate packets that matched this signature. We have no reason to suspect malicious intent. We have disabled the new protocol that triggered this issue in the AWS Tokyo Region. We have also developed an enhanced way to detect and remediate this issue before customer impact, as we carefully apply this change to all other AWS Regions. We are confident that there will be no additional customer impact from this issue."}
{"title": "Summary of AWS Direct Connect Event in the Tokyo (AP-NORTHEAST-1) Region", "url": "https://aws.amazon.com/message/12721/", "snippet": "We understand how critical AWS services are for our customers and many businesses in Japan, and we sincerely apologize for the impact that this event may have caused. We have a long track record of operating our services with high levels of availability and will do everything possible to maintain our customers\u2019 trust and help them achieve the availability they need for their customers and businesses."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "July 2, 2012"}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "We\u2019d like to share more about the service disruption which occurred last Friday night, June 29th, in one of our Availability Zones in the US East-1 Region. The event was triggered during a large scale electrical storm which swept through the Northern Virginia area. We regret the problems experienced by customers affected by the disruption and, in addition to giving more detail, also wanted to provide information on actions we\u2019ll be taking to mitigate these issues in the future."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "Our US East-1 Region consists of more than 10 datacenters structured into multiple Availability Zones. These Availability Zones are in distinct physical locations and are engineered to isolate failure from each other. Last Friday, due to weather warnings of the approaching storm, all change activity in the US East-1 Region had been cancelled and extra personnel had been called into the datacenters for the evening."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "On Friday night, as the storm progressed, several US East-1 datacenters in Availability Zones which would remain unaffected by events that evening saw utility power fluctuations. Backup systems in those datacenters responded as designed, resulting in no loss of power or customer impact. At 7:24pm PDT, a large voltage spike was experienced by the electrical switching equipment in two of the US East-1 datacenters supporting a single Availability Zone. All utility electrical switches in both datacenters initiated transfer to generator power. In one of the datacenters, the transfer completed without incident. In the other, the generators started successfully, but each generator independently failed to provide stable voltage as they were brought into service. As a result, the generators did not pick up the load and servers operated without interruption during this period on the Uninterruptable Power Supply (\u201cUPS\u201d) units. Shortly thereafter, utility power was restored and our datacenter personnel transferred the datacenter back to utility power. The utility power in the Region failed a second time at 7:57pm PDT. Again, all rooms of this one facility failed to successfully transfer to generator power while all of our other datacenters in the Region continued to operate without customer impact."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "In the single datacenter that did not successfully transfer to the generator backup, all servers continued to operate normally on Uninterruptable Power Supply (\u201cUPS\u201d) power. As onsite personnel worked to stabilize the primary and backup power generators, the UPS systems were depleting and servers began losing power at 8:04pm PDT. Ten minutes later, the backup generator power was stabilized, the UPSs were restarted, and power started to be restored by 8:14pm PDT. At 8:24pm PDT, the full facility had power to all racks."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "The generators and electrical switching equipment in the datacenter that experienced the failure were all the same brand and all installed in late 2010 and early 2011. Prior to installation in this facility, the generators were rigorously tested by the manufacturer. At datacenter commissioning time, they again passed all load tests (approximately 8 hours of testing) without issue. On May 12th of this year, we conducted a full load test where the entire datacenter switched to and ran successfully on these same generators, and all systems operated correctly. The generators and electrical equipment in this datacenter are less than two years old, maintained by manufacturer representatives to manufacturer standards, and tested weekly. In addition, these generators operated flawlessly, once brought online Friday night, for just over 30 hours until utility power was restored to this datacenter. The equipment will be repaired, recertified by the manufacturer, and retested at full load onsite or it will be replaced entirely. In the interim, because the generators ran successfully for 30 hours after being manually brought online, we are confident they will perform properly if the load is transferred to them. Therefore, prior to completing the engineering work mentioned above, we will lengthen the amount of time the electrical switching equipment gives the generators to reach stable power before the switch board assesses whether the generators are ready to accept the full power load. Additionally, we will expand the power quality tolerances allowed when evaluating whether to switch the load to generator power. We will expand the size of the onsite 24x7 engineering staff to ensure that if there is a repeat event, the switch to generator will be completed manually (if necessary) before UPSs discharge and there is any customer impact."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "Though the resources in this datacenter, including Elastic Compute Cloud (EC2) instances, Elastic Block Store (EBS) storage volumes, Relational Database Service (RDS) instances, and Elastic Load Balancer (ELB) instances, represent a single-digit percentage of the total resources in the US East-1 Region, there was significant impact to many customers. The impact manifested in two forms. The first was the unavailability of instances and volumes running in the affected datacenter. This kind of impact was limited to the affected Availability Zone. Other Availability Zones in the US East-1 Region continued functioning normally. The second form of impact was degradation of service \u201ccontrol planes\u201d which allow customers to take action and create, remove, or change resources across the Region. While control planes aren\u2019t required for the ongoing use of resources, they are particularly useful in outages where customers are trying to react to the loss of resources in one Availability Zone by moving to another."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "EC2 and EBS"}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "Approximately 7% of the EC2 instances in the US-EAST-1 Region were in the impacted Availability Zone and impacted by the power loss. These instances were offline until power was restored and systems restarted. EC2 instances operating in other Availability Zones within the US East-1 Region continued to function as they did prior to the event. Internet connectivity into the Region was unaffected. The vast majority of these instances came back online between 11:15pm PDT and just after midnight. Time for the completion of this recovery was extended by a bottleneck in our server booting process. Removing this bottleneck is one of the actions we\u2019ll take to improve recovery times in the face of power failure. EBS had a comparable percentage (relative to EC2) of its volumes in the Region impacted by this event. The majority of EBS servers had been brought up by 12:25am PDT on Saturday. However, for EBS data volumes that had in-flight writes at the time of the power loss, those volumes had the potential to be in an inconsistent state. Rather than return those volumes in a potentially inconsistent state, once the EBS servers are back up and available, EBS brings customer volumes back online in an impaired state where all I/O on the volume is paused. Customers can then verify the volume is consistent and resume using it. Though the time to recover these EBS volumes has been reduced dramatically over the last 6 months, the number of volumes requiring processing was large enough that it still took several hours to complete the backlog. By 2:45am PDT, 90% of outstanding volumes had been turned over to customers. We have identified several areas in the recovery process that we will further optimize to improve the speed of processing recovered volumes."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "The control planes for EC2 and EBS were significantly impacted by the power failure, and calls to create new resources or change existing resources failed. From 8:04pm PDT to 9:10pm PDT, customers were not able to launch new EC2 instances, create EBS volumes, or attach volumes in any Availability Zone in the US-East-1 Region. At 9:10pm PDT, control plane functionality was restored for the Region. Customers trying to attach or detach impacted EBS volumes would have continued to experienced errors until their impacted EBS volumes were recovered. The duration of the recovery time for the EC2 and EBS control planes was the result of our inability to rapidly fail over to a new primary datastore. The EC2 and EBS APIs are implemented on multi-Availability Zone replicated datastores. These datastores are used to store metadata for resources such as instances, volumes, and snapshots. To protect against datastore corruption, currently when the primary copy loses power, the system automatically flips to a read-only mode in the other Availability Zones until power is restored to the affected Availability Zone or until we determine it is safe to promote another copy to primary. We are addressing the sources of blockage which forced manual assessment and required hand-managed failover for the control plane, and have work already underway to have this flip happen automatically."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "Elastic Load Balancing"}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "Elastic Load Balancers (ELBs) allow web traffic directed at a single IP address to be spread across many EC2 instances. They are a tool for high availability as traffic to a single end-point can be handled by many redundant servers. ELBs live in individual Availability Zones and front EC2 instances in those same zones or in other Availability Zones."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "For single-Availability Zone ELBs, the ELB service maintains one ELB in the specified Availability Zone. If that ELB fails, the ELB control plane assigns its configuration and IP address to another ELB server in that Availability Zone. This normally requires a very short period of time. If there is a large scale issue in the Availability Zone, there may be insufficient capacity to immediately provide a new ELB and replacement will wait for capacity to be made available."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "ELBs can also be deployed in multiple Availability Zones. In this configuration, each Availability Zone\u2019s end-point will have a separate IP address. A single Domain Name will point to all of the end-points\u2019 IP addresses. When a client, such as a web browser, queries DNS with a Domain Name, it receives the IP address (\u201cA\u201d) records of all of the ELBs in random order. While some clients only process a single IP address, many (such as newer versions of web-browsers) will retry the subsequent IP addresses if they fail to connect to the first. A large number of non-browser clients only operate with a single IP address."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "For multi-Availability Zone ELBs, the ELB service maintains ELBs redundantly in the Availability Zones a customer requests them to be in so that failure of a single machine or datacenter won\u2019t take down the end-point. The ELB service avoids impact (even for clients which can only process a single IP address) by detecting failure and eliminating the problematic ELB instance\u2019s IP address from the list returned by DNS. The ELB control plane processes all management events for ELBs including traffic shifts due to failure, size scaling for ELB due to traffic growth, and addition and removal of EC2 instances from association with a given ELB."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "During the disruption this past Friday night, the control plane (which encompasses calls to add a new ELB, scale an ELB, add EC2 instances to an ELB, and remove traffic from ELBs) began performing traffic shifts to account for the loss of load balancers in the affected Availability Zone. As the power and systems returned, a large number of ELBs came up in a state which triggered a bug we hadn\u2019t seen before. The bug caused the ELB control plane to attempt to scale these ELBs to larger ELB instance sizes. This resulted in a sudden flood of requests which began to backlog the control plane. At the same time, customers began launching new EC2 instances to replace capacity lost in the impacted Availability Zone, requesting the instances be added to existing load balancers in the other zones. These requests further increased the ELB control plane backlog. Because the ELB control plane currently manages requests for the US East-1 Region through a shared queue, it fell increasingly behind in processing these requests; and pretty soon, these requests started taking a very long time to complete."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "While direct impact was limited to those ELBs which had failed in the power-affected datacenter and hadn\u2019t yet had their traffic shifted, the ELB service\u2019s inability to quickly process new requests delayed recovery for many customers who were replacing lost EC2 capacity by launching new instances in other Availability Zones. For multi-Availability Zone ELBs, if a client attempted to connect to an ELB in a healthy Availability Zone, it succeeded. If a client attempted to connect to an ELB in the impacted Availability Zone and didn\u2019t retry using one of the alternate IP addresses returned, it would fail to connect until the backlogged traffic shift occurred and it issued a new DNS query. As mentioned, many modern web browsers perform multiple attempts when given multiple IP addresses; but many clients, especially game consoles and other consumer electronics, only use one IP address returned from the DNS query."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "As a result of these impacts and our learning from them, we are breaking ELB processing into multiple queues to improve overall throughput and to allow more rapid processing of time-sensitive actions such as traffic shifts. We are also going to immediately develop a backup DNS re-weighting that can very quickly shift all ELB traffic away from an impacted Availability Zone without contacting the control plane."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "Relational Database Service (RDS)"}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "RDS provides two modes of operation: Single Availability Zone (Single-AZ), where a single database instance operates in one Availability Zone; and Multi Availability Zone (Multi-AZ), where two database instances are synchronously operated in two different Availability Zones. For Multi-AZ RDS, one of the two database instances is the \u201cprimary\u201d and the other is a \u201cstandby.\u201d The primary handles all database requests and replicates to the standby. In the case where a primary fails, the standby is promoted to be the new primary."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "Single-AZ RDS Instances, by default, have backups turned on. When a Single-AZ RDS instance fails, there are two kinds of recovery that are possible. If EBS volumes do not require recovery, the database instance can simply be restarted. If recovery is required, the backups are used to restore the database. In some cases, where backups have been turned off by customers, there can be no recovery and the instance is lost unless manual backups have been taken."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "Multi-AZ RDS Instances detect failure in the primary or standby and immediately take action. If the primary fails, the DNS CNAME record is updated to point to the standby. If the standby fails, a new instance is launched and instantiated from the primary as the new standby. Once failure is confirmed, failover can take place in less than a minute."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "When servers lost power in the impacted datacenter, many Single-AZ RDS instances in that Availability Zone became unavailable. There was no way to recover these instances until servers were powered up, booted, and brought online. By 10pm PDT, a large number of the affected Single-AZ RDS instances had been brought online. There were many remaining instances which required EBS to recover storage volumes. These followed the timeline described above for EBS impact. Once volumes were recovered, customers could apply backups and restore their Single-AZ RDS instances. In addition to the actions noted above with EBS, RDS will be working to improve the speed at which volumes available for recovery can be processed."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "At the point of power loss, most Multi-AZ instances almost instantly promoted their standby in a healthy AZ to \u201cprimary\u201d as expected. However, a small number of Multi-AZ RDS instances did not complete failover, due to a software bug. The bug was introduced in April when we made changes to the way we handle storage failure. It is only manifested when a certain sequence of communication failure is experienced, situations we saw during this event as a variety of server shutdown sequences occurred. This triggered a failsafe which required manual intervention to complete the failover. In most cases, the manual work could be completed without EBS recovery taking place. The majority of remaining Multi-AZ failovers were completed by 11:00pm PDT. The remaining Multi-AZ instances were processed when EBS volume recovery completed for their storage volumes."}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "To address the issues we had with some Multi-AZ RDS Instances failovers, we have a mitigation for the bug in test and will be rolling it out in production in the coming weeks.\n\nFinal Thoughts"}
{"title": "Summary of the AWS Service Event in the US East Region", "url": "https://aws.amazon.com/message/67457/", "snippet": "We apologize for the inconvenience and trouble this caused for affected customers. We know how critical our services are to our customers\u2019 businesses. If you\u2019ve followed the history of AWS, the customer focus we have, and the pace with which we iterate, we think you know that we will do everything we can to learn from this event and use it to drive improvement across our services. We will spend many hours over the coming days and weeks improving our understanding of the details of the various parts of this event and determining how to make further changes to improve our services and processes."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Letter from our co-founders & co-CEOs"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "We want to acknowledge the outage that disrupted service for customers earlier this month. We understand that our products are mission critical to your business, and we don't take that responsibility lightly. The buck stops with us. Full stop. For those customers affected, we are working to regain your trust."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": ":info: At Atlassian, one of our core values is \u201cOpen company, no bullshit\u201d. We bring this value to life in part by openly discussing incidents and using them as opportunities to learn. We are publishing this Post-Incident Review (PIR) for our customers, our Atlassian community, and the broader technical community. Atlassian is proud of our incident management process which emphasizes that a blameless culture and a focus on improving our technical systems and processes are critical to providing high-scale, trustworthy services. While we do our best to avoid any type of incident, we believe that incidents serve as powerful learning opportunities to help us improve."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Rest assured, Atlassian's cloud platform allows us to meet the diverse needs of our over 200,000 cloud customers of every size and across every industry. Prior to this incident, our cloud has consistently delivered 99.9% uptime and exceeded uptime SLAs. We've made long-term investments in our platform and in a number of centralized platform capabilities, with a scalable infrastructure and a steady cadence of security enhancements."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "To our customers and our partners, we thank you for your continued trust and partnership. We hope the details and actions outlined in this document show that Atlassian will continue to provide a world-class cloud platform and a powerful portfolio of products to meet the needs of every team.\n\n-Scott and Mike"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Executive summary"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "On Tuesday, April 5th, 2022, starting at 7:38 UTC, 775 Atlassian customers lost access to their Atlassian products. The outage spanned up to 14 days for a subset of these customers, with the first set of customers being restored on April 8th and all customer sites progressively restored by April 18th."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "This was not a result of a cyberattack and there was no unauthorized access to customer data. Atlassian has a comprehensive data management program with published SLAs and a history of exceeding these SLAs.\n\nAlthough this was a major incident, no customer lost more than five minutes of data. In addition, over 99.6% of our customers and users continued to use our cloud products without any disruption during the restoration activities."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Throughout this document, we refer to those customers whose sites were deleted as part of this incident as \u201caffected\u201d or \u201cimpacted\u201d customers. This PIR provides the exact details of the incident, outlines the steps we took to recover, and describes how we will prevent situations like this from happening in the future. We provide a high-level summary of the incident in this section, with further detail in the remainder of the document."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "What happened?"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "In 2021, we completed the acquisition and integration of a standalone Atlassian app for Jira Service Management and Jira Software called \u201cInsight \u2013 Asset Management\u201d. The functionality of this standalone app was then native within Jira Service Management and no longer available for Jira Software. Because of this, we needed to delete the standalone legacy app on customer sites that had it installed. Our engineering teams used an existing script and process to delete instances of this standalone application, but there were two problems:"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Communication gap. There was a communication gap between the team that requested the deletion and the team that ran the deletion. Instead of providing the IDs of the intended app being marked for deletion, the team provided the IDs of the entire cloud site where the apps were to be deleted."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Insufficient system warnings. The API used to perform the deletion accepted both site and app identifiers and assumed the input was correct \u2013 this meant that if a site ID is passed, a site would be deleted; if an app ID was passed, an app would be deleted. There was no warning signal to confirm the type of deletion (site or app) being requested."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "The script that was executed followed our standard peer-review process that focused on which endpoint was being called and how. It did not cross-check the provided cloud site IDs to validate whether they referred to the Insight App or to the entire site, and the problem was that the script contained the ID for a customer's entire site. The result was an immediate deletion of 883 sites (representing 775 customers) between 07:38 UTC and 08:01 UTC on Tuesday, April 5th, 2022."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "How did we respond?"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Once the incident was confirmed on April 5th at 08:17 UTC, we triggered our major incident management process and formed a cross-functional incident management team. The global incident response team worked 24/7 for the duration of the incident until all sites were restored, validated, and returned to customers. In addition, incident management leaders met every three hours to coordinate the workstreams."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Early on, we realized there would be a number of challenges to simultaneously restoring hundreds of customers with multiple products.\n\nAt the start of the incident, we knew exactly which sites were affected and our priority was to establish communication with the approved owner for each impacted site to inform them of the outage.\n\nHowever, some customer contact information was deleted. This meant that customers could not file support tickets as they normally would. This also meant we did not have immediate access to key customer contacts.\n\nWhat are we doing to prevent situations like this in the future?\n\nWe have taken a number of immediate actions and are committed to making changes to avoid this situation in the future. Here are four specific areas where we have made or will make significant changes:"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Establish universal \"soft deletes\" across all systems. Overall, a deletion of this type should be prohibited or have multiple layers of protections to avoid errors, including staged rollout and tested rollback plan for \u201csoft deletes\u201d. We will globally prevent the deletion of customer data and metadata that has not gone through a soft-delete process."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Accelerate in our Disaster Recovery (DR) program to automate restoration for the multi-site, multi-product deletion events for a larger set of customers. We will leverage the automation and learnings from this incident to accelerate the DR program to meet the recovery time objective (RTO) as defined in our policy for this scale of incident. We will regularly run DR exercises that involve restoring all products for a large set of sites."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Revise incident management process for large-scale incidents. We will improve our standard operating procedure for large-scale incidents and practice it with simulations of this scale of incident. We will update our training and tooling to handle the large number of teams working in parallel."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Create large-scale incident communications playbook. We will acknowledge incidents early, through multiple channels. We will release public communications on incidents within hours. To better reach impacted customers, we will improve the backup of key contacts and retrofit support tooling to enable customers without a valid URL or Atlassian ID to make direct contact with our technical support team."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Our full list of action items is detailed in the full post-incident review below.\n\nOverview of Atlassian\u2019s cloud architecture\nTo understand contributing factors to the incident as discussed throughout this document, it is helpful to first understand the deployment architecture for Atlassian's products, services, and infrastructure."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Atlassian\u2019s cloud hosting architecture"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Atlassian uses Amazon Web Services (AWS) as a cloud service provider and its highly available data center facilities in multiple regions worldwide. Each AWS region is a separate geographical location with multiple, isolated, and physically-separated groups of data centers known as Availability Zones (AZs)."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "We leverage AWS\u2019 compute, storage, network, and data services to build our products and platform components, which enables us to utilize redundancy capabilities offered by AWS, such as availability zones and regions."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Distributed services architecture"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "With this AWS architecture, we host a number of platform and product services that are used across our solutions. This includes platform capabilities that are shared and consumed across multiple Atlassian products, such as Media, Identity, and Commerce, experiences such as our Editor, and product-specific capabilities, like Jira Issue service and Confluence Analytics."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Atlassian developers provision these services through an internally developed platform-as-a-service (PaaS), called Micros, which automatically orchestrates the deployment of shared services, infrastructure, data stores, and their management capabilities, including security and compliance control requirements (see figure 1 above). Typically, an Atlassian product consists of multiple \"containerized\" services that are deployed on AWS using Micros. Atlassian products use core platform capabilities (see figure 2 below) that range from request routing to binary object stores, authentication/authorization, transactional user-generated content (UGC) and entity relationships stores, data lakes, common logging, request tracing, observability, and analytical services. These micro-services are built using approved technical stacks standardized at the platform level:"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Multi-tenant architecture"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "On top of our cloud infrastructure, we built and operate a multi-tenant micro-service architecture along with a shared platform that supports our products. In a multi-tenant architecture, a single service serves multiple customers, including databases and compute instances required to run our cloud products. Each shard (essentially a container \u2013 see figure 3 below) contains the data for multiple tenants, but each tenant's data is isolated and inaccessible to other tenants."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Tenant provisioning and lifecycle\nWhen a new customer is provisioned, a series of events trigger the orchestration of distributed services and provisioning of data stores. These events can be generally mapped to one of seven steps in the lifecycle:\n\n1. Commerce systems are immediately updated with the latest metadata and access control information for that customer, and then a provisioning orchestration system aligns the \"state of the provisioned resources\" with the license state through a series of tenant and product events.\n\nTenant events\n\nThese event affect the tenant as a whole and can either be:\n\nCreation: a tenant is created and used for brand new sites\nDestruction: an entire tenant is deleted\nProduct events"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Activation: after the activation of licensed products or third-party apps\nDeactivation: after the de-activation of certain products or apps\nSuspension: after the suspension of a given existing product, thus disabling access to a given site that they own\nUn-suspension: after the un-suspension of a given existing product, thus enabling access to a site that they own\nLicense update: contains information regarding the number of license seats for a given product as well as its status (active/inactive)"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "2. Creation of the customer site and activation of the correct set of products for the customer. The concept of a site is the container of multiple products licensed to a particular customer. (e.g. Confluence and Jira Software for <site-name>.atlassian.net). This (see figure 4 below) is an important point to understand in the context of this report, as the site container is what was deleted in this incident and the concept of a site is discussed throughout the document."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "3. Provisioning of products within the customer site in the designated region.\n\nWhen a product is provisioned it will have the majority of its content hosted close to where users are accessing it. To optimize product performance, we don't limit data movement when it's hosted globally and we may move data between regions as needed.\n\nFor some of our products, we also offer data residency. Data residency allows customers to choose whether product data is globally distributed or held in place in one of our defined geographic locations.\n\n4. Creation and storage of the customer site and product(s) core metadata and configuration.\n\n5. Creation and storage of the site and product(s) identity data, such as users, groups, permissions, etc.\n\n6. Provisioning of product databases within a site, e.g. Jira family of products, Confluence, Compass, Atlas.\n\n7. Provisioning of the product(s) licensed apps.\n\n\n\nFigure 5: Overview of how customer site is provisioned across our distributed architecture.\n\nFigure 5 above demonstrates how a customer's site is deployed across our distributed architecture, not just in a single database or store. This includes multiple physical and logical locations that store meta-data, configuration data, product data, platform data and other related site info.\n\nDisaster Recovery program\nOur Disaster Recovery (DR) program encompasses all of our efforts to provide resiliency against infrastructure failures and restorability of service storage from backups. Two important concepts to understand disaster recovery programs are:"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Recovery time objective (RTO): How quickly can the data be recovered and returned to a customer during a disaster?\nRecovery point objective (RPO): How fresh is the recovered data after it is recovered from a backup? How much data will be lost since the last backup?\nDuring this incident, we missed our RTO but met our RPO."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Resiliency\nWe prepare for infrastructure-level failures; for example, the loss of an entire database, service, or AWS Availability Zones. This preparation includes replication of data and services across multiple availability zones and regular failover testing."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Service storage restorability"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "We also prepare to recover from data corruption of service storage due to risks such as ransomware, bad actors, software defects, and operational errors. This preparation includes immutable backups and service storage backup restoration testing. We are able to take any individual data store and restore it to a previous point in time."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Multi-site, multi-product automated restorability\nAt the time of the incident, we did not have the ability to select a large set of customer sites and restore all of their inter-connected products from backups to a previous point in time."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Our capabilities have been focused on infrastructure, data corruption, single service events, or single-site deletions. In the past, we have had to deal with and test these kinds of failures. The site-level deletion did not have runbooks that could be quickly automated for the scale of this event which required tooling and automation across all the products and services to happen in a coordinated way."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "The following sections will go into more depth about this complexity and what we are doing at Atlassian to evolve and optimize our abilities to maintain this architecture at scale."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "What happened, timeline, and recovery\nWhat happened"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "In 2021, we completed the acquisition and integration of a standalone Atlassian app for Jira Service Management and Jira Software, called \u201cInsight \u2013 Asset Management\u201d. The functionality of this standalone app was then native within Jira Service Management and was no longer available for Jira Software. Because of this, we needed to delete the standalone legacy app on customer sites that had it installed. Our engineering teams used an existing script and process to delete instances of this standalone application."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "However, two problems ensued:"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Communication gap. There was a communication gap between the team that requested the deletion and the team that ran it. Instead of providing the IDs of the intended app being marked for deletion, the team provided the IDs of the entire cloud site where the apps were to be deleted."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Insufficient system warnings. The API used to perform the deletion accepts both site and app identifiers and assumes the input is correct \u2013 this means that if a site ID is passed, a site will be deleted; if an app ID is passed, an app will be deleted. There was no warning signal to confirm the type of deletion (site or app) being requested."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "The script that was executed followed our standard peer-review process, which focused on which endpoint was being called and how. It did not cross-check the provided cloud site IDs to validate whether they referred to the app or to the entire site. The script was tested in Staging per our standard change management processes, however, it would not have detected that the IDs input were incorrect as the IDs did not exist in the Staging environment."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "When run in Production, the script initially ran against 30 sites. The first Production run was successful, and deleted the Insight app for those 30 sites with no other side effects. However, IDs for those 30 sites were sourced prior to the miscommunication event and included the correct Insight app IDs."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "The script for the subsequent Production run included site IDs in place of Insight app IDs and executed against a set of 883 sites. The script started running on April 5th at 07:38 UTC and was completed at 08:01 UTC. The script deleted sites sequentially based on the input list, so the first customer's site was deleted shortly after the script started running at 07:38 UTC. The result was an immediate deletion of the 883 sites, with no warning signal to our engineering teams."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "The following Atlassian products were unavailable for impacted customers: Jira family of products, Confluence, Atlassian Access, Opsgenie, and Statuspage."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "As soon as we learned of the incident, our teams were focused on restoration for all impacted customers. At that time, we estimated the number of impacted sites to be ~700 (883 total sites were impacted, but we subtracted out the Atlassian-owned sites). Of the 700, a significant portion were inactive, free, or small accounts with a low number of active users. Based on this, we initially estimated the approximate number of impacted customers at around 400."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "We now have a much more accurate view, and for complete transparency based on Atlassian's official customer definition, 775 customers were affected by the outage. However, the majority of users were represented within the original 400 customer estimate. The outage spanned up to 14 days for a subset of these customers, with the first set of customers being restored on April 8th, and all customers restored as of April 18th."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "How we coordinated"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "The first support ticket was created by an impacted customer at 07:46 UTC on April 5th. Our internal monitoring did not detect an issue because the sites were deleted via a standard workflow. At 08:17 UTC, we triggered our major incident management process, forming a cross-functional incident management team, and in seven minutes, at 08:24 UTC, it had been escalated to Critical. At 08:53 UTC, our team confirmed that the customer support ticket and the script run were related. Once we realized the complexity of restoration, we assigned our highest level of severity to the incident at 12:38 UTC."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "The incident management team was composed of individuals from multiple teams across Atlassian, including engineering, customer support, program management, communications, and many more. The core team met every three hours for the duration of the incident until all sites were restored, validated, and returned to customers."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "To manage the restoration progress we created a new Jira project, SITE, and a workflow to track restorations on a site-by-site basis across multiple teams (engineering, program management, support, etc). This approach empowered all teams to easily identify and track issues related to any individual site restoration."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "We also implemented a code freeze across all of engineering for the duration of the incident on April 8th at 03:30 UTC. This allowed us to focus on customer restoration, eliminate the risk of change causing inconsistencies in customer data, minimize the risk of other outages, and reduce the likelihood of unrelated changes distracting the team from recovery."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Timeline of the incident\n\n\nFigure 6: Timeline of the incident and key restoration milestones.\n\nHigh-level overview of recovery workstreams\nRecovery ran as three primary workstreams \u2013 detection, early recovery, and acceleration. While we've described each workstream separately below, during recovery there was work happening in parallel across all of the workstreams.\n\nWorkstream 1: Detection, starting the recovery & identifying our approach\nTimestamp: Days 1-2 (April 5th \u2013 6th)\n\nAt 08:53 UTC on April 5th, we identified that the Insight app script caused the deletion of sites. We confirmed that this was not the result of an internal malicious act or cyberattack. Relevant product and platform infrastructure teams were paged and brought into the incident.\n\nAt the beginning of the incident, we recognized:"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Restoring hundreds of deleted sites is a complex, multi-step process (detailed in the architecture section above), requiring many teams and multiple days to successfully complete.\nWe had the ability to recover a single site, but we had not built capabilities and processes for recovering a large batch of sites.\nAs a result, we needed to substantially parallelize and automate the restoration process in order to help impacted customers regain access to their Atlassian products as quickly as possible."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Workstream 1 involved large numbers of development teams engaging in the following activities:"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Identifying and executing restoration steps for batches of sites in the pipeline.\nWriting and improving automation to allow the team(s) to execute restoration steps for larger numbers of sites in a batch.\nWorkstream 2: Early recovery and the Restoration 1 approach\nTimestamp: Days 1-4 (April 5th \u2013 9th)"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "We understood what caused the site deletion on April 5th at 08:53 UTC, within an hour after the script finished its run. We also identified the restoration process that had previously been used to recover a small number of sites into production. However, the recovery process for restoring deleted sites at such a scale wasn't well defined."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "To get moving quickly, the early stages of the incident split into two working groups:"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "The manual working group validated the steps required and manually executed the restoration process for a small number of sites.\nThe automation working group took the existing restoration process and built automation to safely execute the steps across larger batches of sites.\nOverview of the Restoration 1 approach (see figure 7 below):"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "It required the creation of a new site for each deleted one, followed by every downstream product, service, and data store needing to restore their data."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "The new site would come with new identifiers such as cloudId. These identifiers are all considered immutable, meaning that many systems embed these identifiers in data records. As a result, we needed to update large quantities of data if these identifiers changed, which is particularly problematic for third-party ecosystem apps."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Modifying a new site to replicate the state of the deleted site had complex and often unforeseen dependencies between steps."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Figure 7: Key steps in Restoration 1 approach.\n\nThe Restoration 1 approach included approximately 70 individual steps that, when aggregated at a high level, followed a largely sequential flow of:"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Creation of the new site, licenses, Cloud ID, and activation of the correct set of products\nMigrating the site to the correct region\nRestoring & re-mapping the site's core metadata and configuration\nRestoring & re-mapping the site's Identity data \u2013 users, groups, permissions, etc\nRestoring the site's main product databases\nRestoring & re-mapping the site's media associations \u2013 attachments, etc\nSetting the correct feature flags for the site\nRestoring & re-mapping the site's data across all services\nRestoring & re-mapping the site's third-party apps\nAtlassian validating the site was functioning correctly\nCustomers validating the site was functioning correctly\nOnce optimized, the Restoration 1 approach took approximately 48 hours to restore a batch of sites, and was used for the recovery of 53% of impacted users across 112 sites between April 5th and April 14th."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Workstream 3: Accelerated recovery and the Restoration 2 approach\nTimestamp: Days 4 \u2013 13 (April 9th \u2013 17th)\n\nWith the Restoration 1 approach, it would have taken us three weeks to restore all customers. Therefore, we proposed a new approach on April 9th to speed up the restoration of all sites, Restoration 2 (see figure 8 below).\n\nThe Restoration 2 approach offered improved parallelism between restoration steps by reducing complexity and the number of dependencies that were present with the Restoration 1 approach."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Restoration 2 involved the re-creation (or un-deletion) of records associated with the site across all respective systems, beginning with the Catalogue Service record. A key element of this new approach was to re-use all of the old site identifiers. This removed over half of the steps from the prior process that were used to map the old identifiers to the new identifiers, including the need to coordinate with every third-party app vendor for each site."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "However, the move from the Restoration 1 to the Restoration 2 approach added substantial overhead in the incident response:"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Many of the automation scripts and processes established in the Restoration 1 approach had to be modified for Restoration 2.\nTeams performing restorations (including incident coordinators) had to manage parallel batches of restorations in both approaches, while we tested and validated the Restoration 2 process.\nUsing a new approach meant that we needed to test and validate the Restoration 2 process before scaling it up, which required duplicating validation work that was previously completed for Restoration 1."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Figure 8: Key steps in Restoration 2 approach.\n\nThe graphic above represents the Restoration 2 approach, which included over 30 steps that followed a largely parallelized flow of:"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Re-creation of records associated with the site across all respective systems\nRestoring the site's Identity data \u2013 users, groups, permissions, etc\nRestoring the site's main product databases\nRestoring the site's data across all services\nRestoring the site's third-party apps\nAutomatic validation\nCustomers validating the site was functioning correctly"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "As part of the accelerated recovery, we also took steps to front-load and automate site restoration because manual restoration would not scale well for large batches. The sequential nature of the recovery process meant site restoration could be slower for large database restorations and user base/permissions restorations. Optimizations we implemented included:"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "We developed the tooling and guard rails needed to front-load and parallelize long-running steps like database restorations and Identity synchronizations so that they were completed in advance of other restoration steps.\nEngineering teams built automation for their individual steps that enabled large batches of restorations to be safely executed.\nAutomation was built to validate sites were functioning correctly after all restoration steps were completed.\nThe accelerated Restoration 2 approach took approximately 12 hours to restore a site and was used for the recovery of approximately 47% of impacted users across 771 sites between April 14th and 17th."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Minimal data loss following the restoration of deleted sites"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Our databases are backed up using a combination of full backups and incremental backups that allow us to choose any particular \"Point in Time\" to recover our data stores within the backup retention period (30 days). For most customers during this incident, we identified the main data stores for our products and decided on using a restore point of five minutes prior to the deletion of sites as a safe synchronization point. The non-primary data stores were restored to the same point or by replaying the recorded events. Using a fixed restore point for primary stores enabled us to get consistency of data across all the data stores."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "For 57 customers restored early on in our incident response, a lack of consistent policies and manual retrieval of database backup snapshots resulted in some Confluence and Insight databases being restored to a point more than five minutes prior to site deletion. The inconsistency was discovered during a post-restoration audit process. We have since recovered the remainder of the data, contacted the customers affected by this, and are helping them apply changes to further restore their data."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "In summary:"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "We met our Recovery Point Objective (RPO) of one hour during this incident.\nData loss from the incident is capped at five minutes prior to the deletion of the site.\nA small number of customers had their Confluence or Insight databases restored to a point more than five minutes prior to site deletion, however, we are able to recover the data and are currently working with customers on getting this data restored.\nIncident communications\nWhen we talk about incident communications, it encompasses touch-points with customers, partners, the media, industry analysts, investors, and the broader technology community."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "What happened\n\n\nFigure 9: Timeline of the key incident communications milestones.\n\nTimestamp: Days 1 \u2013 3 (April 5th \u2013 7th)\nEarly response"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "The first support ticket was created on April 5th at 7:46 UTC and Atlassian support responded acknowledging the incident by 8:31 UTC. At 9:03 UTC, the first Statuspage update was posted letting customers know that we were investigating the incident. And at 11:13 UTC, we confirmed via Statuspage that we had identified the root cause and that we were working on a fix. By 1:00 UTC on April 6th, the initial customer ticket communications stated that the outage was due to a maintenance script, and that we expected minimal data loss. Atlassian responded to media inquiries with a statement on April 6th at 17:30 UTC. Atlassian tweeted its first broad external message acknowledging the incident on April 7th at 00:56 UTC."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Timestamp: Days 4 \u2013 7 (April 8th \u2013 11th)\nBroader, personalized outreach begins"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "On April 8th at 1:50 UTC, Atlassian emailed affected customers an apology from co-founder and co-CEO, Scott Farquhar. In the days that followed, we worked to restore the deleted contact information and create support tickets for all impacted sites that didn't yet have one filed. Our support team then continued to send regular updates about restoration efforts through the support tickets associated with each impacted site."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Timestamp: Days 8 \u2013 14 (April 12th \u2013 18th)\nGreater clarity and complete restoration"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "On April 12th, Atlassian published an update from CTO, Sri Viswanath, providing more technical details of what happened, who was affected, whether there was data loss, our progress on restoration, and that it may take up to two weeks to fully restore all sites. The blog was accompanied by another press statement attributed to Sri. We also referenced Sri's blog in our first proactive Atlassian Community post from Head of Engineering, Stephen Deasy, which subsequently became the dedicated place for additional updates and Q&A with the broader public. An April 18th update to this post announced the full restoration of all affected customer sites."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": ":question_mark: Why didn\u2019t we respond publicly sooner?"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "We prioritized communicating directly with affected customers via Statuspage, email, support tickets, and 1:1 interactions. However, we were unable to reach many customers because we lost their contact information when their sites were deleted. We should have implemented broader communications much earlier, in order to inform affected customers and end-users about our incident response and resolution timeline."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "While we immediately knew what had caused the incident, the architectural complexity and the unique circumstances of this incident slowed down our ability to quickly scope and accurately estimate time to resolution. Rather than wait until we had a full picture, we should have been transparent about what we did know and what we didn't know. Providing general restoration estimates (even if directional) and being clear about when we expected to have a more complete picture would have allowed our customers to better plan around the incident. This is particularly true for System Admins and technical contacts, who are on the front lines of managing stakeholders and users within their organizations."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Support experience & customer outreach"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "As previously mentioned, the same script that deleted customer sites also deleted key customer identifiers and contact information (e.g. Cloud URL, site System Admin contacts) from our production environments. This is notable because our core systems (e.g. support, licensing, billing) all leverage the existence of a Cloud URL and site System Admin contacts as primary identifiers for security, routing, and prioritization purposes. When we lost these identifiers, we initially lost our ability to systematically identify and engage with customers."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "How was support for our customers impacted?"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "First, the majority of impacted customers could not reach our support team through the normal online contact form. This form is designed to require a user to log in with their Atlassian ID and to provide a valid Cloud URL. Without a valid URL, the user is prevented from submitting a technical support ticket. In the course of normal business, this verification is intentional for site security and ticket triage. However, this requirement created an unintended outcome for customers impacted by this outage; they were blocked from submitting a high-priority site support ticket."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Second, the deletion of site System Admin data caused by the incident created a gap in our ability to proactively engage with impacted customers. In the first few days of the incident, we sent proactive communications to the impacted customer's billing and technical contacts registered with Atlassian. However, we quickly identified that many billing and technical contacts for the impacted customers were outdated. Without the System Admin information for each site, we did not have a complete list of active and approved contacts through which to engage."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "How did we respond?\nOur support teams had three equally important priorities to accelerate site restoration and repair the breakage in our communication channels in the first days of the incident."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "First, getting a reliable list of validated customer contacts. As our engineering teams worked to restore customer sites, our customer-facing teams focused on restoring validated contact information. We used every mechanism at our disposal (billing systems, prior support tickets, other secured user backups, direct customer outreach, etc) to rebuild our contact list. Our goal was to have one incident-related support ticket for each impacted site to streamline direct outreach and response times."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Second, re-establishing workflows, queues, and SLAs specific to this incident. Deletion of the Cloud ID and the inability to authenticate users correctly also impacted our ability to process incident-related support tickets through our normal systems. Tickets did not appear correctly in relevant priority and escalations queues and dashboards. We quickly created a cross-functional team (support, product, IT) to design and add additional logic, SLAs, workflow states, and dashboards. Because this had to be done within our production system, it took several days to fully develop, test, and deploy."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Third, massively scaling manual validations to accelerate site restorations. As engineering made progress through initial restores it became clear that the capacity of our global support teams would be required to help accelerate site recovery via manual testing and validation checks. This validation process would become a critical path to getting restored sites to our customers, once our engineering team accelerated data restores. We had to create an independent stream of standard operating procedures (SOPs), workflows, handoffs, and staffing rosters to mobilize 450+ support engineers to run validation checks, with shifts providing 24/7 coverage, to accelerate restores into the hands of customers."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Even with these key priorities well established by the end of the first week, we were limited in our ability to provide meaningful updates due to the lack of clarity around the incident resolution timelines due to the complexity of the restoration processes. We should have acknowledged our uncertainty in providing a site restoration date sooner and made ourselves available earlier for in-person discussions so that our customers could make plans accordingly."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "How will we improve?\nWe have immediately blocked bulk sites deletes until appropriate changes can be made."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "As we move forward from this incident and re-evaluate our internal processes, we want to recognize that people don't cause incidents. Rather, systems allow for mistakes to be made. This section summarizes the factors that contributed to this incident. We also discuss our plans to accelerate how we will fix these weaknesses and problems."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Learning 1: \"Soft deletes\" should be universal across all systems\nOverall, deletion of this type should be prohibited or have multiple layers of protection to avoid errors. The primary improvement we are making is to globally prevent the deletion of customer data and metadata that has not gone through a soft-delete process."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "a) Data deletion should only happen as a soft-delete"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Deletion of an entire site should be prohibited; and, soft-delete should require multi-level protections to prevent error. We will implement a \"soft delete\" policy, preventing external scripts or systems from deleting customer data in a Production environment. Our \"soft delete\" policy will allow for sufficient data retention so that data recovery can be executed quickly and safely. The data will only be deleted from the Production environment after a retention period has expired."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": ":info: Actions"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": ":check_mark: Implement a \u201csoft delete\u201d in the provisioning workflows and all relevant data stores: Additionally, the Tenant Platform team will verify that data deletions can only happen after deactivations, as well as other safeguards in this space. In the longer term, Tenant Platform will take a leading role to further develop correct state management of tenant data."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "b) Soft-delete should have a standardized and verified review process\n\nSoft-delete actions are high-risk operations. As such, we should have standardized or automated review processes that include defined rollbacks and testing procedures to address these operations.\n\n:info: Actions"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": ":check_mark: Enforce staged rollout of any soft-delete actions: All new operations that require deletion will first be tested within our own sites to validate our approach and verify automation. Once we've completed that validation, we will progressively move customers through the same process and continue to test for irregularities before applying the automation to the entire selected user base."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": ":check_mark: Soft-delete actions must have a tested rollback plan: Any activity to soft-delete data must test restoration of the deleted data prior to running in production and have a tested rollback plan."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Learning 2: As part of the DR program, automate restoration for multi-site, multi-product deletion events for a larger set of customers"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Atlassian Data Management describes our data management processes in detail. To provide high availability, we provision and maintain a synchronous standby replica in multiple AWS Availability Zones (AZ). The AZ failover is automated and typically takes 60-120 seconds, and we regularly handle data center outages and other common disruptions with no customer impact."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "We also maintain immutable backups that are designed to be resilient against data corruption events, which enable recovery to a previous point in time. Backups are retained for 30 days, and Atlassian continuously tests and audits storage backups for restoration. If required, we can restore all customers to a new environment."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Using these backups, we regularly roll back individual customers or a small set of customers who accidentally delete their own data. However, the site-level deletion did not have runbooks that could be quickly automated for the scale of this event which required tooling and automation across all the products and services to happen in a coordinated way."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "What we have not (yet) automated is restoring a large subset of customers into our existing (and currently in use) environment without affecting any of our other customers."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Within our cloud environment, each data store contains data from multiple customers. Because the data deleted in this incident was only a portion of data stores that are continuing to be used by other customers, we have to manually extract and restore individual pieces from our backups. Each customer site recovery is a lengthy and complex process, requiring internal validation and final customer verification when the site is restored."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": ":info: Actions"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": ":check_mark: Accelerate multi-product, multi-site restorations for a larger set of customers: DR program meets our current RPO standards of one hour. We will leverage the automation and learnings from this incident to accelerate the DR program to meet the RTO as defined in our policy for this scale of incident."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": ":check_mark: Automate and add the verification of this case to the DR testing: We will regularly run DR exercises that involve restoring all products for large set of sites. These DR tests will verify that runbooks are up to date as our architecture evolves and any new edge cases are encountered. We will continuously improve our restoration approach, automate more of the restoration process, and reduce recovery time."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Learning 3: Improve incident management process for large-scale events\nOur incident management program is well-suited for managing the major and minor incidents that have occurred over the years. We frequently simulate incident response for smaller-scale, shorter-duration incidents, that typically involve fewer people and teams."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "However, at its peak, this incident had hundreds of engineers and customer support employees working simultaneously to restore customer sites. Our incident management program and teams were not designed to handle the depth, expansiveness, and duration of this type of incident (see figure 10 below).\n\n\n\nFigure 10: Overview of large-scale incident management process.\n\nOur large-scale incident management process will be better defined and practiced often"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "We have playbooks for product-level incidents, but not for the events of this scale, with hundreds of people working simultaneously across the company. In Incident Management tooling we have automation that creates communication streams like Slack, Zoom, and Confluence doc but it lacks creating sub-streams that are required for large-scale incidents to isolate restoration streams."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": ":info: Actions"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": ":check_mark: Define a playbook and tooling for large-scale incidents and conduct simulated exercises: Define and document the types of incidents that may be considered large-scale and require this level of response. Outline key coordination steps and build tooling to help Incident Managers and other business functions streamline the response and start recovery. Incident Managers with teams will regularly run simulations, trainings, and refinement of tooling and documents to continually improve."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Learning 4: Improve our communications processes\na) We deleted critical customer identifiers, impacting communications and actions to those affected"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "The same script which deleted customer sites also deleted key customer identifiers (e.g. site URL, site System Admin contacts) from our Production environments. As a result, (1) customers were blocked from filing technical support tickets via our normal support channel; (2) it took days for us to get a reliable list of key customer contacts (such as site System Admins) impacted by the outage for proactive engagement; and (3) support workflows, SLAs, dashboards, and escalation processes did not properly function initially because of the unique nature of the incident."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "During the outage, customer escalations also came through multiple channels (email, phone calls, CEO tickets, LinkedIn and other social channels, and support tickets). Disparate tools and processes across our customer-facing teams slowed our response and made holistic tracking and reporting of these escalations more difficult."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "b) We did not have an incident communications playbook thorough enough to deal with this level of complexity"}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "We did not have an incident communications playbook that outlined principles as well as roles and responsibilities to mobilize a unified, cross-functional incident communications team quickly enough. We did not provide acknowledgment of the incident quickly and consistently through multiple channels, especially on social media. More broad, public communications surrounding the outage, along with the repetition of the critical message that there was no data loss and this was not the result of a cyberattack, would have been the correct approach."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": ":info: Actions\n\n:check_mark: Improve the backup of key contacts: Backup authorized account contact information outside of the product instance.\n\n:check_mark: Retrofit support tooling: Create mechanisms for customers without a valid site URL or Atlassian ID to make direct contact with our technical support team."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": ":check_mark: Customer escalation system and processes: Invest in a unified, account-based, escalation system and workflows that allow for multiple work objects (tickets, tasks, etc) to be stored underneath a single customer account object, for improved coordination and visibility across all of our customer-facing teams."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": ":check_mark: Expedite 24/7 Escalation Management coverage: Execute against global footprint expansion plans for the Escalation Management function to allow for consistent 24/7 coverage with designated staff based in each major geographic region along with support roles to assist with required product and sales subject-matter experts and leadership."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": ":check_mark: Update our incident communications playbook with new learnings and revisit it regularly: Revisit the playbook to define clear roles and lines of communications internally. Use the DACI framework for incidents and have 24/7 back-ups for each role in case of sickness, holidays, or other unforeseen events. Conduct a quarterly audit to verify readiness at all times. Follow the incident communications template in all communications: address what happened, who was impacted, timeline to restoration, site restoration percentages, expected data loss, with the associated confidence levels, along with clear guidance on how to contact support."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Closing remarks\nWhile the outage is resolved and customers are fully restored, our work continues. At this stage, we are implementing the changes outlined above to improve our processes, increase our resiliency, and prevent a situation like this from happening again."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "Atlassian is a learning organization, and our teams have certainly learned a lot of hard lessons from this experience. We are putting these lessons to work in order to make lasting changes to our business. Ultimately, we will emerge stronger and provide you with better service because of this experience."}
{"title": "Post-Incident Review on the Atlassian April 2022 outage", "url": "https://www.atlassian.com/engineering/post-incident-review-april-2022-outage", "snippet": "We hope that the learnings from this incident will be helpful to other teams who are working diligently to provide reliable services to their customers.\n\nLastly, I want to thank those who are reading this and learning with us and those who are part of our extended Atlassian community and team."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Now that we have fully restored functionality to all affected services, we would like to share more details with our customers about the events that occurred with the Amazon Elastic Compute Cloud (\u201cEC2\u201d) last week, our efforts to restore the services, and what we are doing to prevent this sort of issue from happening again. We are very aware that many of our customers were significantly impacted by this event, and as with any significant service issue, our intention is to share the details of what happened and how we will improve the service for our customers."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "The issues affecting EC2 customers last week primarily involved a subset of the Amazon Elastic Block Store (\u201cEBS\u201d) volumes in a single Availability Zone within the US East Region that became unable to service read and write operations. In this document, we will refer to these as \u201cstuck\u201d volumes. This caused instances trying to use these affected volumes to also get \u201cstuck\u201d when they attempted to read or write to them. In order to restore these volumes and stabilize the EBS cluster in that Availability Zone, we disabled all control APIs (e.g. Create Volume, Attach Volume, Detach Volume, and Create Snapshot) for EBS in the affected Availability Zone for much of the duration of the event. For two periods during the first day of the issue, the degraded EBS cluster affected the EBS APIs and caused high error rates and latencies for EBS calls to these APIs across the entire US East Region. As with any complicated operational issue, this one was caused by several root causes interacting with one another and therefore gives us many opportunities to protect the service against any similar event reoccurring."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Overview of EBS System"}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "It is helpful to understand the EBS architecture so that we can better explain the event. EBS is a distributed, replicated block data store that is optimized for consistency and low latency read and write access from EC2 instances. There are two main components of the EBS service: (i) a set of EBS clusters (each of which runs entirely inside of an Availability Zone) that store user data and serve requests to EC2 instances; and (ii) a set of control plane services that are used to coordinate user requests and propagate them to the EBS clusters running in each of the Availability Zones in the Region."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "An EBS cluster is comprised of a set of EBS nodes. These nodes store replicas of EBS volume data and serve read and write requests to EC2 instances. EBS volume data is replicated to multiple EBS nodes for durability and availability. Each EBS node employs a peer-to-peer based, fast failover strategy that aggressively provisions new replicas if one of the copies ever gets out of sync or becomes unavailable. The nodes in an EBS cluster are connected to each other via two networks. The primary network is a high bandwidth network used in normal operation for all necessary communication with other EBS nodes, with EC2 instances, and with the EBS control plane services. The secondary network, the replication network, is a lower capacity network used as a back-up network to allow EBS nodes to reliably communicate with other nodes in the EBS cluster and provide overflow capacity for data replication. This network is not designed to handle all traffic from the primary network but rather provide highly-reliable connectivity between EBS nodes inside of an EBS cluster."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "When a node loses connectivity to a node to which it is replicating data to, it assumes the other node failed. To preserve durability, it must find a new node to which it can replicate its data (this is called re-mirroring). As part of the re-mirroring process, the EBS node searches its EBS cluster for another node with enough available server space, establishes connectivity with the server, and propagates the volume data. In a normally functioning cluster, finding a location for the new replica occurs in milliseconds. While data is being re-mirrored, all nodes that have copies of the data hold onto the data until they can confirm that another node has taken ownership of their portion. This provides an additional level of protection against customer data loss. Also, when data on a customer\u2019s volume is being re-mirrored, access to that data is blocked until the system has identified a new primary (or writable) replica. This is required for consistency of EBS volume data under all potential failure modes. From the perspective of an EC2 instance trying to do I/O on a volume while this is happening, the volume will appear \u201cstuck\u201d."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "In addition to the EBS clusters, there is a set of control plane services that accepts user requests and propagates them to the appropriate EBS cluster. There is one set of EBS control plane services per EC2 Region, but the control plane itself is highly distributed across the Availability Zones to provide availability and fault tolerance. These control plane services also act as the authority to the EBS clusters when they elect primary replicas for each volume in the cluster (for consistency, there must only be a single primary replica for each volume at any time). While there are a few different services that comprise the control plane, we will refer to them collectively as the \u201cEBS control plane\u201d in this document."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Primary Outage"}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "At 12:47 AM PDT on April 21st, a network change was performed as part of our normal AWS scaling activities in a single Availability Zone in the US East Region. The configuration change was to upgrade the capacity of the primary network. During the change, one of the standard steps is to shift traffic off of one of the redundant routers in the primary EBS network to allow the upgrade to happen. The traffic shift was executed incorrectly and rather than routing the traffic to the other router on the primary network, the traffic was routed onto the lower capacity redundant EBS network. For a portion of the EBS cluster in the affected Availability Zone, this meant that they did not have a functioning primary or secondary network because traffic was purposely shifted away from the primary network and the secondary network couldn\u2019t handle the traffic level it was receiving. As a result, many EBS nodes in the affected Availability Zone were completely isolated from other EBS nodes in its cluster. Unlike a normal network interruption, this change disconnected both the primary and secondary network simultaneously, leaving the affected nodes completely isolated from one another."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "When this network connectivity issue occurred, a large number of EBS nodes in a single EBS cluster lost connection to their replicas. When the incorrect traffic shift was rolled back and network connectivity was restored, these nodes rapidly began searching the EBS cluster for available server space where they could re-mirror data. Once again, in a normally functioning cluster, this occurs in milliseconds. In this case, because the issue affected such a large number of volumes concurrently, the free capacity of the EBS cluster was quickly exhausted, leaving many of the nodes \u201cstuck\u201d in a loop, continuously searching the cluster for free space. This quickly led to a \u201cre-mirroring storm,\u201d where a large number of volumes were effectively \u201cstuck\u201d while the nodes searched the cluster for the storage space it needed for its new replica. At this point, about 13% of the volumes in the affected Availability Zone were in this \u201cstuck\u201d state."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "After the initial sequence of events described above, the degraded EBS cluster had an immediate impact on the EBS control plane. When the EBS cluster in the affected Availability Zone entered the re-mirroring storm and exhausted its available capacity, the cluster became unable to service \u201ccreate volume\u201d API requests. Because the EBS control plane (and the create volume API in particular) was configured with a long time-out period, these slow API calls began to back up and resulted in thread starvation in the EBS control plane. The EBS control plane has a regional pool of available threads it can use to service requests. When these threads were completely filled up by the large number of queued requests, the EBS control plane had no ability to service API requests and began to fail API requests for other Availability Zones in that Region as well. At 2:40 AM PDT on April 21st, the team deployed a change that disabled all new Create Volume requests in the affected Availability Zone, and by 2:50 AM PDT, latencies and error rates for all other EBS related APIs recovered."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Two factors caused the situation in this EBS cluster to degrade further during the early part of the event. First, the nodes failing to find new nodes did not back off aggressively enough when they could not find space, but instead, continued to search repeatedly. There was also a race condition in the code on the EBS nodes that, with a very low probability, caused them to fail when they were concurrently closing a large number of requests for replication. In a normally operating EBS cluster, this issue would result in very few, if any, node crashes; however, during this re-mirroring storm, the volume of connection attempts was extremely high, so it began triggering this issue more frequently. Nodes began to fail as a result of the bug, resulting in more volumes left needing to re-mirror. This created more \u201cstuck\u201d volumes and added more requests to the re-mirroring storm."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "By 5:30 AM PDT, error rates and latencies again increased for EBS API calls across the Region. When data for a volume needs to be re-mirrored, a negotiation must take place between the EC2 instance, the EBS nodes with the volume data, and the EBS control plane (which acts as an authority in this process) so that only one copy of the data is designated as the primary replica and recognized by the EC2 instance as the place where all accesses should be sent. This provides strong consistency of EBS volumes. As more EBS nodes continued to fail because of the race condition described above, the volume of such negotiations with the EBS control plane increased. Because data was not being successfully re-mirrored, the number of these calls increased as the system retried and new requests came in. The load caused a brown out of the EBS control plane and again affected EBS APIs across the Region. At 8:20 AM PDT, the team began disabling all communication between the degraded EBS cluster in the affected Availability Zone and the EBS control plane. While this prevented all EBS API access in the affected Availability Zone (we will discuss recovery of this in the next section), other latencies and error rates returned to normal for EBS APIs for the rest of the Region."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "A large majority of the volumes in the degraded EBS cluster were still functioning properly and the focus was to recover the cluster without affecting more volumes. At 11:30AM PDT, the team developed a way to prevent EBS servers in the degraded EBS cluster from futilely contacting other servers (who didn\u2019t have free space at this point anyway) without affecting the other essential communication between nodes in the cluster. After this change was made, the cluster stopped degrading further and additional volumes were no longer at risk of becoming \u201cstuck\u201d. Before this change was deployed, the failed servers resulting from the race condition resulted in an additional 5% of the volumes in the affected Availability Zone becoming \u201cstuck\u201d. However, volumes were also slowly re-mirroring as some capacity was made available which allowed existing \u201cstuck\u201d volumes to become \u201dunstuck\u201d. The net result was that when this change was deployed, the total \u201cstuck\u201d volumes in the affected Availability Zone was 13%."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Customers also experienced elevated error rates until Noon PDT on April 21st when attempting to launch new EBS-backed EC2 instances in Availability Zones other than the affected zone. This occurred for approximately 11 hours, from the onset of the outage until Noon PM PDT on April 21st. Except for the periods of broader API issues describe above, customers were able to create EBS-backed EC2 instances but were experiencing significantly-elevated error rates and latencies. New EBS-backed EC2 launches were being affected by a specific API in the EBS control plane that is only needed for attaching new instances to volumes. Initially, our alarming was not fine-grained enough for this EBS control plane API and the launch errors were overshadowed by the general error from the degraded EBS cluster. At 11:30 AM PDT, a change to the EBS control plane fixed this issue and latencies and error rates for new EBS-backed EC2 instances declined rapidly and returned to near-normal at Noon PDT."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Recovering EBS in the Affected Availability Zone"}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "By 12:04 PM PDT on April 21st, the outage was contained to the one affected Availability Zone and the degraded EBS cluster was stabilized. APIs were working well for all other Availability Zones and additional volumes were no longer becoming \u201cstuck\u201d. Our focus shifted to completing the recovery. Approximately 13% of the volumes in the Availability Zone remained \u201cstuck\u201d and the EBS APIs were disabled in that one affected Availability Zone. The key priority became bringing additional storage capacity online to allow the \u201cstuck\u201d volumes to find enough space to create new replicas."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "The team faced two challenges which delayed getting capacity online. First, when a node fails, the EBS cluster does not reuse the failed node until every data replica is successfully re-mirrored. This is a conscious decision so that we can recover data if a cluster fails to behave as designed. Because we did not want to re-purpose this failed capacity until we were sure we could recover affected user volumes on the failed nodes, the team had to install a large amount of additional new capacity to replace that capacity in the cluster. This required the time-consuming process of physically relocating excess server capacity from across the US East Region and installing that capacity into the degraded EBS cluster. Second, because of the changes made to reduce the node-to-node communication used by peers to find new capacity (which is what stabilized the cluster in the step described above), the team had difficulty incorporating the new capacity into the cluster. The team had to carefully make changes to their negotiation throttles to allow negotiation to occur with the newly-built servers without again inundating the old servers with requests that they could not service. This process took longer than we expected as the team had to navigate a number of issues as they worked around the disabled communication. At about 02:00AM PDT on April 22nd, the team successfully started adding significant amounts of new capacity and working through the replication backlog. Volumes were restored consistently over the next nine hours and all but about 2.2% of the volumes in the affected Availability Zone were restored by 12:30PM PDT on April 22nd. While the restored volumes were fully replicated, not all of them immediately became \u201cunstuck\u201d from the perspective of the attached EC2 instances because some were blocked waiting for the EBS"}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Volumes were restored consistently over the next nine hours and all but about 2.2% of the volumes in the affected Availability Zone were restored by 12:30PM PDT on April 22nd. While the restored volumes were fully replicated, not all of them immediately became \u201cunstuck\u201d from the perspective of the attached EC2 instances because some were blocked waiting for the EBS control plane to be contactable, so they could safely re-establish a connection with the EC2 instance and elect a new writable copy."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Once there was sufficient capacity added to the cluster, the team worked on re-establishing EBS control plane API access to the affected Availability Zone and restoring access to the remaining \u201cstuck\u201d volumes. There was a large backlog of state changes that had to be propagated both from the degraded EBS nodes to the EBS control plane and vice versa. This effort was done gradually to avoid impact to the restored volumes and the EBS control plane. Our initial attempts to bring API access online to the impacted Availability Zone centered on throttling the state propagation to avoid overwhelming the EBS control plane. We also began building out a separate instance of the EBS control plane, one we could keep partitioned to the affected Availability Zone to avoid impacting other Availability Zones in the Region, while we processed the backlog. We rapidly developed throttles that turned out to be too coarse-grained to permit the right requests to pass through and stabilize the system. Through the evening of April 22nd into the morning of April 23rd, we worked on developing finer-grain throttles. By Saturday morning, we had finished work on the dedicated EBS control plane and the finer-grain throttles. Initial tests of traffic against the EBS control plane demonstrated progress and shortly after 11:30 AM PDT on April 23rd we began steadily processing the backlog. By 3:35PM PDT, we finished enabling access to the EBS control plane to the degraded Availability Zone. This allowed most of the remaining volumes, which were waiting on the EBS control plane to help negotiate which replica would be writable, to once again be usable from their attached instances. At 6:15 PM PDT on April 23rd, API access to EBS resources was restored in the affected Availability Zone."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "With the opening up of API access in the affected Availability Zone, APIs were now operating across all Availability Zones in the Region. The recovery of the remaining 2.2% of affected volumes required a more manual process to restore. The team had snapshotted these volumes to S3 backups early in the event as an extra precaution against data loss while the event was unfolding. At this point, the team finished developing and testing code to restore volumes from these snapshots and began processing batches through the night. At 12:30 PM PDT on April 24, we had finished the volumes that we could recover in this way and had recovered all but 1.04% of the affected volumes. At this point, the team began forensics on the remaining volumes which had suffered machine failure and for which we had not been able to take a snapshot. At 3:00 PM PDT, the team began restoring these. Ultimately, 0.07% of the volumes in the affected Availability Zone could not be restored for customers in a consistent state."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Impact on Amazon Relational Database Service (RDS)\n\nIn addition to the direct effect this EBS issue had on EC2 instances, it also impacted the Relational Database Service (\u201cRDS\u201d). RDS depends upon EBS for database and log storage, and as a result a portion of the RDS databases hosted in the primary affected Availability Zone became inaccessible."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Customers can choose to operate RDS instances either in a single Availability Zone (\u201csingle-AZ\u201d) or replicated across multiple Availability Zones (\u201cmulti-AZ\u201d). Single-AZ database instances are exposed to disruptions in an Availability Zone. In this case, a single-AZ database instance would have been affected if one of the EBS volumes it was relying on got \u201cstuck\u201d. In the primary affected Availability Zone, a peak of 45% of single-AZ instances were impacted with \u201cstuck\u201d I/O. This was a relatively-bigger portion of the RDS population than the corresponding EBS volume population because RDS database instances make use of multiple EBS volumes. This increases aggregate I/O capacity for database workloads under normal conditions, but means that a \u201cstuck\u201d I/O on any volume for a single-AZ database instance can make it inoperable until the volume is restored. The percentage of \u201cstuck\u201d single-AZ database instances in the affected Availability Zone decreased steadily during the event as the EBS recovery proceeded. The percentage of \u201cstuck\u201d single-AZ database instances in the affected Availability Zone decreased to 41.0% at the end of 24 hours, 23.5% at 36 hours and 14.6% at the end of 48 hours, and the rest recovered throughout the weekend. Though we recovered nearly all of the affected database instances, 0.4% of single-AZ database instances in the affected Availability Zone had an underlying EBS storage volume that was not recoverable. For these database instances, customers with automatic backups turned on (the default setting) had the option to initiate point-in-time database restore operations."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "RDS multi-AZ deployments provide redundancy by synchronously replicating data between two database replicas in different Availability Zones. In the event of a failure on the primary replica, RDS is designed to automatically detect the disruption and fail over to the secondary replica. Of multi-AZ database instances in the US East Region, 2.5% did not automatically failover after experiencing \u201cstuck\u201d I/O. The primary cause was that the rapid succession of network interruption (which partitioned the primary from the secondary) and \u201cstuck\u201d I/O on the primary replica triggered a previously un-encountered bug. This bug left the primary replica in an isolated state where it was not safe for our monitoring agent to automatically fail over to the secondary replica without risking data loss, and manual intervention was required. We are actively working on a fix to resolve this issue."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Preventing the Event"}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "The trigger for this event was a network configuration change. We will audit our change process and increase the automation to prevent this mistake from happening in the future. However, we focus on building software and services to survive failures. Much of the work that will come out of this event will be to further protect the EBS service in the face of a similar failure in the future."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "We will be making a number of changes to prevent a cluster from getting into a re-mirroring storm in the future. With additional excess capacity, the degraded EBS cluster would have more quickly absorbed the large number of re-mirroring requests and avoided the re-mirroring storm. We now understand the amount of capacity needed for large recovery events and will be modifying our capacity planning and alarming so that we carry the additional safety capacity that is needed for large scale failures. We have already increased our capacity buffer significantly, and expect to have the requisite new capacity in place in a few weeks. We will also modify our retry logic in the EBS server nodes to prevent a cluster from getting into a re-mirroring storm. When a large interruption occurs, our retry logic will back off more aggressively and focus on re-establishing connectivity with previous replicas rather than futilely searching for new nodes with which to re-mirror. We have begun working through these changes and are confident we can address the root cause of the re-mirroring storm by modifying this logic. Finally, we have identified the source of the race condition that led to EBS node failure. We have a fix and will be testing it and deploying it to our clusters in the next couple of weeks. These changes provide us with three separate protections against having a repeat of this event."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Impact to Multiple Availability Zones"}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "EC2 provides two very important availability building blocks: Regions and Availability Zones. By design, Regions are completely separate deployments of our infrastructure. Regions are completely isolated from each other and provide the highest degree of independence. Many users utilize multiple EC2 Regions to achieve extremely-high levels of fault tolerance. However, if you want to move data between Regions, you need to do it via your applications as we don\u2019t replicate any data between Regions on our users\u2019 behalf. You also need to use a separate set of APIs to manage each Region. Regions provide users with a powerful availability building block, but it requires effort on the part of application builders to take advantage of this isolation. Within Regions, we provide Availability Zones to help users build fault-tolerant applications easily. Availability Zones are physically and logically separate infrastructure that are built to be highly independent while still providing users with high speed, low latency network connectivity, easy ways to replicate data, and a consistent set of management APIs. For example, when running inside a Region, users have the ability to take EBS snapshots which can be restored in any Availability Zone and can programmatically manipulate EC2 and EBS resources with the same APIs. We provide this loose coupling because it allows users to easily build highly-fault-tolerant applications."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "This event had two distinct impacts. First, there was an impact to running applications in the affected Availability Zone because affected EBS volumes became \u201cstuck\u201d. Because of the architecture of the EBS service, the impact to running instances was limited to the affected Availability Zone. As a result, many users who wrote their applications to take advantage of multiple Availability Zones did not have significant availability impact as a result of this event. Some customers reported that they had \u201cstuck\u201d EBS volumes in Availability Zones other than the impacted Availability Zone on Thursday. While our monitoring clearly shows the effect of the re-mirroring storm on the EBS control plane and on volumes within the affected Availability Zone, it does not reflect significant impact to existing EBS volumes within other Availability Zones in the Region. We do see that there were slightly more \u201cstuck\u201d volumes than we would have expected in the healthy Availability Zones, though still an extremely small number. To put this in perspective, the peak \u201cstuck\u201d volume percentage we saw in the Region outside of the affected Availability Zone was less than 0.07%. We investigated a number of these \u201cstuck\u201d volumes. The slightly-elevated number of \u201cstuck\u201d volumes in these non-impacted zones was caused by the delays in recovering from normal re-mirrors because of the increased latencies and error rates of the EBS control plane described above; there is always a background rate of volume re-mirroring going on. We also believe that the work described below to further insulate the EBS control plane will prevent even this slightly-elevated rate if something similar happened."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "While users\u2019 applications taking advantage of multiple Availability Zone (\u201cmulti-AZ\u201d) architectures were able to avoid impact from this event, there was definitely an impact on the EBS control plane that affected the ability to create and manipulate EBS volumes across the Region. One of the advantages of EC2 is the ability to rapidly replace failed resources. When the EBS control plane was degraded or unavailable, it made it difficult for customers with affected volumes to replace their volumes or EBS-booted EC2 instances in other healthy Availability Zones. Preventing this from reoccurring is a top priority."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Even though we provide a degree of loose coupling for our customers, our design goal is to make Availability Zones indistinguishable from completely independent. Our EBS control plane is designed to allow users to access resources in multiple Availability Zones while still being tolerant to failures in individual zones. This event has taught us that we must make further investments to realize this design goal. There are three things we will do to prevent a single Availability Zone from impacting the EBS control plane across multiple Availability Zones. The first is that we will immediately improve our timeout logic to prevent thread exhaustion when a single Availability Zone cluster is taking too long to process requests. This would have prevented the API impact from 12:50 AM PDT to 2:40 AM PDT on April 21st. To address the cause of the second API impact, we will also add the ability for our EBS control plane to be more Availability Zone aware and shed load intelligently when it is over capacity. This is similar to other throttles that we already have in our systems. Additionally, we also see an opportunity to push more of our EBS control plane into per-EBS cluster services. By moving more functionality out of the EBS control plane and creating per-EBS cluster deployments of these services (which run in the same Availability Zone as the EBS cluster they are supporting), we can provide even better Availability Zone isolation for the EBS control plane."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Making it Easier to Take Advantage of Multiple Availability Zones"}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "We also intend to make it easier for customers to take advantage of multiple Availability Zones. First, we will offer multiple Availability Zones for all of our services, including Amazon Virtual Private Cloud (\u201cVPC\u201d). Today, VPC customers only have access to a single Availably Zone. We will be adjusting our roadmap to give VPC customers access to multiple Availability Zones as soon as possible. This will allow VPC customers to build highly-available applications using multiple Availability Zones just as EC2 customers not using a VPC do today."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "A related finding from this event is we need to do a better job of making highly-reliable multi-AZ deployments easy to design and operate. Some customers\u2019 applications (or critical components of the application like the database) are deployed in only a single Availability Zone, while others have instances spread across Availability Zones but still have critical, single points of failure in a single Availability Zone. In cases like these, operational issues can negatively impact application availability when a robust multi-Availability Zone deployment would allow the application to continue without impact. We will look to provide customers with better tools to create multi-AZ applications that can support the loss of an entire Availability Zone without impacting application availability. We know we need to help customers design their application logic using common design patterns. In this event, some customers were seriously impacted, and yet others had resources that were impacted but saw nearly no impact on their applications."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "In order to work more closely with our customers and partners on best practices for architecting in the cloud, we will be hosting a series of free webinars starting Monday, May 2. The first topics we will cover will be Designing Fault-tolerant Applications, Architecting for the Cloud, and Web Hosting Best Practices. We anticipate adding more topics to the series over the next few weeks, and will continue to do these on a frequent ongoing basis. The webinars over the next two weeks will be hosted several times daily to support our customers around the world in multiple time zones. We will set aside a significant portion of the webinars for detailed Q&A. Follow-up discussions for customers or partners will also be arranged. These webinars, as well as a series of whitepapers on best practices for architecting for the AWS cloud, are available in a new Architecture Center on the AWS website. We\u2019ll also continue to deliver additional services like S3, SimpleDB and multi-AZ RDS that perform multi-AZ level balancing automatically so customers can benefit from multiple Availability Zones without doing any of the heavy-lifting in their applications."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Speeding Up Recovery"}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "We will also invest in increasing our visibility, control, and automation to recover volumes in an EBS cluster. We have a number of operational tools for managing an EBS cluster, but the fine-grained control and throttling the team used to recover the cluster will be built directly into the EBS nodes. We will also automate the recovery models that we used for the various types of volume recovery that we had to do. This would have saved us significant time in the recovery process. We will also look at what changes we can make to preserve volume functionality during periods of degraded cluster operation, including adding the ability to take a snapshot of a \u201cstuck\u201d volume. If customers had this ability, they would have been able to more easily recover their applications in other Availability Zones in the Region."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Improving Communication and Service Health Tools During Operational Issues"}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "In addition to the technical insights and improvements that will result from this event, we also identified improvements that need to be made in our customer communications. We would like our communications to be more frequent and contain more information. We understand that during an outage, customers want to know as many details as possible about what\u2019s going on, how long it will take to fix, and what we are doing so that it doesn\u2019t happen again. Most of the AWS team, including the entire senior leadership team, was directly involved in helping to coordinate, troubleshoot and resolve the event. Initially, our primary focus was on thinking through how to solve the operational problems for customers rather than on identifying root causes. We felt that that focusing our efforts on a solution and not the problem was the right thing to do for our customers, and that it helped us to return the services and our customers back to health more quickly. We updated customers when we had new information that we felt confident was accurate and refrained from speculating, knowing that once we had returned the services back to health that we would quickly transition to the data collection and analysis stage that would drive this post mortem."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "That said, we think we can improve in this area. We switched to more regular updates part of the way through this event and plan to continue with similar frequency of updates in the future. In addition, we are already working on how we can staff our developer support team more expansively in an event such as this, and organize to provide early and meaningful information, while still avoiding speculation."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "We also can do a better job of making it easier for customers to tell if their resources have been impacted, and we are developing tools to allow you to see via the APIs if your instances are impaired.\n\nService Credit for Affected Customers"}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "For customers with an attached EBS volume or a running RDS database instance in the affected Availability Zone in the US East Region at the time of the disruption, regardless of whether their resources and application were impacted or not, we are going to provide a 10 day credit equal to 100% of their usage of EBS Volumes, EC2 Instances and RDS database instances that were running in the affected Availability Zone. These customers will not have to do anything in order to receive this credit, as it will be automatically applied to their next AWS bill. Customers can see whether they qualify for the service credit by logging into their AWS Account Activity page."}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "In Conclusion"}
{"title": "Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region", "url": "https://aws.amazon.com/message/65648/", "snippet": "Last, but certainly not least, we want to apologize. We know how critical our services are to our customers\u2019 businesses and we will do everything we can to learn from this event and use it to drive improvement across our services. As with any significant operational issue, we will spend many hours over the coming days and weeks improving our understanding of the details of the various parts of this event and determining how to make changes to improve our services and processes."}
{"title": "Summary of the December 24, 2012 Amazon ELB Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680587/", "snippet": "We would like to share more details with our customers about the event that occurred with the Amazon Elastic Load Balancing Service (\u201cELB\u201d) earlier this week in the US-East Region. While the service disruption only affected applications using the ELB service (and only a fraction of the ELB load balancers were affected), the impacted load balancers saw significant impact for a prolonged period of time."}
{"title": "Summary of the December 24, 2012 Amazon ELB Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680587/", "snippet": "The service disruption began at 12:24 PM PST on December 24th when a portion of the ELB state data was logically deleted. This data is used and maintained by the ELB control plane to manage the configuration of the ELB load balancers in the region (for example tracking all the backend hosts to which traffic should be routed by each load balancer). The data was deleted by a maintenance process that was inadvertently run against the production ELB state data. This process was run by one of a very small number of developers who have access to this production environment. Unfortunately, the developer did not realize the mistake at the time. After this data was deleted, the ELB control plane began experiencing high latency and error rates for API calls to manage ELB load balancers. In this initial part of the service disruption, there was no impact to the request handling functionality of running ELB load balancers because the missing ELB state data was not integral to the basic operation of running load balancers."}
{"title": "Summary of the December 24, 2012 Amazon ELB Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680587/", "snippet": "Over the next couple hours, our technical teams focused on the API errors. The team was puzzled as many APIs were succeeding (customers were able to create and manage new load balancers but not manage existing load balancers) and others were failing. As this continued, some customers began to experience performance issues with their running load balancers. These issues only occurred after the ELB control plane attempted to make changes to a running load balancer. When a user modifies a load balancer configuration or a load balancer needs to scale up or down, the ELB control plane makes changes to the load balancer configuration. During this event, because the ELB control plane lacked some of the necessary ELB state data to successfully make these changes, load balancers that were modified were improperly configured by the control plane. This resulted in degraded performance and errors for customer applications using these modified load balancers. It was when the ELB technical team started digging deeply into these degraded load balancers that the team identified the missing ELB state data as the root cause of the service disruption. At this point, the focus shifted to preventing additional service impact and recovering the missing ELB state data."}
{"title": "Summary of the December 24, 2012 Amazon ELB Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680587/", "snippet": "At 5:02 PM PST, the team disabled several of the ELB control plane workflows (including the scaling and descaling workflows) to prevent additional running load balancers from being affected by the missing ELB state data. At the peak of the event, 6.8% of running ELB load balancers were impacted. The rest of the load balancers in the system were unable to scale or be modified by customers, but were operating correctly. The team was able to manually recover some of the affected running load balancers on Monday night, and worked through the night to try to restore the missing ELB state data to allow the rest of the affected load balancers to recover (and to open all of the ELB APIs back up)."}
{"title": "Summary of the December 24, 2012 Amazon ELB Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680587/", "snippet": "The team attempted to restore the ELB state data to a point-in-time just before 12:24 PM PST on December 24th (just before the event began). By restoring the data to this time, we would be able to merge in events that happened after this point to create an accurate state for each ELB load balancer. Unfortunately, the initial method used by the team to restore the ELB state data consumed several hours and failed to provide a usable snapshot of the data. This delayed recovery until an alternate recovery process was found. At 2:45 AM PST on December 25th, the team successfully restored a snapshot of the ELB state data to a time just before the data was deleted. The team then began merging this restored data with the system state changes that happened between this snapshot and the current time. By 5:40 AM PST, this data merge had been completed and the new ELB state data had been verified. The team then began slowly re-enabling the ELB service workflows and APIs. This process was done carefully to ensure that no impact was made to unaffected running load balancers and to ensure that each affected load balancer was correctly recovered. The system began recovering the remaining affected load balancers, and by 8:15 AM PST, the team had re-enabled the majority of APIs and backend workflows. By 10:30 AM PST, almost all affected load balancers had been restored to full operation. While the service was substantially recovered at this time, the team continued to closely monitor the service before communicating broadly that it was operating normally at 12:05 PM PST."}
{"title": "Summary of the December 24, 2012 Amazon ELB Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680587/", "snippet": "We have made a number of changes to protect the ELB service from this sort of disruption in the future. First, we have modified the access controls on our production ELB state data to prevent inadvertent modification without specific Change Management (CM) approval. Normally, we protect our production service data with non-permissive access control policies that prevent all access to production data. The ELB service had authorized additional access for a small number of developers to allow them to execute operational processes that are currently being automated. This access was incorrectly set to be persistent rather than requiring a per access approval. We have reverted this incorrect configuration and all access to production ELB data will require a per-incident CM approval. This would have prevented the ELB state data from being deleted in this event. This is a protection that we use across all of our services that has prevented this sort of problem in the past, but was not appropriately enabled for this ELB state data. We have also modified our data recovery process to reflect the learning we went through in this event. We are confident that we could recover ELB state data in a similar event significantly faster (if necessary) for any future operational event. We will also incorporate our learning from this event into our service architecture. We believe that we can reprogram our ELB control plane workflows to more thoughtfully reconcile the central service data with the current load balancer state. This would allow the service to recover automatically from logical data loss or corruption without needing manual data restoration."}
{"title": "Summary of the December 24, 2012 Amazon ELB Service Event in the US-East Region", "url": "https://aws.amazon.com/message/680587/", "snippet": "Last, but certainly not least, we want to apologize. We know how critical our services are to our customers\u2019 businesses, and we know this disruption came at an inopportune time for some of our customers. We will do everything we can to learn from this event and use it to drive further improvement in the ELB service."}
